Index: backend/validation/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/__init__.py b/backend/validation/__init__.py
new file mode 100644
--- /dev/null	(date 1758169658983)
+++ b/backend/validation/__init__.py	(date 1758169658983)
@@ -0,0 +1,45 @@
+"""
+WallStreetBots Validation Framework
+
+Comprehensive validation infrastructure for trading strategies including:
+- Multiple hypothesis testing (Reality Check / SPA)
+- Live drift detection (CUSUM, PSR)
+- Factor analysis with HAC standard errors
+- Regime robustness testing
+- Cross-market validation
+- Ensemble correlation analysis
+- Capital efficiency analysis
+- Alpha validation gates
+
+Usage:
+    from backend.validation import ValidationRunner
+    runner = ValidationRunner()
+    results = runner.run_comprehensive_validation(strategy_returns, benchmark_returns, market_data)
+"""
+
+from .reality_check import RealityCheckValidator, run_reality_check_validation
+from .drift_monitor import PerformanceDriftMonitor, CUSUMDrift, PSRDrift
+from .factor_analysis import AlphaFactorAnalyzer, FactorResult
+from .regime_testing import RegimeValidator, RegimeRobustnessTester
+from .ensemble_evaluator import EnsembleValidator
+from .capital_efficiency import CapitalEfficiencyAnalyzer, KellyResult
+from .validation_runner import ValidationRunner
+
+__all__ = [
+    'AlphaFactorAnalyzer',
+    'CUSUMDrift',
+    'CapitalEfficiencyAnalyzer',
+    'EnsembleValidator',
+    'FactorResult',
+    'KellyResult',
+    'PSRDrift',
+    'PerformanceDriftMonitor',
+    'RealityCheckValidator',
+    'RegimeRobustnessTester',
+    'RegimeValidator',
+    'ValidationRunner',
+    'run_reality_check_validation'
+]
+
+__version__ = '1.0.0'
+__author__ = 'WallStreetBots Team'
\ No newline at end of file
Index: backend/validation/alpha_validation_gate.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/alpha_validation_gate.py b/backend/validation/alpha_validation_gate.py
new file mode 100644
--- /dev/null	(date 1758167952103)
+++ b/backend/validation/alpha_validation_gate.py	(date 1758167952103)
@@ -0,0 +1,1 @@
+"""Alpha Validation Gate with Data-Driven Scorecard."""
Index: backend/validation/fast_edge/capacity_screener.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/fast_edge/capacity_screener.py b/backend/validation/fast_edge/capacity_screener.py
new file mode 100644
--- /dev/null	(date 1758169658986)
+++ b/backend/validation/fast_edge/capacity_screener.py	(date 1758169658986)
@@ -0,0 +1,668 @@
+"""
+Trading Capacity Screening for Fast-Edge Implementation
+Screens and validates trading strategies for capacity constraints and scalability.
+"""
+
+import asyncio
+import logging
+import time
+from datetime import datetime, timedelta
+from typing import Dict, List, Optional, Any, Tuple, Set, Union
+from dataclasses import dataclass, field
+from enum import Enum
+import numpy as np
+import pandas as pd
+from collections import defaultdict, deque
+import json
+
+
+class CapacityConstraint(Enum):
+    LIQUIDITY_LIMITED = "liquidity_limited"
+    IMPACT_COST_LIMITED = "impact_cost_limited"
+    FREQUENCY_LIMITED = "frequency_limited"
+    INFORMATION_DECAY = "information_decay"
+    COMPETITION_CROWDING = "competition_crowding"
+    REGULATORY_LIMITED = "regulatory_limited"
+
+
+class StrategyCategory(Enum):
+    MOMENTUM = "momentum"
+    MEAN_REVERSION = "mean_reversion"
+    ARBITRAGE = "arbitrage"
+    VOLATILITY = "volatility"
+    EARNINGS = "earnings"
+    MACRO = "macro"
+
+
+@dataclass
+class CapacityMetrics:
+    """Metrics for strategy capacity analysis."""
+    strategy_name: str
+    strategy_category: StrategyCategory
+    target_aum: float  # Target Assets Under Management
+    current_aum: float  # Current AUM
+    max_capacity: float  # Maximum theoretical capacity
+    utilization_rate: float  # Current capacity utilization
+    daily_volume_requirement: float  # Daily volume needed
+    market_impact_bps: float  # Expected market impact in bps
+    liquidity_coverage_ratio: float  # Available liquidity vs needed
+    information_decay_half_life_hours: float  # How long edge persists
+    competition_pressure_score: float  # 0-1, higher = more competition
+    regulatory_constraints: List[str] = field(default_factory=list)
+
+
+@dataclass
+class LiquidityProfile:
+    """Liquidity profile for securities/strategies."""
+    symbol: str
+    avg_daily_volume: float
+    avg_daily_dollar_volume: float
+    typical_spread_bps: float
+    depth_at_best: float  # Shares at best bid/ask
+    depth_5_levels: float  # Shares within 5 price levels
+    market_impact_model: Dict[str, float]  # Impact parameters
+    intraday_volume_pattern: List[float]  # Hourly volume pattern
+    volatility_20d: float
+
+
+class MarketImpactModel:
+    """Models market impact for different trade sizes."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.impact_cache: Dict[str, Dict] = {}
+
+    def calculate_impact(self, symbol: str, trade_size: float,
+                        liquidity_profile: LiquidityProfile) -> Dict[str, float]:
+        """Calculate expected market impact for trade."""
+        try:
+            # Cache key
+            cache_key = f"{symbol}_{trade_size}_{int(time.time() // 3600)}"
+            if cache_key in self.impact_cache:
+                return self.impact_cache[cache_key]
+
+            # Participation rate
+            participation_rate = trade_size / liquidity_profile.avg_daily_volume
+
+            # Linear impact component (spread and small size impact)
+            linear_impact_bps = liquidity_profile.typical_spread_bps / 2
+
+            # Square-root impact component (market impact)
+            sqrt_component = np.sqrt(participation_rate) * liquidity_profile.volatility_20d * 100
+
+            # Temporary impact (recovers over time)
+            temporary_impact_bps = linear_impact_bps + 0.1 * sqrt_component
+
+            # Permanent impact (doesn't recover)
+            permanent_impact_bps = 0.3 * sqrt_component
+
+            # Total impact
+            total_impact_bps = temporary_impact_bps + permanent_impact_bps
+
+            # Adjust for liquidity depth
+            depth_adjustment = 1.0
+            if trade_size > liquidity_profile.depth_at_best:
+                depth_adjustment = 1.5  # 50% penalty for going beyond best level
+
+            if trade_size > liquidity_profile.depth_5_levels:
+                depth_adjustment = 2.0  # 100% penalty for going beyond 5 levels
+
+            result = {
+                'total_impact_bps': total_impact_bps * depth_adjustment,
+                'temporary_impact_bps': temporary_impact_bps * depth_adjustment,
+                'permanent_impact_bps': permanent_impact_bps * depth_adjustment,
+                'participation_rate': participation_rate,
+                'depth_adjustment': depth_adjustment
+            }
+
+            # Cache result
+            self.impact_cache[cache_key] = result
+            return result
+
+        except Exception as e:
+            self.logger.error(f"Impact calculation error for {symbol}: {e}")
+            return {
+                'total_impact_bps': 50.0,  # Conservative fallback
+                'temporary_impact_bps': 30.0,
+                'permanent_impact_bps': 20.0,
+                'participation_rate': 0.1,
+                'depth_adjustment': 1.0
+            }
+
+    def estimate_optimal_trade_size(self, symbol: str, target_profit_bps: float,
+                                  liquidity_profile: LiquidityProfile,
+                                  max_participation_rate: float = 0.05) -> float:
+        """Estimate optimal trade size given profit target and constraints."""
+        try:
+            # Binary search for optimal size
+            min_size = 1000  # Minimum trade size
+            max_size = liquidity_profile.avg_daily_volume * max_participation_rate
+
+            optimal_size = min_size
+            max_profit_after_impact = 0
+
+            # Test different sizes
+            test_sizes = np.logspace(np.log10(min_size), np.log10(max_size), 20)
+
+            for size in test_sizes:
+                impact = self.calculate_impact(symbol, size, liquidity_profile)
+                profit_after_impact = target_profit_bps - impact['total_impact_bps']
+
+                if profit_after_impact > max_profit_after_impact:
+                    max_profit_after_impact = profit_after_impact
+                    optimal_size = size
+
+            return optimal_size
+
+        except Exception as e:
+            self.logger.error(f"Optimal size calculation error: {e}")
+            return 10000  # Conservative fallback
+
+
+class LiquidityDataProvider:
+    """Provides liquidity data for capacity screening."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.liquidity_cache: Dict[str, Tuple[LiquidityProfile, datetime]] = {}
+
+    def get_liquidity_profile(self, symbol: str) -> Optional[LiquidityProfile]:
+        """Get liquidity profile for symbol."""
+        try:
+            # Check cache
+            if symbol in self.liquidity_cache:
+                profile, timestamp = self.liquidity_cache[symbol]
+                if datetime.now() - timestamp < timedelta(hours=1):
+                    return profile
+
+            # Generate realistic liquidity profile
+            # In production, this would fetch from market data provider
+            profile = self._generate_liquidity_profile(symbol)
+
+            # Cache profile
+            self.liquidity_cache[symbol] = (profile, datetime.now())
+            return profile
+
+        except Exception as e:
+            self.logger.error(f"Error getting liquidity profile for {symbol}: {e}")
+            return None
+
+    def _generate_liquidity_profile(self, symbol: str) -> LiquidityProfile:
+        """Generate realistic liquidity profile for demo."""
+        # Base profiles by symbol type
+        if symbol in ['SPY', 'QQQ', 'AAPL', 'MSFT']:
+            # Ultra liquid
+            base_volume = np.random.uniform(50_000_000, 100_000_000)
+            base_price = 150
+            spread_bps = np.random.uniform(1, 3)
+        elif symbol in ['TSLA', 'NVDA', 'AMD', 'META']:
+            # High liquid
+            base_volume = np.random.uniform(20_000_000, 50_000_000)
+            base_price = 200
+            spread_bps = np.random.uniform(2, 5)
+        else:
+            # Medium liquid
+            base_volume = np.random.uniform(1_000_000, 10_000_000)
+            base_price = 50
+            spread_bps = np.random.uniform(3, 8)
+
+        return LiquidityProfile(
+            symbol=symbol,
+            avg_daily_volume=base_volume,
+            avg_daily_dollar_volume=base_volume * base_price,
+            typical_spread_bps=spread_bps,
+            depth_at_best=base_volume * 0.001,  # 0.1% of daily volume
+            depth_5_levels=base_volume * 0.005,  # 0.5% of daily volume
+            market_impact_model={'alpha': 0.6, 'beta': 0.1},
+            intraday_volume_pattern=self._generate_volume_pattern(),
+            volatility_20d=np.random.uniform(0.15, 0.40)
+        )
+
+    def _generate_volume_pattern(self) -> List[float]:
+        """Generate realistic intraday volume pattern."""
+        # U-shaped pattern with high volume at open/close
+        base_pattern = [
+            0.15, 0.12, 0.08, 0.06, 0.05, 0.04, 0.04, 0.04,  # 9:30-1:30
+            0.04, 0.04, 0.05, 0.06, 0.08, 0.10, 0.15         # 1:30-4:00
+        ]
+        # Add some noise
+        return [max(0.01, p + np.random.normal(0, 0.01)) for p in base_pattern]
+
+
+class CapacityScreener:
+    """Screens strategies for capacity constraints and scalability."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.impact_model = MarketImpactModel()
+        self.liquidity_provider = LiquidityDataProvider()
+        self.screening_history: List[CapacityMetrics] = []
+
+    def screen_strategy_capacity(self, strategy_name: str, strategy_category: StrategyCategory,
+                               target_aum: float, universe: List[str],
+                               expected_turnover: float, target_profit_bps: float,
+                               holding_period_hours: float) -> CapacityMetrics:
+        """Screen strategy for capacity constraints."""
+        try:
+            # Calculate aggregate liquidity requirements
+            daily_volume_req = self._calculate_volume_requirement(
+                target_aum, expected_turnover, holding_period_hours
+            )
+
+            # Get liquidity for universe
+            universe_liquidity = {}
+            total_available_liquidity = 0
+
+            for symbol in universe:
+                liquidity = self.liquidity_provider.get_liquidity_profile(symbol)
+                if liquidity:
+                    universe_liquidity[symbol] = liquidity
+                    total_available_liquidity += liquidity.avg_daily_dollar_volume
+
+            # Calculate capacity constraints
+            liquidity_coverage = total_available_liquidity / daily_volume_req if daily_volume_req > 0 else float('inf')
+
+            # Calculate market impact
+            avg_trade_size = daily_volume_req / len(universe) if universe else 0
+            total_impact_bps = 0
+
+            for symbol, liquidity in universe_liquidity.items():
+                if liquidity:
+                    impact = self.impact_model.calculate_impact(symbol, avg_trade_size / liquidity.avg_daily_dollar_volume * liquidity.avg_daily_volume, liquidity)
+                    total_impact_bps += impact['total_impact_bps'] / len(universe)
+
+            # Estimate maximum capacity
+            max_capacity = self._estimate_max_capacity(
+                universe_liquidity, target_profit_bps, strategy_category
+            )
+
+            # Calculate information decay
+            info_decay_half_life = self._estimate_information_decay(
+                strategy_category, holding_period_hours
+            )
+
+            # Calculate competition pressure
+            competition_score = self._estimate_competition_pressure(
+                strategy_category, universe
+            )
+
+            # Calculate utilization rate
+            utilization_rate = target_aum / max_capacity if max_capacity > 0 else 1.0
+
+            metrics = CapacityMetrics(
+                strategy_name=strategy_name,
+                strategy_category=strategy_category,
+                target_aum=target_aum,
+                current_aum=target_aum * 0.1,  # Assume starting at 10%
+                max_capacity=max_capacity,
+                utilization_rate=min(1.0, utilization_rate),
+                daily_volume_requirement=daily_volume_req,
+                market_impact_bps=total_impact_bps,
+                liquidity_coverage_ratio=liquidity_coverage,
+                information_decay_half_life_hours=info_decay_half_life,
+                competition_pressure_score=competition_score,
+                regulatory_constraints=self._get_regulatory_constraints(strategy_category)
+            )
+
+            self.screening_history.append(metrics)
+            return metrics
+
+        except Exception as e:
+            self.logger.error(f"Capacity screening error for {strategy_name}: {e}")
+            raise
+
+    def _calculate_volume_requirement(self, target_aum: float, turnover: float,
+                                    holding_period_hours: float) -> float:
+        """Calculate daily volume requirement."""
+        # Annual turnover to daily
+        trading_days_per_year = 252
+        daily_turnover = turnover / trading_days_per_year
+
+        # Adjust for holding period
+        intraday_factor = min(1.0, 24 / holding_period_hours)
+
+        return target_aum * daily_turnover * intraday_factor
+
+    def _estimate_max_capacity(self, universe_liquidity: Dict[str, LiquidityProfile],
+                             target_profit_bps: float, strategy_category: StrategyCategory) -> float:
+        """Estimate maximum strategy capacity."""
+        if not universe_liquidity:
+            return 0
+
+        # Base capacity from liquidity
+        total_liquidity = sum(liq.avg_daily_dollar_volume for liq in universe_liquidity.values())
+
+        # Maximum participation rates by strategy type
+        max_participation_rates = {
+            StrategyCategory.MOMENTUM: 0.03,      # 3% max participation
+            StrategyCategory.MEAN_REVERSION: 0.05, # 5% max participation
+            StrategyCategory.ARBITRAGE: 0.10,      # 10% max participation
+            StrategyCategory.VOLATILITY: 0.02,     # 2% max participation
+            StrategyCategory.EARNINGS: 0.08,       # 8% max participation
+            StrategyCategory.MACRO: 0.15           # 15% max participation
+        }
+
+        max_participation = max_participation_rates.get(strategy_category, 0.05)
+
+        # Base capacity from liquidity constraint
+        liquidity_capacity = total_liquidity * max_participation
+
+        # Adjust for impact tolerance
+        impact_tolerance_bps = target_profit_bps * 0.3  # Use max 30% of profit for impact
+        impact_adjustment = min(1.0, impact_tolerance_bps / 10)  # Scale down if low tolerance
+
+        return liquidity_capacity * impact_adjustment
+
+    def _estimate_information_decay(self, strategy_category: StrategyCategory,
+                                  holding_period_hours: float) -> float:
+        """Estimate information decay half-life."""
+        # Base decay rates by strategy type
+        base_decay_hours = {
+            StrategyCategory.MOMENTUM: 4.0,        # Fast decay
+            StrategyCategory.MEAN_REVERSION: 12.0, # Medium decay
+            StrategyCategory.ARBITRAGE: 0.5,       # Very fast decay
+            StrategyCategory.VOLATILITY: 8.0,      # Medium-fast decay
+            StrategyCategory.EARNINGS: 48.0,       # Slow decay
+            StrategyCategory.MACRO: 168.0          # Very slow decay (1 week)
+        }
+
+        base_decay = base_decay_hours.get(strategy_category, 8.0)
+
+        # Adjust based on holding period
+        decay_adjustment = min(2.0, holding_period_hours / base_decay)
+
+        return base_decay * decay_adjustment
+
+    def _estimate_competition_pressure(self, strategy_category: StrategyCategory,
+                                     universe: List[str]) -> float:
+        """Estimate competition pressure score."""
+        # Base competition by strategy type
+        base_competition = {
+            StrategyCategory.MOMENTUM: 0.8,        # High competition
+            StrategyCategory.MEAN_REVERSION: 0.7,  # High competition
+            StrategyCategory.ARBITRAGE: 0.9,       # Very high competition
+            StrategyCategory.VOLATILITY: 0.6,      # Medium competition
+            StrategyCategory.EARNINGS: 0.5,        # Medium competition
+            StrategyCategory.MACRO: 0.3            # Low competition
+        }
+
+        base_score = base_competition.get(strategy_category, 0.6)
+
+        # Adjust for universe size (smaller universe = more competition)
+        universe_adjustment = max(0.5, 1.0 - len(universe) / 500)
+
+        return min(1.0, base_score * universe_adjustment)
+
+    def _get_regulatory_constraints(self, strategy_category: StrategyCategory) -> List[str]:
+        """Get regulatory constraints by strategy type."""
+        constraints = []
+
+        if strategy_category == StrategyCategory.ARBITRAGE:
+            constraints.extend(['position_limits', 'short_selling_rules'])
+
+        if strategy_category in [StrategyCategory.MOMENTUM, StrategyCategory.VOLATILITY]:
+            constraints.append('pattern_day_trader_rules')
+
+        if strategy_category == StrategyCategory.EARNINGS:
+            constraints.extend(['insider_trading_rules', 'blackout_periods'])
+
+        return constraints
+
+    def generate_capacity_report(self, metrics: CapacityMetrics) -> Dict[str, Any]:
+        """Generate comprehensive capacity report."""
+        # Determine capacity status
+        if metrics.utilization_rate > 0.9:
+            capacity_status = "CRITICAL"
+        elif metrics.utilization_rate > 0.7:
+            capacity_status = "WARNING"
+        elif metrics.utilization_rate > 0.5:
+            capacity_status = "GOOD"
+        else:
+            capacity_status = "EXCELLENT"
+
+        # Calculate runway
+        growth_rate = 0.20  # Assume 20% annual growth
+        years_to_capacity = 0
+        if metrics.utilization_rate < 1.0 and growth_rate > 0:
+            years_to_capacity = np.log(1 / metrics.utilization_rate) / np.log(1 + growth_rate)
+
+        # Generate recommendations
+        recommendations = self._generate_capacity_recommendations(metrics)
+
+        return {
+            'strategy_name': metrics.strategy_name,
+            'capacity_status': capacity_status,
+            'current_utilization': f"{metrics.utilization_rate:.1%}",
+            'max_capacity': f"${metrics.max_capacity:,.0f}",
+            'current_aum': f"${metrics.current_aum:,.0f}",
+            'years_to_capacity': f"{years_to_capacity:.1f}" if years_to_capacity < 100 else "10+",
+            'daily_volume_req': f"${metrics.daily_volume_requirement:,.0f}",
+            'market_impact': f"{metrics.market_impact_bps:.1f} bps",
+            'liquidity_coverage': f"{metrics.liquidity_coverage_ratio:.1f}x",
+            'info_decay_half_life': f"{metrics.information_decay_half_life_hours:.1f} hours",
+            'competition_pressure': f"{metrics.competition_pressure_score:.1%}",
+            'regulatory_constraints': metrics.regulatory_constraints,
+            'recommendations': recommendations,
+            'timestamp': datetime.now().isoformat()
+        }
+
+    def _generate_capacity_recommendations(self, metrics: CapacityMetrics) -> List[str]:
+        """Generate actionable capacity recommendations."""
+        recommendations = []
+
+        if metrics.utilization_rate > 0.8:
+            recommendations.append("Consider capacity expansion strategies")
+
+        if metrics.market_impact_bps > 20:
+            recommendations.append("Reduce position sizes or improve execution")
+
+        if metrics.liquidity_coverage_ratio < 2.0:
+            recommendations.append("Expand trading universe for better liquidity")
+
+        if metrics.information_decay_half_life_hours < 2.0:
+            recommendations.append("Focus on execution speed optimization")
+
+        if metrics.competition_pressure_score > 0.7:
+            recommendations.append("Enhance signal sophistication to reduce competition")
+
+        if len(metrics.regulatory_constraints) > 2:
+            recommendations.append("Review regulatory compliance requirements")
+
+        if not recommendations:
+            recommendations.append("Strategy has good capacity characteristics")
+
+        return recommendations
+
+    def compare_strategies(self, metrics_list: List[CapacityMetrics]) -> Dict[str, Any]:
+        """Compare multiple strategies by capacity metrics."""
+        if not metrics_list:
+            return {}
+
+        comparison = {
+            'strategies': [],
+            'rankings': {
+                'by_capacity': [],
+                'by_efficiency': [],
+                'by_scalability': []
+            },
+            'summary_stats': {}
+        }
+
+        for metrics in metrics_list:
+            strategy_data = {
+                'name': metrics.strategy_name,
+                'category': metrics.strategy_category.value,
+                'max_capacity': metrics.max_capacity,
+                'utilization_rate': metrics.utilization_rate,
+                'market_impact_bps': metrics.market_impact_bps,
+                'efficiency_score': self._calculate_efficiency_score(metrics),
+                'scalability_score': self._calculate_scalability_score(metrics)
+            }
+            comparison['strategies'].append(strategy_data)
+
+        # Rankings
+        strategies = comparison['strategies']
+        comparison['rankings']['by_capacity'] = sorted(strategies, key=lambda x: x['max_capacity'], reverse=True)
+        comparison['rankings']['by_efficiency'] = sorted(strategies, key=lambda x: x['efficiency_score'], reverse=True)
+        comparison['rankings']['by_scalability'] = sorted(strategies, key=lambda x: x['scalability_score'], reverse=True)
+
+        # Summary statistics
+        capacities = [s['max_capacity'] for s in strategies]
+        impacts = [s['market_impact_bps'] for s in strategies]
+
+        comparison['summary_stats'] = {
+            'total_capacity': sum(capacities),
+            'avg_capacity': np.mean(capacities),
+            'median_capacity': np.median(capacities),
+            'avg_impact_bps': np.mean(impacts),
+            'capacity_concentration': max(capacities) / sum(capacities) if sum(capacities) > 0 else 0
+        }
+
+        return comparison
+
+    def _calculate_efficiency_score(self, metrics: CapacityMetrics) -> float:
+        """Calculate efficiency score (capacity per unit of impact)."""
+        if metrics.market_impact_bps > 0:
+            return metrics.max_capacity / metrics.market_impact_bps / 1_000_000
+        return 0
+
+    def _calculate_scalability_score(self, metrics: CapacityMetrics) -> float:
+        """Calculate scalability score (combines multiple factors)."""
+        # Components of scalability
+        utilization_score = 1.0 - metrics.utilization_rate
+        liquidity_score = min(1.0, metrics.liquidity_coverage_ratio / 5.0)
+        decay_score = min(1.0, metrics.information_decay_half_life_hours / 24.0)
+        competition_score = 1.0 - metrics.competition_pressure_score
+
+        # Weighted average
+        scalability = (
+            utilization_score * 0.3 +
+            liquidity_score * 0.3 +
+            decay_score * 0.2 +
+            competition_score * 0.2
+        )
+
+        return scalability
+
+    def get_screening_summary(self) -> Dict[str, Any]:
+        """Get summary of all capacity screenings."""
+        if not self.screening_history:
+            return {'message': 'No strategies screened yet'}
+
+        total_capacity = sum(m.max_capacity for m in self.screening_history)
+        total_current_aum = sum(m.current_aum for m in self.screening_history)
+
+        by_category = defaultdict(list)
+        for metrics in self.screening_history:
+            by_category[metrics.strategy_category.value].append(metrics)
+
+        category_summary = {}
+        for category, metrics_list in by_category.items():
+            category_summary[category] = {
+                'count': len(metrics_list),
+                'total_capacity': sum(m.max_capacity for m in metrics_list),
+                'avg_impact_bps': np.mean([m.market_impact_bps for m in metrics_list]),
+                'avg_utilization': np.mean([m.utilization_rate for m in metrics_list])
+            }
+
+        return {
+            'total_strategies_screened': len(self.screening_history),
+            'total_max_capacity': total_capacity,
+            'total_current_aum': total_current_aum,
+            'overall_utilization': total_current_aum / total_capacity if total_capacity > 0 else 0,
+            'by_category': category_summary,
+            'last_screening': max([m.strategy_name for m in self.screening_history]) if self.screening_history else None
+        }
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    def demo_capacity_screening():
+        print("=== Trading Capacity Screening Demo ===")
+
+        screener = CapacityScreener()
+
+        # Screen different strategy types
+        strategies_to_screen = [
+            {
+                'name': 'Momentum_SPY_Components',
+                'category': StrategyCategory.MOMENTUM,
+                'target_aum': 50_000_000,
+                'universe': ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA'] * 20,  # 100 stocks
+                'turnover': 12.0,  # 12x annual turnover
+                'target_profit_bps': 150,
+                'holding_period_hours': 6
+            },
+            {
+                'name': 'Mean_Reversion_ETFs',
+                'category': StrategyCategory.MEAN_REVERSION,
+                'target_aum': 25_000_000,
+                'universe': ['SPY', 'QQQ', 'IWM', 'VTI', 'EFA'] * 10,  # 50 ETFs
+                'turnover': 6.0,   # 6x annual turnover
+                'target_profit_bps': 100,
+                'holding_period_hours': 24
+            },
+            {
+                'name': 'Earnings_Drift_Large_Cap',
+                'category': StrategyCategory.EARNINGS,
+                'target_aum': 100_000_000,
+                'universe': ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA'] * 5,  # 35 stocks
+                'turnover': 4.0,   # 4x annual turnover
+                'target_profit_bps': 200,
+                'holding_period_hours': 72
+            }
+        ]
+
+        screened_metrics = []
+
+        for strategy in strategies_to_screen:
+            print(f"\nScreening: {strategy['name']}")
+
+            metrics = screener.screen_strategy_capacity(
+                strategy['name'],
+                strategy['category'],
+                strategy['target_aum'],
+                strategy['universe'],
+                strategy['turnover'],
+                strategy['target_profit_bps'],
+                strategy['holding_period_hours']
+            )
+
+            screened_metrics.append(metrics)
+
+            # Generate report
+            report = screener.generate_capacity_report(metrics)
+            print(f"  Status: {report['capacity_status']}")
+            print(f"  Max Capacity: {report['max_capacity']}")
+            print(f"  Utilization: {report['current_utilization']}")
+            print(f"  Market Impact: {report['market_impact']}")
+            print(f"  Liquidity Coverage: {report['liquidity_coverage']}")
+
+            if report['recommendations']:
+                print("  Recommendations:")
+                for rec in report['recommendations'][:2]:
+                    print(f"    â€¢ {rec}")
+
+        # Compare strategies
+        print("\n=== Strategy Comparison ===")
+        comparison = screener.compare_strategies(screened_metrics)
+
+        print(f"Total Combined Capacity: ${comparison['summary_stats']['total_capacity']:,.0f}")
+        print(f"Average Impact: {comparison['summary_stats']['avg_impact_bps']:.1f} bps")
+
+        print("\nTop Strategies by Capacity:")
+        for i, strategy in enumerate(comparison['rankings']['by_capacity'][:3]):
+            print(f"  {i+1}. {strategy['name']}: ${strategy['max_capacity']:,.0f}")
+
+        print("\nTop Strategies by Efficiency:")
+        for i, strategy in enumerate(comparison['rankings']['by_efficiency'][:3]):
+            print(f"  {i+1}. {strategy['name']}: {strategy['efficiency_score']:.1f}")
+
+        # Get overall summary
+        summary = screener.get_screening_summary()
+        print("\n=== Screening Summary ===")
+        print(f"Strategies Screened: {summary['total_strategies_screened']}")
+        print(f"Total Capacity: ${summary['total_max_capacity']:,.0f}")
+        print(f"Overall Utilization: {summary['overall_utilization']:.1%}")
+
+    demo_capacity_screening()
\ No newline at end of file
Index: backend/validation/capital_efficiency.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/capital_efficiency.py b/backend/validation/capital_efficiency.py
new file mode 100644
--- /dev/null	(date 1758168088188)
+++ b/backend/validation/capital_efficiency.py	(date 1758168088188)
@@ -0,0 +1,148 @@
+"""Capital Efficiency / Kelly Analysis with Stable + Safe Implementation."""
+
+import pandas as pd
+import numpy as np
+from typing import Dict, Tuple, Any, List
+from dataclasses import dataclass
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class KellyResult:
+    """Kelly sizing analysis results."""
+    kelly_fraction: float
+    conservative_kelly: float
+    recommended_position_size: float
+    win_rate: float
+    win_loss_ratio: float
+    expected_return: float
+
+
+def _max_dd(r: pd.Series) -> float:
+    """Calculate maximum drawdown safely."""
+    eq = (1 + r.dropna()).cumprod()
+    dd = eq / eq.cummax() - 1
+    return float(dd.min() if len(dd) else 0.0)
+
+
+def _sharpe(r: pd.Series) -> float:
+    """Calculate Sharpe ratio safely."""
+    r = r.dropna()
+    if len(r) < 2 or r.std(ddof=1) == 0:
+        return 0.0
+    return float(np.sqrt(252) * r.mean() / r.std(ddof=1))
+
+
+class CapitalEfficiencyAnalyzer:
+    """Analyzes capital efficiency and optimal leverage."""
+    
+    def __init__(self, daily_margin_rate: float = 0.00008):  # ~2%/yr
+        self.daily_margin_rate = daily_margin_rate
+
+    def analyze_leverage_efficiency(self, strategy, capital_levels: List[float]) -> Dict[str, Any]:
+        """Analyze leverage efficiency across different capital levels."""
+        results: Dict[Tuple[int, float], Dict] = {}
+        
+        for cap in capital_levels:
+            for lev in [1.0, 1.5, 2.0]:
+                try:
+                    bt = strategy.backtest_with_capital(cap * lev)
+                    r = bt.returns.copy()
+                    margin_cost = (lev - 1.0) * self.daily_margin_rate
+                    r_adj = r - margin_cost
+                    
+                    results[(cap, lev)] = {
+                        'sharpe_ratio': _sharpe(r_adj),
+                        'return_on_actual_capital': float((1 + r_adj).prod() - 1),
+                        'max_drawdown': _max_dd(r_adj),
+                        'margin_calls': getattr(bt, 'margin_calls', 0)
+                    }
+                except Exception as e:
+                    logger.warning(f"Failed to analyze leverage {lev}x for capital {cap}: {e}")
+                    continue
+
+        optimal = {}
+        for cap in capital_levels:
+            candidates = {lev: v for (c, lev), v in results.items() if c == cap}
+            if candidates:
+                best = max(candidates, key=lambda k: candidates[k]['return_on_actual_capital'])
+                optimal[cap] = {
+                    'optimal_leverage': best, 
+                    'expected_return_on_capital': candidates[best]['return_on_actual_capital']
+                }
+
+        return {'detailed_results': results, 'optimal_setups': optimal}
+
+    def kelly_sizing_analysis(self, trade_returns: pd.Series, cap_at: float = 0.25) -> KellyResult:
+        """Perform Kelly sizing analysis with stability caps."""
+        r = trade_returns.dropna()
+        if r.empty: 
+            return KellyResult(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)
+        
+        wins = r[r > 0]
+        losses = r[r <= 0]
+        win_rate = float((r > 0).mean())
+        avg_win = float(wins.mean()) if len(wins) else 0.0
+        avg_loss = float(abs(losses.mean())) if len(losses) else 0.0
+        
+        if avg_loss == 0 or win_rate in (0.0, 1.0): 
+            return KellyResult(0.0, 0.0, 0.0, win_rate, 0.0, 0.0)
+        
+        b = avg_win / avg_loss
+        p = win_rate
+        q = 1 - p
+        
+        # Kelly formula: f = (bp - q) / b
+        k = (b * p - q) / max(b, 1e-9)
+        k = max(min(k, 1.0), 0.0)  # clamp to [0,1]
+        
+        # Conservative Kelly (half Kelly)
+        ck = 0.5 * k
+        
+        # Expected return
+        expected_return = p * avg_win - (1 - p) * avg_loss
+        
+        return KellyResult(
+            kelly_fraction=k,
+            conservative_kelly=ck,
+            recommended_position_size=float(min(ck, cap_at)),
+            win_rate=win_rate,
+            win_loss_ratio=b,
+            expected_return=expected_return
+        )
+
+    def analyze_capital_allocation(self, strategies: Dict[str, Any], 
+                                 total_capital: float) -> Dict[str, Any]:
+        """Analyze optimal capital allocation across strategies."""
+        allocations = {}
+        
+        for name, strategy_data in strategies.items():
+            if 'trade_returns' in strategy_data:
+                kelly_result = self.kelly_sizing_analysis(strategy_data['trade_returns'])
+                
+                # Calculate suggested allocation
+                suggested_allocation = kelly_result.recommended_position_size * total_capital
+                
+                allocations[name] = {
+                    'kelly_result': kelly_result,
+                    'suggested_allocation': suggested_allocation,
+                    'allocation_percentage': suggested_allocation / total_capital,
+                    'risk_adjusted_return': kelly_result.expected_return / max(kelly_result.recommended_position_size, 1e-9)
+                }
+        
+        # Normalize allocations if they exceed total capital
+        total_suggested = sum(a['suggested_allocation'] for a in allocations.values())
+        if total_suggested > total_capital:
+            scale_factor = total_capital / total_suggested
+            for allocation in allocations.values():
+                allocation['suggested_allocation'] *= scale_factor
+                allocation['allocation_percentage'] *= scale_factor
+        
+        return {
+            'strategy_allocations': allocations,
+            'total_capital': total_capital,
+            'total_allocated': sum(a['suggested_allocation'] for a in allocations.values()),
+            'capital_utilization': sum(a['suggested_allocation'] for a in allocations.values()) / total_capital
+        }
\ No newline at end of file
Index: backend/validation/operations/chaos_testing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/operations/chaos_testing.py b/backend/validation/operations/chaos_testing.py
new file mode 100644
--- /dev/null	(date 1758169876676)
+++ b/backend/validation/operations/chaos_testing.py	(date 1758169876676)
@@ -0,0 +1,607 @@
+"""
+Chaos Testing Framework for Trading Systems
+Implements controlled failure injection to validate system resilience.
+"""
+
+import asyncio
+import logging
+import random
+import time
+from datetime import datetime, timedelta
+from typing import Dict, List, Optional, Any, Callable, Union, Tuple
+from dataclasses import dataclass, field
+from enum import Enum
+import json
+import threading
+from contextlib import asynccontextmanager
+import psutil
+import signal
+import subprocess
+import numpy as np
+
+
+class ChaosExperimentType(Enum):
+    NETWORK_PARTITION = "network_partition"
+    LATENCY_INJECTION = "latency_injection"
+    MEMORY_PRESSURE = "memory_pressure"
+    CPU_STRESS = "cpu_stress"
+    DISK_FILL = "disk_fill"
+    PROCESS_KILL = "process_kill"
+    DATABASE_FAILURE = "database_failure"
+    API_THROTTLING = "api_throttling"
+    BROKER_DISCONNECT = "broker_disconnect"
+    MARKET_DATA_DELAY = "market_data_delay"
+
+
+class ExperimentStatus(Enum):
+    PLANNED = "planned"
+    RUNNING = "running"
+    COMPLETED = "completed"
+    FAILED = "failed"
+    ABORTED = "aborted"
+
+
+@dataclass
+class ChaosExperiment:
+    """Definition of a chaos experiment."""
+    name: str
+    experiment_type: ChaosExperimentType
+    description: str
+    duration_seconds: int
+    target_components: List[str]
+    parameters: Dict[str, Any] = field(default_factory=dict)
+    safety_checks: List[str] = field(default_factory=list)
+    rollback_actions: List[str] = field(default_factory=list)
+    success_criteria: Dict[str, Any] = field(default_factory=dict)
+    schedule: Optional[Dict[str, Any]] = None
+
+
+@dataclass
+class ExperimentResult:
+    """Result of a chaos experiment."""
+    experiment_name: str
+    status: ExperimentStatus
+    start_time: datetime
+    end_time: Optional[datetime] = None
+    duration_seconds: Optional[int] = None
+    metrics_before: Dict[str, Any] = field(default_factory=dict)
+    metrics_during: Dict[str, Any] = field(default_factory=dict)
+    metrics_after: Dict[str, Any] = field(default_factory=dict)
+    observations: List[str] = field(default_factory=list)
+    failures_detected: List[str] = field(default_factory=list)
+    recovery_time_seconds: Optional[int] = None
+    hypothesis_validated: Optional[bool] = None
+    notes: str = ""
+
+
+class SafetyController:
+    """Controls safety mechanisms during chaos experiments."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.active_experiments: Dict[str, ChaosExperiment] = {}
+        self.emergency_stop = False
+        self.safety_thresholds = {
+            'max_concurrent_experiments': 2,
+            'min_system_health_score': 0.7,
+            'max_position_exposure_pct': 10.0,  # Max 10% of portfolio at risk
+            'trading_hours_only': True
+        }
+
+    def can_start_experiment(self, experiment: ChaosExperiment) -> Tuple[bool, str]:
+        """Check if experiment can safely start."""
+        if self.emergency_stop:
+            return False, "Emergency stop activated"
+
+        if len(self.active_experiments) >= self.safety_thresholds['max_concurrent_experiments']:
+            return False, f"Too many concurrent experiments: {len(self.active_experiments)}"
+
+        # Check trading hours if required
+        if self.safety_thresholds['trading_hours_only']:
+            now = datetime.now()
+            if now.hour < 9 or now.hour >= 16:  # Outside 9 AM - 4 PM ET
+                return False, "Experiments only allowed during trading hours"
+
+        # Check system health
+        health_score = self._get_system_health_score()
+        if health_score < self.safety_thresholds['min_system_health_score']:
+            return False, f"System health too low: {health_score:.2f}"
+
+        # Check position exposure
+        exposure_pct = self._get_position_exposure_percentage()
+        if exposure_pct > self.safety_thresholds['max_position_exposure_pct']:
+            return False, f"Position exposure too high: {exposure_pct:.1f}%"
+
+        return True, "Safety checks passed"
+
+    def register_experiment(self, experiment: ChaosExperiment):
+        """Register active experiment for safety tracking."""
+        self.active_experiments[experiment.name] = experiment
+        self.logger.info(f"Registered chaos experiment: {experiment.name}")
+
+    def unregister_experiment(self, experiment_name: str):
+        """Unregister completed experiment."""
+        if experiment_name in self.active_experiments:
+            del self.active_experiments[experiment_name]
+            self.logger.info(f"Unregistered chaos experiment: {experiment_name}")
+
+    def emergency_abort_all(self):
+        """Emergency abort all running experiments."""
+        self.emergency_stop = True
+        self.logger.critical("EMERGENCY STOP: Aborting all chaos experiments")
+
+        for experiment_name in list(self.active_experiments.keys()):
+            self.logger.critical(f"Emergency aborting: {experiment_name}")
+            # Trigger rollback for each experiment
+            # This would be implemented by the specific experiment handlers
+
+    def _get_system_health_score(self) -> float:
+        """Calculate overall system health score (0-1)."""
+        try:
+            # CPU usage
+            cpu_percent = psutil.cpu_percent(interval=1)
+            cpu_score = max(0, 1 - (cpu_percent / 100))
+
+            # Memory usage
+            memory = psutil.virtual_memory()
+            memory_score = max(0, 1 - (memory.percent / 100))
+
+            # Disk usage
+            disk = psutil.disk_usage('/')
+            disk_score = max(0, 1 - (disk.percent / 100))
+
+            # Network connectivity (simplified)
+            network_score = 1.0  # Would check broker/market data connections
+
+            # Weighted average
+            health_score = (cpu_score * 0.3 + memory_score * 0.3 +
+                          disk_score * 0.2 + network_score * 0.2)
+
+            return min(1.0, max(0.0, health_score))
+
+        except Exception as e:
+            self.logger.error(f"Error calculating health score: {e}")
+            return 0.5  # Conservative default
+
+    def _get_position_exposure_percentage(self) -> float:
+        """Get current position exposure as percentage of portfolio."""
+        # This would integrate with the actual position management system
+        # For now, return a simulated value
+        return random.uniform(1.0, 15.0)
+
+
+class ExperimentExecutor:
+    """Executes chaos experiments with proper controls."""
+
+    def __init__(self, safety_controller: SafetyController):
+        self.safety_controller = safety_controller
+        self.logger = logging.getLogger(__name__)
+        self.metrics_collector = MetricsCollector()
+
+    async def run_experiment(self, experiment: ChaosExperiment) -> ExperimentResult:
+        """Run a single chaos experiment."""
+        result = ExperimentResult(
+            experiment_name=experiment.name,
+            status=ExperimentStatus.PLANNED,
+            start_time=datetime.now()
+        )
+
+        try:
+            # Safety check
+            can_start, reason = self.safety_controller.can_start_experiment(experiment)
+            if not can_start:
+                result.status = ExperimentStatus.ABORTED
+                result.notes = f"Safety check failed: {reason}"
+                return result
+
+            # Register experiment
+            self.safety_controller.register_experiment(experiment)
+            result.status = ExperimentStatus.RUNNING
+
+            # Collect baseline metrics
+            result.metrics_before = await self.metrics_collector.collect_metrics()
+
+            # Execute experiment
+            await self._execute_experiment_logic(experiment, result)
+
+            # Wait for duration
+            await asyncio.sleep(experiment.duration_seconds)
+
+            # Collect during metrics
+            result.metrics_during = await self.metrics_collector.collect_metrics()
+
+            # Rollback experiment
+            await self._rollback_experiment(experiment)
+
+            # Wait for recovery
+            recovery_start = time.time()
+            await self._wait_for_recovery(experiment)
+            result.recovery_time_seconds = int(time.time() - recovery_start)
+
+            # Collect after metrics
+            result.metrics_after = await self.metrics_collector.collect_metrics()
+
+            # Analyze results
+            result.hypothesis_validated = self._validate_hypothesis(experiment, result)
+            result.status = ExperimentStatus.COMPLETED
+
+        except Exception as e:
+            self.logger.error(f"Experiment {experiment.name} failed: {e}")
+            result.status = ExperimentStatus.FAILED
+            result.notes = str(e)
+
+            # Emergency rollback
+            try:
+                await self._rollback_experiment(experiment)
+            except Exception as rollback_error:
+                self.logger.error(f"Rollback failed: {rollback_error}")
+
+        finally:
+            result.end_time = datetime.now()
+            result.duration_seconds = int((result.end_time - result.start_time).total_seconds())
+            self.safety_controller.unregister_experiment(experiment.name)
+
+        return result
+
+    async def _execute_experiment_logic(self, experiment: ChaosExperiment, result: ExperimentResult):
+        """Execute the specific chaos experiment logic."""
+        experiment_type = experiment.experiment_type
+
+        if experiment_type == ChaosExperimentType.NETWORK_PARTITION:
+            await self._inject_network_partition(experiment, result)
+        elif experiment_type == ChaosExperimentType.LATENCY_INJECTION:
+            await self._inject_latency(experiment, result)
+        elif experiment_type == ChaosExperimentType.MEMORY_PRESSURE:
+            await self._inject_memory_pressure(experiment, result)
+        elif experiment_type == ChaosExperimentType.CPU_STRESS:
+            await self._inject_cpu_stress(experiment, result)
+        elif experiment_type == ChaosExperimentType.PROCESS_KILL:
+            await self._kill_process(experiment, result)
+        elif experiment_type == ChaosExperimentType.BROKER_DISCONNECT:
+            await self._simulate_broker_disconnect(experiment, result)
+        elif experiment_type == ChaosExperimentType.MARKET_DATA_DELAY:
+            await self._inject_market_data_delay(experiment, result)
+        else:
+            raise ValueError(f"Unknown experiment type: {experiment_type}")
+
+    async def _inject_network_partition(self, experiment: ChaosExperiment, result: ExperimentResult):
+        """Inject network partition using iptables."""
+        target_hosts = experiment.parameters.get('target_hosts', [])
+
+        for host in target_hosts:
+            # Block traffic to specific host
+            cmd = f"sudo iptables -A OUTPUT -d {host} -j DROP"
+            result.observations.append(f"Blocking traffic to {host}")
+            # Would execute command in production
+
+        experiment.rollback_actions.extend([
+            f"sudo iptables -D OUTPUT -d {host} -j DROP" for host in target_hosts
+        ])
+
+    async def _inject_latency(self, experiment: ChaosExperiment, result: ExperimentResult):
+        """Inject network latency using tc (traffic control)."""
+        interface = experiment.parameters.get('interface', 'eth0')
+        delay_ms = experiment.parameters.get('delay_ms', 100)
+
+        cmd = f"sudo tc qdisc add dev {interface} root netem delay {delay_ms}ms"
+        result.observations.append(f"Added {delay_ms}ms latency to {interface}")
+
+        experiment.rollback_actions.append(f"sudo tc qdisc del dev {interface} root")
+
+    async def _inject_memory_pressure(self, experiment: ChaosExperiment, result: ExperimentResult):
+        """Inject memory pressure by allocating memory."""
+        memory_mb = experiment.parameters.get('memory_mb', 1024)
+
+        # This would be implemented with actual memory allocation
+        result.observations.append(f"Allocated {memory_mb}MB of memory")
+
+    async def _inject_cpu_stress(self, experiment: ChaosExperiment, result: ExperimentResult):
+        """Inject CPU stress using stress tool."""
+        cpu_cores = experiment.parameters.get('cpu_cores', 2)
+
+        cmd = f"stress --cpu {cpu_cores} --timeout {experiment.duration_seconds}s"
+        result.observations.append(f"Started CPU stress on {cpu_cores} cores")
+
+    async def _kill_process(self, experiment: ChaosExperiment, result: ExperimentResult):
+        """Kill specified process."""
+        process_name = experiment.parameters.get('process_name')
+        signal_type = experiment.parameters.get('signal', 'SIGTERM')
+
+        result.observations.append(f"Killing process {process_name} with {signal_type}")
+
+        # In production, would find and kill the actual process
+        # For safety, we simulate this
+
+    async def _simulate_broker_disconnect(self, experiment: ChaosExperiment, result: ExperimentResult):
+        """Simulate broker API disconnection."""
+        broker_endpoints = experiment.parameters.get('broker_endpoints', [])
+
+        for endpoint in broker_endpoints:
+            result.observations.append(f"Simulating disconnect from {endpoint}")
+            # Would inject failures into broker connection layer
+
+    async def _inject_market_data_delay(self, experiment: ChaosExperiment, result: ExperimentResult):
+        """Inject delays in market data feed."""
+        delay_seconds = experiment.parameters.get('delay_seconds', 5)
+
+        result.observations.append(f"Injecting {delay_seconds}s delay in market data")
+        # Would inject delays into market data processing pipeline
+
+    async def _rollback_experiment(self, experiment: ChaosExperiment):
+        """Execute rollback actions to restore normal operation."""
+        for action in experiment.rollback_actions:
+            try:
+                self.logger.info(f"Rollback action: {action}")
+                # Execute rollback command
+                # In production, would actually execute these commands
+            except Exception as e:
+                self.logger.error(f"Rollback action failed: {action}, error: {e}")
+
+    async def _wait_for_recovery(self, experiment: ChaosExperiment):
+        """Wait for system to recover after rollback."""
+        max_wait_seconds = 60
+        check_interval = 5
+
+        for _ in range(max_wait_seconds // check_interval):
+            health_score = self.safety_controller._get_system_health_score()
+            if health_score > 0.8:  # System recovered
+                return
+            await asyncio.sleep(check_interval)
+
+    def _validate_hypothesis(self, experiment: ChaosExperiment, result: ExperimentResult) -> bool:
+        """Validate experiment hypothesis based on results."""
+        success_criteria = experiment.success_criteria
+
+        if not success_criteria:
+            return True  # No specific criteria
+
+        # Check if system maintained minimum availability
+        min_availability = success_criteria.get('min_availability_pct', 95)
+        # Would calculate actual availability from metrics
+
+        # Check if recovery time was acceptable
+        max_recovery_seconds = success_criteria.get('max_recovery_seconds', 30)
+        if result.recovery_time_seconds and result.recovery_time_seconds > max_recovery_seconds:
+            result.failures_detected.append(f"Recovery took {result.recovery_time_seconds}s > {max_recovery_seconds}s")
+            return False
+
+        return len(result.failures_detected) == 0
+
+
+class MetricsCollector:
+    """Collects system metrics during chaos experiments."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+
+    async def collect_metrics(self) -> Dict[str, Any]:
+        """Collect comprehensive system metrics."""
+        try:
+            metrics = {
+                'timestamp': datetime.now().isoformat(),
+                'system': await self._collect_system_metrics(),
+                'trading': await self._collect_trading_metrics(),
+                'network': await self._collect_network_metrics(),
+                'application': await self._collect_application_metrics()
+            }
+            return metrics
+        except Exception as e:
+            self.logger.error(f"Error collecting metrics: {e}")
+            return {'error': str(e), 'timestamp': datetime.now().isoformat()}
+
+    async def _collect_system_metrics(self) -> Dict[str, Any]:
+        """Collect system-level metrics."""
+        return {
+            'cpu_percent': psutil.cpu_percent(interval=1),
+            'memory_percent': psutil.virtual_memory().percent,
+            'disk_percent': psutil.disk_usage('/').percent,
+            'load_average': psutil.getloadavg(),
+            'process_count': len(psutil.pids())
+        }
+
+    async def _collect_trading_metrics(self) -> Dict[str, Any]:
+        """Collect trading-specific metrics."""
+        # This would integrate with actual trading systems
+        return {
+            'active_orders': random.randint(0, 10),
+            'position_count': random.randint(5, 20),
+            'pnl_unrealized': random.uniform(-1000, 1000),
+            'risk_exposure': random.uniform(0.1, 0.8),
+            'last_execution_latency_ms': random.uniform(10, 200)
+        }
+
+    async def _collect_network_metrics(self) -> Dict[str, Any]:
+        """Collect network metrics."""
+        net_io = psutil.net_io_counters()
+        return {
+            'bytes_sent': net_io.bytes_sent,
+            'bytes_recv': net_io.bytes_recv,
+            'packets_sent': net_io.packets_sent,
+            'packets_recv': net_io.packets_recv,
+            'errors_in': net_io.errin,
+            'errors_out': net_io.errout
+        }
+
+    async def _collect_application_metrics(self) -> Dict[str, Any]:
+        """Collect application-specific metrics."""
+        # This would integrate with application monitoring
+        return {
+            'active_strategies': random.randint(3, 8),
+            'market_data_latency_ms': random.uniform(5, 50),
+            'database_connection_pool': random.randint(5, 20),
+            'cache_hit_rate': random.uniform(0.8, 0.99),
+            'error_rate_per_minute': random.uniform(0, 5)
+        }
+
+
+class ChaosTestingSuite:
+    """Complete chaos testing suite for trading systems."""
+
+    def __init__(self):
+        self.safety_controller = SafetyController()
+        self.executor = ExperimentExecutor(self.safety_controller)
+        self.logger = logging.getLogger(__name__)
+        self.experiments: List[ChaosExperiment] = []
+        self.results: List[ExperimentResult] = []
+        self._setup_standard_experiments()
+
+    def _setup_standard_experiments(self):
+        """Setup standard chaos experiments for trading systems."""
+
+        # Network partition experiment
+        self.experiments.append(ChaosExperiment(
+            name="broker_network_partition",
+            experiment_type=ChaosExperimentType.NETWORK_PARTITION,
+            description="Simulate network partition to broker API",
+            duration_seconds=30,
+            target_components=["broker_api"],
+            parameters={
+                'target_hosts': ['api.broker.com', 'data.broker.com']
+            },
+            success_criteria={
+                'min_availability_pct': 95,
+                'max_recovery_seconds': 15
+            }
+        ))
+
+        # Market data delay experiment
+        self.experiments.append(ChaosExperiment(
+            name="market_data_delay",
+            experiment_type=ChaosExperimentType.MARKET_DATA_DELAY,
+            description="Inject delays in market data processing",
+            duration_seconds=60,
+            target_components=["market_data_feed"],
+            parameters={
+                'delay_seconds': 5
+            },
+            success_criteria={
+                'max_stale_data_seconds': 10,
+                'max_recovery_seconds': 5
+            }
+        ))
+
+        # Memory pressure experiment
+        self.experiments.append(ChaosExperiment(
+            name="memory_pressure_test",
+            experiment_type=ChaosExperimentType.MEMORY_PRESSURE,
+            description="Apply memory pressure to test OOM handling",
+            duration_seconds=45,
+            target_components=["risk_engine"],
+            parameters={
+                'memory_mb': 2048
+            },
+            success_criteria={
+                'no_oom_kills': True,
+                'max_recovery_seconds': 20
+            }
+        ))
+
+        # CPU stress experiment
+        self.experiments.append(ChaosExperiment(
+            name="cpu_stress_test",
+            experiment_type=ChaosExperimentType.CPU_STRESS,
+            description="Apply CPU stress to test performance degradation",
+            duration_seconds=30,
+            target_components=["strategy_engine"],
+            parameters={
+                'cpu_cores': 4
+            },
+            success_criteria={
+                'max_latency_increase_pct': 200,
+                'max_recovery_seconds': 10
+            }
+        ))
+
+    async def run_experiment_by_name(self, experiment_name: str) -> ExperimentResult:
+        """Run a specific experiment by name."""
+        experiment = next((exp for exp in self.experiments if exp.name == experiment_name), None)
+        if not experiment:
+            raise ValueError(f"Experiment not found: {experiment_name}")
+
+        result = await self.executor.run_experiment(experiment)
+        self.results.append(result)
+        return result
+
+    async def run_all_experiments(self) -> List[ExperimentResult]:
+        """Run all configured experiments."""
+        results = []
+        for experiment in self.experiments:
+            try:
+                result = await self.executor.run_experiment(experiment)
+                results.append(result)
+                self.results.append(result)
+
+                # Wait between experiments
+                await asyncio.sleep(30)
+
+            except Exception as e:
+                self.logger.error(f"Failed to run experiment {experiment.name}: {e}")
+
+        return results
+
+    def get_experiment_summary(self) -> Dict[str, Any]:
+        """Get summary of all experiment results."""
+        if not self.results:
+            return {'message': 'No experiments run yet'}
+
+        total_experiments = len(self.results)
+        successful_experiments = len([r for r in self.results if r.status == ExperimentStatus.COMPLETED])
+        failed_experiments = len([r for r in self.results if r.status == ExperimentStatus.FAILED])
+
+        avg_recovery_time = np.mean([r.recovery_time_seconds for r in self.results
+                                   if r.recovery_time_seconds is not None])
+
+        return {
+            'total_experiments': total_experiments,
+            'successful_experiments': successful_experiments,
+            'failed_experiments': failed_experiments,
+            'success_rate': successful_experiments / total_experiments if total_experiments > 0 else 0,
+            'average_recovery_time_seconds': avg_recovery_time,
+            'last_run': max([r.start_time for r in self.results]) if self.results else None,
+            'resilience_score': self._calculate_resilience_score()
+        }
+
+    def _calculate_resilience_score(self) -> float:
+        """Calculate overall system resilience score (0-1)."""
+        if not self.results:
+            return 0.0
+
+        # Factors: success rate, recovery times, hypothesis validation
+        success_rate = len([r for r in self.results if r.status == ExperimentStatus.COMPLETED]) / len(self.results)
+
+        validated_hypotheses = len([r for r in self.results if r.hypothesis_validated])
+        hypothesis_rate = validated_hypotheses / len(self.results) if self.results else 0
+
+        # Fast recovery bonus
+        fast_recoveries = len([r for r in self.results
+                             if r.recovery_time_seconds and r.recovery_time_seconds < 30])
+        recovery_rate = fast_recoveries / len(self.results) if self.results else 0
+
+        # Weighted score
+        resilience_score = (success_rate * 0.4 + hypothesis_rate * 0.4 + recovery_rate * 0.2)
+        return min(1.0, resilience_score)
+
+
+# Example usage
+if __name__ == "__main__":
+    import asyncio
+
+    async def run_chaos_testing_demo():
+        suite = ChaosTestingSuite()
+
+        print("Starting chaos testing suite...")
+        print(f"Configured experiments: {len(suite.experiments)}")
+
+        # Run a single experiment
+        print("\nRunning broker network partition experiment...")
+        result = await suite.run_experiment_by_name("broker_network_partition")
+        print(f"Result: {result.status.value}")
+        print(f"Recovery time: {result.recovery_time_seconds}s")
+
+        # Get summary
+        summary = suite.get_experiment_summary()
+        print("\nChaos testing summary:")
+        print(f"Success rate: {summary['success_rate']:.1%}")
+        print(f"Resilience score: {summary['resilience_score']:.2f}")
+
+    asyncio.run(run_chaos_testing_demo())
\ No newline at end of file
Index: backend/validation/broker_accounting/corporate_actions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/broker_accounting/corporate_actions.py b/backend/validation/broker_accounting/corporate_actions.py
new file mode 100644
--- /dev/null	(date 1758169658988)
+++ b/backend/validation/broker_accounting/corporate_actions.py	(date 1758169658988)
@@ -0,0 +1,720 @@
+"""Corporate actions and earnings calendar management.
+
+Provides guaranteed, versioned corporate actions feed and enforces
+pre-trade blackout rules at the adapter boundary.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Tuple, Any, Set
+import logging
+from datetime import datetime, timedelta, date
+from dataclasses import dataclass, field
+from enum import Enum
+import json
+import asyncio
+
+logger = logging.getLogger(__name__)
+
+
+class ActionType(Enum):
+    """Types of corporate actions."""
+    DIVIDEND = "dividend"
+    STOCK_SPLIT = "stock_split"
+    STOCK_DIVIDEND = "stock_dividend"
+    SPIN_OFF = "spin_off"
+    MERGER = "merger"
+    RIGHTS_OFFERING = "rights_offering"
+    SPECIAL_DIVIDEND = "special_dividend"
+    REVERSE_SPLIT = "reverse_split"
+
+
+class EarningsTime(Enum):
+    """Earnings announcement timing."""
+    BEFORE_MARKET = "before_market"
+    AFTER_MARKET = "after_market"
+    DURING_MARKET = "during_market"
+    TIME_NOT_SUPPLIED = "time_not_supplied"
+
+
+@dataclass
+class CorporateAction:
+    """Corporate action record."""
+    symbol: str
+    action_type: ActionType
+    announcement_date: date
+    ex_date: date
+    record_date: date
+    payable_date: Optional[date]
+    value: float  # Dividend amount, split ratio, etc.
+    description: str
+    data_source: str
+    version: int
+    created_at: datetime
+    updated_at: datetime
+    confirmed: bool = False
+    adjustment_factor: float = 1.0  # Price adjustment factor
+
+
+@dataclass
+class EarningsEvent:
+    """Earnings announcement event."""
+    symbol: str
+    earnings_date: date
+    announcement_time: EarningsTime
+    fiscal_year: int
+    fiscal_quarter: int
+    estimated_eps: Optional[float]
+    data_source: str
+    version: int
+    created_at: datetime
+    updated_at: datetime
+    confirmed: bool = False
+
+
+class CorporateActionsManager:
+    """Manages corporate actions data with versioning and validation."""
+
+    def __init__(self):
+        self.actions = {}  # symbol -> List[CorporateAction]
+        self.actions_by_date = {}  # date -> List[CorporateAction]
+        self.version_counter = 1
+        self.data_sources = set()
+
+    def add_corporate_action(self, action: CorporateAction) -> bool:
+        """Add or update a corporate action."""
+        try:
+            # Validate action
+            validation_result = self._validate_action(action)
+            if not validation_result['valid']:
+                logger.error(f"Invalid corporate action: {validation_result['errors']}")
+                return False
+
+            # Set version if not provided
+            if action.version == 0:
+                action.version = self.version_counter
+                self.version_counter += 1
+
+            # Add to symbol index
+            if action.symbol not in self.actions:
+                self.actions[action.symbol] = []
+
+            # Check for duplicates/updates
+            existing_action = self._find_existing_action(action)
+            if existing_action:
+                # Update existing action
+                existing_action.value = action.value
+                existing_action.description = action.description
+                existing_action.updated_at = datetime.now()
+                existing_action.version = self.version_counter
+                self.version_counter += 1
+                logger.info(f"Updated corporate action for {action.symbol}: {action.action_type.value}")
+            else:
+                # Add new action
+                action.created_at = datetime.now()
+                action.updated_at = datetime.now()
+                self.actions[action.symbol].append(action)
+                logger.info(f"Added corporate action for {action.symbol}: {action.action_type.value}")
+
+            # Add to date index
+            ex_date = action.ex_date
+            if ex_date not in self.actions_by_date:
+                self.actions_by_date[ex_date] = []
+
+            if action not in self.actions_by_date[ex_date]:
+                self.actions_by_date[ex_date].append(action)
+
+            # Track data source
+            self.data_sources.add(action.data_source)
+
+            return True
+
+        except Exception as e:
+            logger.error(f"Failed to add corporate action: {e}")
+            return False
+
+    def get_actions_for_symbol(self, symbol: str,
+                              start_date: Optional[date] = None,
+                              end_date: Optional[date] = None) -> List[CorporateAction]:
+        """Get corporate actions for a symbol within date range."""
+        if symbol not in self.actions:
+            return []
+
+        actions = self.actions[symbol]
+
+        if start_date or end_date:
+            filtered_actions = []
+            for action in actions:
+                if start_date and action.ex_date < start_date:
+                    continue
+                if end_date and action.ex_date > end_date:
+                    continue
+                filtered_actions.append(action)
+            return filtered_actions
+
+        return actions.copy()
+
+    def get_actions_for_date(self, target_date: date) -> List[CorporateAction]:
+        """Get all corporate actions for a specific date."""
+        return self.actions_by_date.get(target_date, []).copy()
+
+    def get_upcoming_actions(self, days_ahead: int = 30) -> List[CorporateAction]:
+        """Get upcoming corporate actions within specified days."""
+        today = date.today()
+        end_date = today + timedelta(days=days_ahead)
+
+        upcoming = []
+        current_date = today
+        while current_date <= end_date:
+            upcoming.extend(self.get_actions_for_date(current_date))
+            current_date += timedelta(days=1)
+
+        return upcoming
+
+    def calculate_adjustment_factor(self, symbol: str, price_date: date,
+                                  target_date: date) -> float:
+        """Calculate cumulative price adjustment factor between two dates."""
+        if price_date >= target_date:
+            return 1.0
+
+        actions = self.get_actions_for_symbol(symbol, price_date, target_date)
+        adjustment_factor = 1.0
+
+        for action in actions:
+            if action.ex_date > price_date and action.ex_date <= target_date:
+                if action.action_type == ActionType.DIVIDEND:
+                    # For dividends, factor is typically small
+                    # This is simplified - actual adjustment depends on stock price
+                    continue  # Dividends don't adjust price significantly
+
+                elif action.action_type == ActionType.STOCK_SPLIT:
+                    # For splits, multiply by split ratio
+                    adjustment_factor *= action.value
+
+                elif action.action_type == ActionType.REVERSE_SPLIT:
+                    # For reverse splits, divide by ratio
+                    adjustment_factor /= action.value
+
+                elif action.action_type == ActionType.STOCK_DIVIDEND:
+                    # Stock dividends adjust by percentage
+                    adjustment_factor *= (1 + action.value / 100)
+
+        return adjustment_factor
+
+    def _validate_action(self, action: CorporateAction) -> Dict[str, Any]:
+        """Validate corporate action data."""
+        errors = []
+
+        # Date validation
+        if action.ex_date < action.announcement_date:
+            errors.append("Ex-date cannot be before announcement date")
+
+        if action.record_date and action.record_date < action.ex_date:
+            errors.append("Record date cannot be before ex-date")
+
+        if action.payable_date and action.record_date and action.payable_date < action.record_date:
+            errors.append("Payable date cannot be before record date")
+
+        # Value validation
+        if action.action_type in [ActionType.DIVIDEND, ActionType.SPECIAL_DIVIDEND]:
+            if action.value <= 0:
+                errors.append("Dividend value must be positive")
+
+        elif action.action_type in [ActionType.STOCK_SPLIT, ActionType.REVERSE_SPLIT]:
+            if action.value <= 0:
+                errors.append("Split ratio must be positive")
+
+        # Symbol validation
+        if not action.symbol or len(action.symbol.strip()) == 0:
+            errors.append("Symbol cannot be empty")
+
+        return {
+            'valid': len(errors) == 0,
+            'errors': errors
+        }
+
+    def _find_existing_action(self, action: CorporateAction) -> Optional[CorporateAction]:
+        """Find existing action that matches this one."""
+        if action.symbol not in self.actions:
+            return None
+
+        for existing in self.actions[action.symbol]:
+            if (existing.action_type == action.action_type and
+                existing.ex_date == action.ex_date):
+                return existing
+
+        return None
+
+
+class EarningsCalendarManager:
+    """Manages earnings calendar with blackout enforcement."""
+
+    def __init__(self):
+        self.earnings_events = {}  # symbol -> List[EarningsEvent]
+        self.earnings_by_date = {}  # date -> List[EarningsEvent]
+        self.version_counter = 1
+        self.blackout_rules = {
+            'default_days_before': 1,
+            'default_days_after': 1,
+            'extended_blackout_symbols': set(),  # Symbols with longer blackouts
+            'no_blackout_symbols': set()  # Symbols exempt from blackouts
+        }
+
+    def add_earnings_event(self, event: EarningsEvent) -> bool:
+        """Add or update an earnings event."""
+        try:
+            # Validate event
+            if not self._validate_earnings_event(event):
+                return False
+
+            # Set version if not provided
+            if event.version == 0:
+                event.version = self.version_counter
+                self.version_counter += 1
+
+            # Add to symbol index
+            if event.symbol not in self.earnings_events:
+                self.earnings_events[event.symbol] = []
+
+            # Check for duplicates
+            existing_event = self._find_existing_earnings(event)
+            if existing_event:
+                # Update existing
+                existing_event.announcement_time = event.announcement_time
+                existing_event.estimated_eps = event.estimated_eps
+                existing_event.updated_at = datetime.now()
+                existing_event.version = self.version_counter
+                self.version_counter += 1
+            else:
+                # Add new
+                event.created_at = datetime.now()
+                event.updated_at = datetime.now()
+                self.earnings_events[event.symbol].append(event)
+
+            # Add to date index
+            earnings_date = event.earnings_date
+            if earnings_date not in self.earnings_by_date:
+                self.earnings_by_date[earnings_date] = []
+
+            if event not in self.earnings_by_date[earnings_date]:
+                self.earnings_by_date[earnings_date].append(event)
+
+            logger.info(f"Added earnings event for {event.symbol}: {event.earnings_date}")
+            return True
+
+        except Exception as e:
+            logger.error(f"Failed to add earnings event: {e}")
+            return False
+
+    def is_in_blackout_period(self, symbol: str, check_date: date) -> Tuple[bool, str]:
+        """Check if symbol is in earnings blackout period."""
+        if symbol in self.blackout_rules['no_blackout_symbols']:
+            return False, "Symbol exempt from blackout"
+
+        earnings = self.get_earnings_for_symbol(symbol)
+
+        for event in earnings:
+            earnings_date = event.earnings_date
+
+            # Determine blackout window
+            if symbol in self.blackout_rules['extended_blackout_symbols']:
+                days_before = 3
+                days_after = 2
+            else:
+                days_before = self.blackout_rules['default_days_before']
+                days_after = self.blackout_rules['default_days_after']
+
+            blackout_start = earnings_date - timedelta(days=days_before)
+            blackout_end = earnings_date + timedelta(days=days_after)
+
+            if blackout_start <= check_date <= blackout_end:
+                return True, f"Earnings blackout: {earnings_date} ({event.announcement_time.value})"
+
+        return False, "Not in blackout period"
+
+    def get_earnings_for_symbol(self, symbol: str,
+                               start_date: Optional[date] = None,
+                               end_date: Optional[date] = None) -> List[EarningsEvent]:
+        """Get earnings events for a symbol within date range."""
+        if symbol not in self.earnings_events:
+            return []
+
+        events = self.earnings_events[symbol]
+
+        if start_date or end_date:
+            filtered_events = []
+            for event in events:
+                if start_date and event.earnings_date < start_date:
+                    continue
+                if end_date and event.earnings_date > end_date:
+                    continue
+                filtered_events.append(event)
+            return filtered_events
+
+        return events.copy()
+
+    def get_upcoming_earnings(self, days_ahead: int = 7) -> List[EarningsEvent]:
+        """Get upcoming earnings within specified days."""
+        today = date.today()
+        end_date = today + timedelta(days=days_ahead)
+
+        upcoming = []
+        current_date = today
+        while current_date <= end_date:
+            upcoming.extend(self.earnings_by_date.get(current_date, []))
+            current_date += timedelta(days=1)
+
+        return upcoming
+
+    def update_blackout_rules(self, rules: Dict[str, Any]):
+        """Update blackout rules configuration."""
+        if 'default_days_before' in rules:
+            self.blackout_rules['default_days_before'] = rules['default_days_before']
+
+        if 'default_days_after' in rules:
+            self.blackout_rules['default_days_after'] = rules['default_days_after']
+
+        if 'extended_blackout_symbols' in rules:
+            self.blackout_rules['extended_blackout_symbols'].update(rules['extended_blackout_symbols'])
+
+        if 'no_blackout_symbols' in rules:
+            self.blackout_rules['no_blackout_symbols'].update(rules['no_blackout_symbols'])
+
+        logger.info(f"Updated blackout rules: {self.blackout_rules}")
+
+    def _validate_earnings_event(self, event: EarningsEvent) -> bool:
+        """Validate earnings event data."""
+        if not event.symbol or len(event.symbol.strip()) == 0:
+            logger.error("Earnings event symbol cannot be empty")
+            return False
+
+        if event.earnings_date < date.today() - timedelta(days=365):
+            logger.error("Earnings date too far in the past")
+            return False
+
+        if event.fiscal_quarter not in [1, 2, 3, 4]:
+            logger.error("Invalid fiscal quarter")
+            return False
+
+        return True
+
+    def _find_existing_earnings(self, event: EarningsEvent) -> Optional[EarningsEvent]:
+        """Find existing earnings event that matches this one."""
+        if event.symbol not in self.earnings_events:
+            return None
+
+        for existing in self.earnings_events[event.symbol]:
+            if (existing.earnings_date == event.earnings_date and
+                existing.fiscal_year == event.fiscal_year and
+                existing.fiscal_quarter == event.fiscal_quarter):
+                return existing
+
+        return None
+
+
+class TradingBlackoutEnforcer:
+    """Enforces trading blackouts at the adapter boundary."""
+
+    def __init__(self, corporate_actions_manager: CorporateActionsManager,
+                 earnings_calendar: EarningsCalendarManager):
+        self.ca_manager = corporate_actions_manager
+        self.earnings_calendar = earnings_calendar
+        self.blocked_orders = []
+        self.whitelist_overrides = set()  # Orders that can bypass blackouts
+
+    def check_order_allowed(self, symbol: str, order_id: str,
+                           order_date: Optional[date] = None) -> Tuple[bool, str]:
+        """Check if order is allowed given blackout rules."""
+        if order_date is None:
+            order_date = date.today()
+
+        # Check whitelist override
+        if order_id in self.whitelist_overrides:
+            return True, "Whitelist override active"
+
+        # Check earnings blackout
+        in_earnings_blackout, earnings_reason = self.earnings_calendar.is_in_blackout_period(
+            symbol, order_date
+        )
+
+        if in_earnings_blackout:
+            self._log_blocked_order(symbol, order_id, "earnings_blackout", earnings_reason)
+            return False, earnings_reason
+
+        # Check corporate actions blackout
+        upcoming_actions = self.ca_manager.get_actions_for_symbol(
+            symbol, order_date, order_date + timedelta(days=5)
+        )
+
+        for action in upcoming_actions:
+            days_until_ex = (action.ex_date - order_date).days
+
+            # Block trading within 1 day of ex-date for significant actions
+            if (days_until_ex <= 1 and
+                action.action_type in [ActionType.STOCK_SPLIT, ActionType.REVERSE_SPLIT,
+                                     ActionType.SPIN_OFF, ActionType.MERGER]):
+                reason = f"Corporate action blackout: {action.action_type.value} ex-date {action.ex_date}"
+                self._log_blocked_order(symbol, order_id, "corporate_action_blackout", reason)
+                return False, reason
+
+        return True, "Order allowed"
+
+    def add_whitelist_override(self, order_id: str, reason: str):
+        """Add order to whitelist (for emergency trading)."""
+        self.whitelist_overrides.add(order_id)
+        logger.warning(f"Added whitelist override for order {order_id}: {reason}")
+
+    def remove_whitelist_override(self, order_id: str):
+        """Remove order from whitelist."""
+        self.whitelist_overrides.discard(order_id)
+
+    def get_blackout_summary(self, symbols: List[str]) -> Dict[str, Any]:
+        """Get blackout status summary for multiple symbols."""
+        today = date.today()
+        summary = {
+            'check_date': today,
+            'symbols_checked': len(symbols),
+            'symbols_in_blackout': 0,
+            'blackout_details': {},
+            'upcoming_blackouts': []
+        }
+
+        for symbol in symbols:
+            # Current blackout status
+            order_allowed, reason = self.check_order_allowed(symbol, f"check_{symbol}")
+
+            if not order_allowed:
+                summary['symbols_in_blackout'] += 1
+                summary['blackout_details'][symbol] = {
+                    'in_blackout': True,
+                    'reason': reason
+                }
+            else:
+                summary['blackout_details'][symbol] = {
+                    'in_blackout': False,
+                    'reason': reason
+                }
+
+            # Upcoming blackouts (next 7 days)
+            for days_ahead in range(1, 8):
+                future_date = today + timedelta(days=days_ahead)
+                future_allowed, future_reason = self.check_order_allowed(symbol, f"future_{symbol}", future_date)
+
+                if not future_allowed:
+                    summary['upcoming_blackouts'].append({
+                        'symbol': symbol,
+                        'date': future_date,
+                        'reason': future_reason
+                    })
+                    break  # Only report first upcoming blackout per symbol
+
+        return summary
+
+    def _log_blocked_order(self, symbol: str, order_id: str, block_type: str, reason: str):
+        """Log blocked order for audit trail."""
+        blocked_record = {
+            'timestamp': datetime.now(),
+            'symbol': symbol,
+            'order_id': order_id,
+            'block_type': block_type,
+            'reason': reason
+        }
+
+        self.blocked_orders.append(blocked_record)
+
+        # Keep only last 1000 blocked orders
+        if len(self.blocked_orders) > 1000:
+            self.blocked_orders = self.blocked_orders[-1000:]
+
+        logger.warning(f"BLOCKED ORDER: {symbol} {order_id} - {reason}")
+
+    def get_blocked_orders_report(self, hours: int = 24) -> Dict[str, Any]:
+        """Get report of recently blocked orders."""
+        cutoff_time = datetime.now() - timedelta(hours=hours)
+        recent_blocks = [
+            block for block in self.blocked_orders
+            if block['timestamp'] >= cutoff_time
+        ]
+
+        if not recent_blocks:
+            return {
+                'period_hours': hours,
+                'total_blocked': 0,
+                'blocks_by_type': {},
+                'blocks_by_symbol': {},
+                'recent_blocks': []
+            }
+
+        # Group by type
+        blocks_by_type = {}
+        for block in recent_blocks:
+            block_type = block['block_type']
+            if block_type not in blocks_by_type:
+                blocks_by_type[block_type] = 0
+            blocks_by_type[block_type] += 1
+
+        # Group by symbol
+        blocks_by_symbol = {}
+        for block in recent_blocks:
+            symbol = block['symbol']
+            if symbol not in blocks_by_symbol:
+                blocks_by_symbol[symbol] = 0
+            blocks_by_symbol[symbol] += 1
+
+        return {
+            'period_hours': hours,
+            'total_blocked': len(recent_blocks),
+            'blocks_by_type': blocks_by_type,
+            'blocks_by_symbol': blocks_by_symbol,
+            'recent_blocks': recent_blocks[-20:]  # Last 20 blocks
+        }
+
+
+class CorporateActionsDataFeed:
+    """Data feed interface for corporate actions and earnings."""
+
+    def __init__(self, ca_manager: CorporateActionsManager,
+                 earnings_manager: EarningsCalendarManager):
+        self.ca_manager = ca_manager
+        self.earnings_manager = earnings_manager
+        self.data_sources = {}  # source_name -> config
+
+    def register_data_source(self, source_name: str, config: Dict[str, Any]):
+        """Register a data source for corporate actions."""
+        self.data_sources[source_name] = config
+        logger.info(f"Registered data source: {source_name}")
+
+    async def sync_corporate_actions(self, source_name: str, symbols: List[str]) -> Dict[str, Any]:
+        """Sync corporate actions from external data source."""
+        if source_name not in self.data_sources:
+            return {'error': f'Data source {source_name} not registered'}
+
+        logger.info(f"Syncing corporate actions from {source_name} for {len(symbols)} symbols")
+
+        sync_results = {
+            'source': source_name,
+            'symbols_processed': 0,
+            'actions_added': 0,
+            'actions_updated': 0,
+            'errors': []
+        }
+
+        try:
+            # In production, this would call actual data provider APIs
+            # For simulation, generate sample data
+            for symbol in symbols[:5]:  # Limit for demo
+                sample_actions = self._generate_sample_corporate_actions(symbol, source_name)
+
+                for action in sample_actions:
+                    success = self.ca_manager.add_corporate_action(action)
+                    if success:
+                        sync_results['actions_added'] += 1
+                    else:
+                        sync_results['errors'].append(f"Failed to add action for {symbol}")
+
+                sync_results['symbols_processed'] += 1
+
+        except Exception as e:
+            sync_results['errors'].append(f"Sync failed: {e!s}")
+
+        logger.info(f"Corporate actions sync completed: {sync_results}")
+        return sync_results
+
+    async def sync_earnings_calendar(self, source_name: str, days_ahead: int = 30) -> Dict[str, Any]:
+        """Sync earnings calendar from external data source."""
+        if source_name not in self.data_sources:
+            return {'error': f'Data source {source_name} not registered'}
+
+        logger.info(f"Syncing earnings calendar from {source_name} for next {days_ahead} days")
+
+        sync_results = {
+            'source': source_name,
+            'events_added': 0,
+            'events_updated': 0,
+            'errors': []
+        }
+
+        try:
+            # Generate sample earnings events
+            sample_events = self._generate_sample_earnings_events(source_name, days_ahead)
+
+            for event in sample_events:
+                success = self.earnings_manager.add_earnings_event(event)
+                if success:
+                    sync_results['events_added'] += 1
+                else:
+                    sync_results['errors'].append(f"Failed to add earnings for {event.symbol}")
+
+        except Exception as e:
+            sync_results['errors'].append(f"Earnings sync failed: {e!s}")
+
+        logger.info(f"Earnings calendar sync completed: {sync_results}")
+        return sync_results
+
+    def _generate_sample_corporate_actions(self, symbol: str, source: str) -> List[CorporateAction]:
+        """Generate sample corporate actions for testing."""
+        actions = []
+
+        # Sample dividend
+        if np.random.random() > 0.7:  # 30% chance of dividend
+            dividend_date = date.today() + timedelta(days=np.random.randint(10, 60))
+            actions.append(CorporateAction(
+                symbol=symbol,
+                action_type=ActionType.DIVIDEND,
+                announcement_date=dividend_date - timedelta(days=30),
+                ex_date=dividend_date,
+                record_date=dividend_date + timedelta(days=2),
+                payable_date=dividend_date + timedelta(days=30),
+                value=round(np.random.uniform(0.25, 2.0), 2),
+                description="Quarterly dividend",
+                data_source=source,
+                version=0,
+                created_at=datetime.now(),
+                updated_at=datetime.now()
+            ))
+
+        # Sample stock split (rare)
+        if np.random.random() > 0.95:  # 5% chance of split
+            split_date = date.today() + timedelta(days=np.random.randint(20, 90))
+            actions.append(CorporateAction(
+                symbol=symbol,
+                action_type=ActionType.STOCK_SPLIT,
+                announcement_date=split_date - timedelta(days=30),
+                ex_date=split_date,
+                record_date=split_date,
+                payable_date=None,
+                value=2.0,  # 2:1 split
+                description="2-for-1 stock split",
+                data_source=source,
+                version=0,
+                created_at=datetime.now(),
+                updated_at=datetime.now()
+            ))
+
+        return actions
+
+    def _generate_sample_earnings_events(self, source: str, days_ahead: int) -> List[EarningsEvent]:
+        """Generate sample earnings events for testing."""
+        events = []
+
+        # Common symbols with upcoming earnings
+        sample_symbols = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META']
+
+        for symbol in sample_symbols:
+            if np.random.random() > 0.6:  # 40% chance of earnings in period
+                earnings_date = date.today() + timedelta(days=np.random.randint(1, days_ahead))
+
+                events.append(EarningsEvent(
+                    symbol=symbol,
+                    earnings_date=earnings_date,
+                    announcement_time=np.random.choice(list(EarningsTime)),
+                    fiscal_year=2024,
+                    fiscal_quarter=np.random.randint(1, 5),
+                    estimated_eps=round(np.random.uniform(1.0, 5.0), 2),
+                    data_source=source,
+                    version=0,
+                    created_at=datetime.now(),
+                    updated_at=datetime.now()
+                ))
+
+        return events
\ No newline at end of file
Index: backend/validation/cross_market_validator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/cross_market_validator.py b/backend/validation/cross_market_validator.py
new file mode 100644
--- /dev/null	(date 1758167944305)
+++ b/backend/validation/cross_market_validator.py	(date 1758167944305)
@@ -0,0 +1,1 @@
+"""Cross-Market Validation with Safe Data Handling."""
Index: backend/validation/advanced_risk/cross_strategy_risk.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/advanced_risk/cross_strategy_risk.py b/backend/validation/advanced_risk/cross_strategy_risk.py
new file mode 100644
--- /dev/null	(date 1758165030171)
+++ b/backend/validation/advanced_risk/cross_strategy_risk.py	(date 1758165030171)
@@ -0,0 +1,593 @@
+"""Cross-strategy risk management and correlation clustering.
+
+Implements hard caps per correlation cluster to prevent stacking
+the same bet across multiple strategies.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Tuple, Any, Set
+import logging
+from datetime import datetime, timedelta
+from dataclasses import dataclass, field
+from scipy.cluster.hierarchy import linkage, fcluster
+from scipy.spatial.distance import squareform
+import warnings
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class StrategyCluster:
+    """Represents a cluster of correlated strategies."""
+    cluster_id: int
+    strategy_names: List[str]
+    avg_correlation: float
+    max_correlation: float
+    cluster_size: int
+    risk_budget: float
+    current_allocation: float
+    available_capacity: float
+
+
+@dataclass
+class RiskAllocation:
+    """Risk allocation for a strategy."""
+    strategy_name: str
+    cluster_id: int
+    position_size: float
+    risk_contribution: float
+    marginal_var: float
+    component_var: float
+    correlation_penalty: float
+
+
+@dataclass
+class PortfolioRiskMetrics:
+    """Portfolio-level risk metrics."""
+    total_var: float
+    diversified_var: float
+    concentration_risk: float
+    correlation_risk: float
+    cluster_concentrations: Dict[int, float]
+    undiversified_var: float
+    diversification_ratio: float
+
+
+class CorrelationClusterAnalyzer:
+    """Analyzes strategy correlations and creates risk clusters."""
+
+    def __init__(self, correlation_threshold: float = 0.6):
+        self.correlation_threshold = correlation_threshold
+        self.strategy_clusters = {}
+        self.cluster_risk_budgets = {}
+        self.correlation_matrix = None
+
+    def analyze_strategy_correlations(self, strategy_returns: Dict[str, pd.Series],
+                                    lookback_days: int = 252) -> Dict[int, StrategyCluster]:
+        """Analyze correlations and create strategy clusters.
+
+        Args:
+            strategy_returns: Dictionary of strategy name -> return series
+            lookback_days: Number of days to use for correlation calculation
+
+        Returns:
+            Dictionary of cluster_id -> StrategyCluster
+        """
+        if len(strategy_returns) < 2:
+            logger.warning("Need at least 2 strategies for correlation analysis")
+            return {}
+
+        # Align return series and get recent data
+        aligned_returns = self._align_returns(strategy_returns, lookback_days)
+
+        if aligned_returns.empty:
+            logger.error("No aligned return data available")
+            return {}
+
+        # Calculate correlation matrix
+        self.correlation_matrix = aligned_returns.corr()
+
+        # Create clusters using hierarchical clustering
+        clusters = self._create_correlation_clusters(self.correlation_matrix)
+
+        # Calculate cluster statistics
+        cluster_stats = self._calculate_cluster_statistics(clusters, self.correlation_matrix)
+
+        # Assign risk budgets
+        cluster_risk_budgets = self._assign_cluster_risk_budgets(cluster_stats)
+
+        # Create StrategyCluster objects
+        strategy_clusters = {}
+        for cluster_id, strategies in clusters.items():
+            stats = cluster_stats[cluster_id]
+            risk_budget = cluster_risk_budgets[cluster_id]
+
+            strategy_clusters[cluster_id] = StrategyCluster(
+                cluster_id=cluster_id,
+                strategy_names=strategies,
+                avg_correlation=stats['avg_correlation'],
+                max_correlation=stats['max_correlation'],
+                cluster_size=len(strategies),
+                risk_budget=risk_budget,
+                current_allocation=0.0,  # To be updated later
+                available_capacity=risk_budget
+            )
+
+        self.strategy_clusters = strategy_clusters
+        self.cluster_risk_budgets = cluster_risk_budgets
+
+        logger.info(f"Created {len(strategy_clusters)} strategy clusters")
+        return strategy_clusters
+
+    def _align_returns(self, strategy_returns: Dict[str, pd.Series],
+                      lookback_days: int) -> pd.DataFrame:
+        """Align return series to common dates."""
+        # Combine all series
+        combined_df = pd.DataFrame(strategy_returns)
+
+        # Get recent data
+        if len(combined_df) > lookback_days:
+            combined_df = combined_df.tail(lookback_days)
+
+        # Remove rows with any NaN values
+        clean_df = combined_df.dropna()
+
+        if len(clean_df) < 30:
+            logger.warning(f"Only {len(clean_df)} clean observations available")
+
+        return clean_df
+
+    def _create_correlation_clusters(self, correlation_matrix: pd.DataFrame) -> Dict[int, List[str]]:
+        """Create strategy clusters using hierarchical clustering."""
+        if correlation_matrix.empty or len(correlation_matrix) < 2:
+            return {}
+
+        try:
+            # Convert correlation to distance
+            distance_matrix = 1 - abs(correlation_matrix.values)
+
+            # Handle NaN values
+            distance_matrix = np.nan_to_num(distance_matrix, nan=1.0)
+
+            # Ensure distance matrix is symmetric and has zero diagonal
+            np.fill_diagonal(distance_matrix, 0)
+            distance_matrix = (distance_matrix + distance_matrix.T) / 2
+
+            # Convert to condensed form for linkage
+            condensed_distances = squareform(distance_matrix, checks=False)
+
+            # Perform hierarchical clustering
+            linkage_matrix = linkage(condensed_distances, method='average')
+
+            # Form clusters based on distance threshold
+            distance_threshold = 1 - self.correlation_threshold
+            cluster_labels = fcluster(linkage_matrix, distance_threshold, criterion='distance')
+
+            # Group strategies by cluster
+            clusters = {}
+            for i, strategy in enumerate(correlation_matrix.index):
+                cluster_id = cluster_labels[i]
+                if cluster_id not in clusters:
+                    clusters[cluster_id] = []
+                clusters[cluster_id].append(strategy)
+
+            return clusters
+
+        except Exception as e:
+            logger.error(f"Clustering failed: {e}")
+            # Fallback: each strategy is its own cluster
+            return {i + 1: [strategy] for i, strategy in enumerate(correlation_matrix.index)}
+
+    def _calculate_cluster_statistics(self, clusters: Dict[int, List[str]],
+                                    correlation_matrix: pd.DataFrame) -> Dict[int, Dict]:
+        """Calculate statistics for each cluster."""
+        cluster_stats = {}
+
+        for cluster_id, strategies in clusters.items():
+            if len(strategies) == 1:
+                cluster_stats[cluster_id] = {
+                    'avg_correlation': 0.0,
+                    'max_correlation': 0.0,
+                    'min_correlation': 0.0,
+                    'correlation_std': 0.0
+                }
+            else:
+                # Get correlation submatrix for this cluster
+                cluster_corr = correlation_matrix.loc[strategies, strategies]
+
+                # Calculate statistics (excluding diagonal)
+                upper_triangle = np.triu(cluster_corr.values, k=1)
+                correlations = upper_triangle[upper_triangle != 0]
+
+                if len(correlations) > 0:
+                    cluster_stats[cluster_id] = {
+                        'avg_correlation': np.mean(correlations),
+                        'max_correlation': np.max(correlations),
+                        'min_correlation': np.min(correlations),
+                        'correlation_std': np.std(correlations)
+                    }
+                else:
+                    cluster_stats[cluster_id] = {
+                        'avg_correlation': 0.0,
+                        'max_correlation': 0.0,
+                        'min_correlation': 0.0,
+                        'correlation_std': 0.0
+                    }
+
+        return cluster_stats
+
+    def _assign_cluster_risk_budgets(self, cluster_stats: Dict[int, Dict]) -> Dict[int, float]:
+        """Assign risk budgets to clusters based on diversification benefit."""
+        total_budget = 1.0  # 100% of risk budget
+        cluster_risk_budgets = {}
+
+        if not cluster_stats:
+            return {}
+
+        # Calculate base allocation (equal weight)
+        num_clusters = len(cluster_stats)
+        base_allocation = total_budget / num_clusters
+
+        # Adjust based on cluster size and correlation
+        adjusted_budgets = {}
+        total_adjustment_weight = 0
+
+        for cluster_id, stats in cluster_stats.items():
+            # Clusters with lower correlation get higher budget
+            correlation_discount = 1 - stats['avg_correlation']
+
+            # Larger clusters get slightly higher budget (diversification within cluster)
+            cluster_size = len(self.strategy_clusters.get(cluster_id, {}).get('strategy_names', [1]))
+            size_bonus = min(1.2, 1 + 0.05 * (cluster_size - 1))
+
+            # Combined adjustment weight
+            adjustment_weight = correlation_discount * size_bonus
+            adjusted_budgets[cluster_id] = adjustment_weight
+            total_adjustment_weight += adjustment_weight
+
+        # Normalize to total budget
+        if total_adjustment_weight > 0:
+            for cluster_id in adjusted_budgets:
+                cluster_risk_budgets[cluster_id] = (
+                    adjusted_budgets[cluster_id] / total_adjustment_weight * total_budget
+                )
+        else:
+            # Fallback to equal allocation
+            for cluster_id in cluster_stats:
+                cluster_risk_budgets[cluster_id] = base_allocation
+
+        return cluster_risk_budgets
+
+    def get_cluster_for_strategy(self, strategy_name: str) -> Optional[int]:
+        """Get cluster ID for a strategy."""
+        for cluster_id, cluster in self.strategy_clusters.items():
+            if strategy_name in cluster.strategy_names:
+                return cluster_id
+        return None
+
+
+class CrossStrategyRiskManager:
+    """Manages risk across multiple strategies with cluster-based limits."""
+
+    def __init__(self):
+        self.cluster_analyzer = CorrelationClusterAnalyzer()
+        self.current_allocations = {}  # strategy_name -> allocation
+        self.risk_limits = {
+            'max_cluster_allocation': 0.40,    # 40% max per cluster
+            'max_single_strategy': 0.20,       # 20% max per strategy
+            'max_correlation_exposure': 0.60,  # 60% max in correlated strategies
+            'min_diversification_ratio': 1.5   # Minimum diversification benefit
+        }
+        self.portfolio_var = 0.0
+        self.last_update = None
+
+    def update_strategy_correlations(self, strategy_returns: Dict[str, pd.Series]):
+        """Update correlation analysis and cluster assignments."""
+        self.cluster_analyzer.analyze_strategy_correlations(strategy_returns)
+        self.last_update = datetime.now()
+        logger.info("Updated strategy correlation clusters")
+
+    def calculate_portfolio_risk_allocation(self,
+                                          proposed_allocations: Dict[str, float],
+                                          strategy_returns: Dict[str, pd.Series]) -> Dict[str, Any]:
+        """Calculate optimal risk allocation across strategies with cluster constraints.
+
+        Args:
+            proposed_allocations: Dictionary of strategy_name -> proposed allocation
+            strategy_returns: Dictionary of strategy_name -> return series
+
+        Returns:
+            Dictionary with allocation recommendations and risk analysis
+        """
+        # Update correlations if needed
+        if not self.cluster_analyzer.strategy_clusters:
+            self.update_strategy_correlations(strategy_returns)
+
+        # Validate and adjust allocations
+        validated_allocations = self._validate_cluster_constraints(proposed_allocations)
+
+        # Calculate portfolio risk metrics
+        portfolio_risk = self._calculate_portfolio_risk_metrics(
+            validated_allocations, strategy_returns
+        )
+
+        # Generate risk allocations
+        risk_allocations = self._calculate_risk_contributions(
+            validated_allocations, strategy_returns
+        )
+
+        # Create recommendations
+        recommendations = self._generate_allocation_recommendations(
+            proposed_allocations, validated_allocations, portfolio_risk
+        )
+
+        return {
+            'validated_allocations': validated_allocations,
+            'original_allocations': proposed_allocations,
+            'portfolio_risk_metrics': portfolio_risk,
+            'risk_allocations': risk_allocations,
+            'cluster_exposures': self._calculate_cluster_exposures(validated_allocations),
+            'recommendations': recommendations,
+            'risk_budget_utilization': self._calculate_risk_budget_utilization(validated_allocations),
+            'constraint_violations': self._check_constraint_violations(validated_allocations)
+        }
+
+    def _validate_cluster_constraints(self, proposed_allocations: Dict[str, float]) -> Dict[str, float]:
+        """Validate and adjust allocations to respect cluster constraints."""
+        validated_allocations = proposed_allocations.copy()
+
+        # Check cluster allocation limits
+        cluster_allocations = {}
+        for strategy_name, allocation in validated_allocations.items():
+            cluster_id = self.cluster_analyzer.get_cluster_for_strategy(strategy_name)
+            if cluster_id is not None:
+                if cluster_id not in cluster_allocations:
+                    cluster_allocations[cluster_id] = 0
+                cluster_allocations[cluster_id] += allocation
+
+        # Adjust allocations if cluster limits are exceeded
+        for cluster_id, total_allocation in cluster_allocations.items():
+            max_cluster_allocation = self.risk_limits['max_cluster_allocation']
+
+            if total_allocation > max_cluster_allocation:
+                # Scale down all strategies in this cluster proportionally
+                cluster = self.cluster_analyzer.strategy_clusters.get(cluster_id)
+                if cluster:
+                    scale_factor = max_cluster_allocation / total_allocation
+
+                    for strategy_name in cluster.strategy_names:
+                        if strategy_name in validated_allocations:
+                            validated_allocations[strategy_name] *= scale_factor
+
+                    logger.warning(f"Scaled down cluster {cluster_id} allocation by {scale_factor:.2f}")
+
+        # Check individual strategy limits
+        max_strategy_allocation = self.risk_limits['max_single_strategy']
+        for strategy_name, allocation in validated_allocations.items():
+            if allocation > max_strategy_allocation:
+                validated_allocations[strategy_name] = max_strategy_allocation
+                logger.warning(f"Capped {strategy_name} allocation at {max_strategy_allocation:.1%}")
+
+        return validated_allocations
+
+    def _calculate_portfolio_risk_metrics(self, allocations: Dict[str, float],
+                                        strategy_returns: Dict[str, pd.Series]) -> PortfolioRiskMetrics:
+        """Calculate comprehensive portfolio risk metrics."""
+        if not allocations or not strategy_returns:
+            return PortfolioRiskMetrics(
+                total_var=0, diversified_var=0, concentration_risk=0,
+                correlation_risk=0, cluster_concentrations={},
+                undiversified_var=0, diversification_ratio=1.0
+            )
+
+        # Align returns and get covariance matrix
+        aligned_returns = self.cluster_analyzer._align_returns(strategy_returns, 252)
+
+        if aligned_returns.empty:
+            return PortfolioRiskMetrics(
+                total_var=0, diversified_var=0, concentration_risk=0,
+                correlation_risk=0, cluster_concentrations={},
+                undiversified_var=0, diversification_ratio=1.0
+            )
+
+        # Filter to strategies with allocations
+        strategy_names = [name for name in allocations.keys() if name in aligned_returns.columns]
+        if not strategy_names:
+            return PortfolioRiskMetrics(
+                total_var=0, diversified_var=0, concentration_risk=0,
+                correlation_risk=0, cluster_concentrations={},
+                undiversified_var=0, diversification_ratio=1.0
+            )
+
+        strategy_returns_filtered = aligned_returns[strategy_names]
+        weights = np.array([allocations[name] for name in strategy_names])
+
+        # Calculate covariance matrix
+        cov_matrix = strategy_returns_filtered.cov().values * 252  # Annualize
+
+        # Portfolio variance
+        portfolio_var = np.dot(weights, np.dot(cov_matrix, weights))
+
+        # Undiversified variance (sum of individual variances)
+        individual_vars = np.diag(cov_matrix)
+        undiversified_var = np.dot(weights**2, individual_vars)
+
+        # Concentration risk (Herfindahl index)
+        concentration_risk = np.sum(weights**2)
+
+        # Correlation risk (portfolio var - undiversified var)
+        correlation_risk = portfolio_var - undiversified_var
+
+        # Diversification ratio
+        weighted_vol = np.dot(weights, np.sqrt(individual_vars))
+        portfolio_vol = np.sqrt(portfolio_var)
+        diversification_ratio = weighted_vol / portfolio_vol if portfolio_vol > 0 else 1.0
+
+        # Cluster concentrations
+        cluster_concentrations = self._calculate_cluster_exposures(allocations)
+
+        return PortfolioRiskMetrics(
+            total_var=portfolio_var,
+            diversified_var=portfolio_var,
+            concentration_risk=concentration_risk,
+            correlation_risk=correlation_risk,
+            cluster_concentrations=cluster_concentrations,
+            undiversified_var=undiversified_var,
+            diversification_ratio=diversification_ratio
+        )
+
+    def _calculate_risk_contributions(self, allocations: Dict[str, float],
+                                    strategy_returns: Dict[str, pd.Series]) -> List[RiskAllocation]:
+        """Calculate risk contributions for each strategy."""
+        risk_allocations = []
+
+        for strategy_name, allocation in allocations.items():
+            cluster_id = self.cluster_analyzer.get_cluster_for_strategy(strategy_name)
+
+            # Simplified risk contribution calculation
+            if strategy_name in strategy_returns:
+                strategy_vol = strategy_returns[strategy_name].std() * np.sqrt(252)
+                risk_contribution = allocation * strategy_vol
+
+                # Calculate correlation penalty
+                correlation_penalty = 0
+                if cluster_id and cluster_id in self.cluster_analyzer.strategy_clusters:
+                    cluster = self.cluster_analyzer.strategy_clusters[cluster_id]
+                    correlation_penalty = cluster.avg_correlation * allocation
+
+                risk_allocations.append(RiskAllocation(
+                    strategy_name=strategy_name,
+                    cluster_id=cluster_id or 0,
+                    position_size=allocation,
+                    risk_contribution=risk_contribution,
+                    marginal_var=risk_contribution,  # Simplified
+                    component_var=risk_contribution,  # Simplified
+                    correlation_penalty=correlation_penalty
+                ))
+
+        return risk_allocations
+
+    def _calculate_cluster_exposures(self, allocations: Dict[str, float]) -> Dict[int, float]:
+        """Calculate total exposure per cluster."""
+        cluster_exposures = {}
+
+        for strategy_name, allocation in allocations.items():
+            cluster_id = self.cluster_analyzer.get_cluster_for_strategy(strategy_name)
+            if cluster_id is not None:
+                if cluster_id not in cluster_exposures:
+                    cluster_exposures[cluster_id] = 0
+                cluster_exposures[cluster_id] += allocation
+
+        return cluster_exposures
+
+    def _generate_allocation_recommendations(self, original_allocations: Dict[str, float],
+                                           validated_allocations: Dict[str, float],
+                                           portfolio_risk: PortfolioRiskMetrics) -> List[str]:
+        """Generate allocation recommendations based on risk analysis."""
+        recommendations = []
+
+        # Check if allocations were adjusted
+        for strategy_name, original_alloc in original_allocations.items():
+            validated_alloc = validated_allocations.get(strategy_name, 0)
+            if abs(original_alloc - validated_alloc) > 0.01:  # 1% threshold
+                recommendations.append(
+                    f"Reduced {strategy_name} allocation from {original_alloc:.1%} "
+                    f"to {validated_alloc:.1%} due to risk constraints"
+                )
+
+        # Diversification recommendations
+        if portfolio_risk.diversification_ratio < self.risk_limits['min_diversification_ratio']:
+            recommendations.append(
+                f"Portfolio diversification ratio ({portfolio_risk.diversification_ratio:.2f}) "
+                f"below target ({self.risk_limits['min_diversification_ratio']:.2f}). "
+                "Consider adding uncorrelated strategies."
+            )
+
+        # Concentration warnings
+        if portfolio_risk.concentration_risk > 0.3:
+            recommendations.append(
+                f"High concentration risk ({portfolio_risk.concentration_risk:.2f}). "
+                "Consider more balanced allocation across strategies."
+            )
+
+        # Cluster concentration warnings
+        for cluster_id, exposure in portfolio_risk.cluster_concentrations.items():
+            if exposure > self.risk_limits['max_cluster_allocation']:
+                cluster = self.cluster_analyzer.strategy_clusters.get(cluster_id)
+                if cluster:
+                    recommendations.append(
+                        f"Cluster {cluster_id} exposure ({exposure:.1%}) exceeds limit "
+                        f"({self.risk_limits['max_cluster_allocation']:.1%}). "
+                        f"Strategies: {', '.join(cluster.strategy_names)}"
+                    )
+
+        if not recommendations:
+            recommendations.append("Allocation meets all risk constraints and diversification targets.")
+
+        return recommendations
+
+    def _calculate_risk_budget_utilization(self, allocations: Dict[str, float]) -> Dict[int, Dict[str, float]]:
+        """Calculate how much of each cluster's risk budget is utilized."""
+        utilization = {}
+
+        cluster_exposures = self._calculate_cluster_exposures(allocations)
+
+        for cluster_id, cluster in self.cluster_analyzer.strategy_clusters.items():
+            current_exposure = cluster_exposures.get(cluster_id, 0)
+            budget = cluster.risk_budget
+
+            utilization[cluster_id] = {
+                'budget': budget,
+                'used': current_exposure,
+                'available': max(0, budget - current_exposure),
+                'utilization_pct': current_exposure / budget if budget > 0 else 0
+            }
+
+        return utilization
+
+    def _check_constraint_violations(self, allocations: Dict[str, float]) -> List[str]:
+        """Check for any constraint violations."""
+        violations = []
+
+        # Check individual strategy limits
+        for strategy_name, allocation in allocations.items():
+            if allocation > self.risk_limits['max_single_strategy']:
+                violations.append(
+                    f"{strategy_name} allocation ({allocation:.1%}) exceeds "
+                    f"single strategy limit ({self.risk_limits['max_single_strategy']:.1%})"
+                )
+
+        # Check cluster limits
+        cluster_exposures = self._calculate_cluster_exposures(allocations)
+        for cluster_id, exposure in cluster_exposures.items():
+            if exposure > self.risk_limits['max_cluster_allocation']:
+                violations.append(
+                    f"Cluster {cluster_id} exposure ({exposure:.1%}) exceeds "
+                    f"cluster limit ({self.risk_limits['max_cluster_allocation']:.1%})"
+                )
+
+        return violations
+
+    def update_risk_limits(self, new_limits: Dict[str, float]):
+        """Update risk limits configuration."""
+        self.risk_limits.update(new_limits)
+        logger.info(f"Updated risk limits: {self.risk_limits}")
+
+    def get_risk_dashboard(self) -> Dict[str, Any]:
+        """Get comprehensive risk dashboard."""
+        return {
+            'cluster_summary': {
+                cluster_id: {
+                    'strategies': cluster.strategy_names,
+                    'avg_correlation': cluster.avg_correlation,
+                    'risk_budget': cluster.risk_budget,
+                    'current_allocation': cluster.current_allocation
+                }
+                for cluster_id, cluster in self.cluster_analyzer.strategy_clusters.items()
+            },
+            'risk_limits': self.risk_limits,
+            'current_allocations': self.current_allocations,
+            'last_correlation_update': self.last_update.isoformat() if self.last_update else None,
+            'portfolio_var': self.portfolio_var
+        }
\ No newline at end of file
Index: backend/validation/broker_accounting/daily_reconciliation.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/broker_accounting/daily_reconciliation.py b/backend/validation/broker_accounting/daily_reconciliation.py
new file mode 100644
--- /dev/null	(date 1758164780448)
+++ b/backend/validation/broker_accounting/daily_reconciliation.py	(date 1758164780448)
@@ -0,0 +1,629 @@
+"""Daily broker reconciliation system.
+
+Ensures positions, cash, and PnL match between internal ledger and broker
+statements within tolerance limits.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Tuple, Any
+import logging
+from datetime import datetime, timedelta
+from dataclasses import dataclass
+from enum import Enum
+import asyncio
+
+logger = logging.getLogger(__name__)
+
+
+class ReconciliationType(Enum):
+    """Types of reconciliation checks."""
+    POSITIONS = "positions"
+    CASH = "cash"
+    PNL = "pnl"
+    TRADES = "trades"
+    CORPORATE_ACTIONS = "corporate_actions"
+
+
+@dataclass
+class Position:
+    """Position record for reconciliation."""
+    symbol: str
+    quantity: int
+    market_value: float
+    avg_cost: float
+    unrealized_pnl: float
+    account_id: str
+    as_of_date: datetime
+
+
+@dataclass
+class ReconciliationItem:
+    """Individual reconciliation discrepancy."""
+    item_type: ReconciliationType
+    symbol: Optional[str]
+    internal_value: float
+    broker_value: float
+    difference: float
+    difference_pct: float
+    tolerance_exceeded: bool
+    description: str
+
+
+@dataclass
+class ReconciliationResult:
+    """Results of daily reconciliation."""
+    date: datetime
+    account_id: str
+    passed: bool
+    total_discrepancies: int
+    critical_discrepancies: int
+    total_difference_abs: float
+    items: List[ReconciliationItem]
+    summary: Dict[str, Any]
+
+
+class BrokerInterface:
+    """Interface for broker API calls."""
+
+    def __init__(self, broker_name: str, api_config: Dict[str, Any]):
+        self.broker_name = broker_name
+        self.api_config = api_config
+        self.connected = False
+
+    async def connect(self) -> bool:
+        """Connect to broker API."""
+        try:
+            # In production, this would establish actual API connection
+            logger.info(f"Connecting to {self.broker_name}")
+            self.connected = True
+            return True
+        except Exception as e:
+            logger.error(f"Failed to connect to {self.broker_name}: {e}")
+            return False
+
+    async def get_positions(self, account_id: str) -> List[Position]:
+        """Get current positions from broker."""
+        if not self.connected:
+            raise RuntimeError("Not connected to broker")
+
+        # In production, this would make actual API calls
+        # For simulation, return sample data
+        sample_positions = [
+            Position(
+                symbol="SPY",
+                quantity=100,
+                market_value=45000.0,
+                avg_cost=440.0,
+                unrealized_pnl=1000.0,
+                account_id=account_id,
+                as_of_date=datetime.now()
+            ),
+            Position(
+                symbol="QQQ",
+                quantity=50,
+                market_value=17500.0,
+                avg_cost=340.0,
+                unrealized_pnl=500.0,
+                account_id=account_id,
+                as_of_date=datetime.now()
+            )
+        ]
+
+        logger.info(f"Retrieved {len(sample_positions)} positions from {self.broker_name}")
+        return sample_positions
+
+    async def get_cash_balance(self, account_id: str) -> Dict[str, float]:
+        """Get cash balances from broker."""
+        if not self.connected:
+            raise RuntimeError("Not connected to broker")
+
+        # Sample cash balance data
+        return {
+            'total_cash': 25000.0,
+            'buying_power': 50000.0,
+            'settled_cash': 24000.0,
+            'unsettled_cash': 1000.0
+        }
+
+    async def get_account_summary(self, account_id: str) -> Dict[str, float]:
+        """Get account summary from broker."""
+        if not self.connected:
+            raise RuntimeError("Not connected to broker")
+
+        return {
+            'net_liquidation_value': 87500.0,
+            'total_cash_value': 25000.0,
+            'stock_market_value': 62500.0,
+            'unrealized_pnl': 1500.0,
+            'realized_pnl': 2500.0
+        }
+
+    async def get_trade_history(self, account_id: str, date: datetime) -> List[Dict[str, Any]]:
+        """Get trade history for a specific date."""
+        if not self.connected:
+            raise RuntimeError("Not connected to broker")
+
+        # Sample trade data
+        return [
+            {
+                'symbol': 'SPY',
+                'side': 'buy',
+                'quantity': 10,
+                'price': 450.0,
+                'timestamp': date.replace(hour=10, minute=30),
+                'commission': 0.5,
+                'trade_id': 'T123456'
+            }
+        ]
+
+
+class InternalLedger:
+    """Internal position and PnL tracking system."""
+
+    def __init__(self):
+        self.positions = {}  # symbol -> Position
+        self.cash_balance = 0.0
+        self.trade_history = []
+        self.pnl_history = []
+
+    def get_positions(self, account_id: str) -> List[Position]:
+        """Get current positions from internal ledger."""
+        return list(self.positions.values())
+
+    def get_cash_balance(self, account_id: str) -> Dict[str, float]:
+        """Get cash balance from internal ledger."""
+        return {
+            'total_cash': self.cash_balance,
+            'buying_power': self.cash_balance * 2,  # 2:1 margin
+            'settled_cash': self.cash_balance * 0.95,
+            'unsettled_cash': self.cash_balance * 0.05
+        }
+
+    def get_account_summary(self, account_id: str) -> Dict[str, float]:
+        """Get account summary from internal ledger."""
+        total_stock_value = sum(pos.market_value for pos in self.positions.values())
+        total_unrealized = sum(pos.unrealized_pnl for pos in self.positions.values())
+
+        return {
+            'net_liquidation_value': self.cash_balance + total_stock_value,
+            'total_cash_value': self.cash_balance,
+            'stock_market_value': total_stock_value,
+            'unrealized_pnl': total_unrealized,
+            'realized_pnl': 2400.0  # Sample value
+        }
+
+    def update_position(self, symbol: str, quantity: int, price: float):
+        """Update position in internal ledger."""
+        if symbol not in self.positions:
+            self.positions[symbol] = Position(
+                symbol=symbol,
+                quantity=0,
+                market_value=0.0,
+                avg_cost=0.0,
+                unrealized_pnl=0.0,
+                account_id="internal",
+                as_of_date=datetime.now()
+            )
+
+        position = self.positions[symbol]
+        old_quantity = position.quantity
+
+        # Update quantity
+        position.quantity += quantity
+
+        # Update average cost (simplified)
+        if position.quantity != 0:
+            total_cost = (old_quantity * position.avg_cost) + (quantity * price)
+            position.avg_cost = total_cost / position.quantity
+
+        # Update market value and unrealized PnL
+        current_price = price  # In production, would get current market price
+        position.market_value = position.quantity * current_price
+        position.unrealized_pnl = (current_price - position.avg_cost) * position.quantity
+        position.as_of_date = datetime.now()
+
+    def initialize_sample_data(self):
+        """Initialize with sample data for testing."""
+        self.cash_balance = 24500.0  # Slightly different from broker
+
+        # Add sample positions (slightly different from broker)
+        self.update_position("SPY", 100, 441.0)  # Different avg cost
+        self.update_position("QQQ", 50, 339.0)   # Different avg cost
+
+
+class DailyReconciler:
+    """Daily reconciliation engine."""
+
+    def __init__(self, broker_interface: BrokerInterface, internal_ledger: InternalLedger):
+        self.broker = broker_interface
+        self.ledger = internal_ledger
+        self.tolerances = {
+            'position_quantity': 0,  # Zero tolerance for position quantity
+            'position_value_pct': 0.001,  # 0.1% tolerance for position values
+            'cash_pct': 0.002,  # 0.2% tolerance for cash
+            'pnl_pct': 0.005,  # 0.5% tolerance for PnL
+            'total_value_pct': 0.001  # 0.1% tolerance for total account value
+        }
+
+    async def run_daily_reconciliation(self, account_id: str) -> ReconciliationResult:
+        """Run complete daily reconciliation."""
+        logger.info(f"Starting daily reconciliation for account {account_id}")
+
+        reconciliation_items = []
+
+        try:
+            # Ensure broker connection
+            if not self.broker.connected:
+                await self.broker.connect()
+
+            # Reconcile positions
+            position_items = await self._reconcile_positions(account_id)
+            reconciliation_items.extend(position_items)
+
+            # Reconcile cash
+            cash_items = await self._reconcile_cash(account_id)
+            reconciliation_items.extend(cash_items)
+
+            # Reconcile account summary
+            summary_items = await self._reconcile_account_summary(account_id)
+            reconciliation_items.extend(summary_items)
+
+            # Check tolerance violations
+            critical_discrepancies = [item for item in reconciliation_items if item.tolerance_exceeded]
+
+            # Determine overall pass/fail
+            passed = len(critical_discrepancies) == 0
+
+            # Create summary
+            summary = self._create_summary(reconciliation_items)
+
+            result = ReconciliationResult(
+                date=datetime.now(),
+                account_id=account_id,
+                passed=passed,
+                total_discrepancies=len(reconciliation_items),
+                critical_discrepancies=len(critical_discrepancies),
+                total_difference_abs=sum(abs(item.difference) for item in reconciliation_items),
+                items=reconciliation_items,
+                summary=summary
+            )
+
+            logger.info(f"Reconciliation completed: {result.passed}, "
+                       f"{result.total_discrepancies} discrepancies, "
+                       f"{result.critical_discrepancies} critical")
+
+            return result
+
+        except Exception as e:
+            logger.error(f"Reconciliation failed: {e}")
+            # Return failed result
+            return ReconciliationResult(
+                date=datetime.now(),
+                account_id=account_id,
+                passed=False,
+                total_discrepancies=0,
+                critical_discrepancies=1,
+                total_difference_abs=0,
+                items=[],
+                summary={'error': str(e)}
+            )
+
+    async def _reconcile_positions(self, account_id: str) -> List[ReconciliationItem]:
+        """Reconcile position holdings."""
+        items = []
+
+        # Get positions from both sources
+        broker_positions = await self.broker.get_positions(account_id)
+        internal_positions = self.ledger.get_positions(account_id)
+
+        # Create lookup dictionaries
+        broker_pos_dict = {pos.symbol: pos for pos in broker_positions}
+        internal_pos_dict = {pos.symbol: pos for pos in internal_positions}
+
+        # Get all symbols
+        all_symbols = set(broker_pos_dict.keys()) | set(internal_pos_dict.keys())
+
+        for symbol in all_symbols:
+            broker_pos = broker_pos_dict.get(symbol)
+            internal_pos = internal_pos_dict.get(symbol)
+
+            # Check quantity differences
+            broker_qty = broker_pos.quantity if broker_pos else 0
+            internal_qty = internal_pos.quantity if internal_pos else 0
+
+            if broker_qty != internal_qty:
+                items.append(ReconciliationItem(
+                    item_type=ReconciliationType.POSITIONS,
+                    symbol=symbol,
+                    internal_value=internal_qty,
+                    broker_value=broker_qty,
+                    difference=internal_qty - broker_qty,
+                    difference_pct=0,  # Quantity differences are absolute
+                    tolerance_exceeded=abs(internal_qty - broker_qty) > self.tolerances['position_quantity'],
+                    description=f"Position quantity mismatch for {symbol}"
+                ))
+
+            # Check market value differences (if both positions exist)
+            if broker_pos and internal_pos:
+                broker_value = broker_pos.market_value
+                internal_value = internal_pos.market_value
+
+                if broker_value != 0:
+                    value_diff_pct = abs(internal_value - broker_value) / abs(broker_value)
+                else:
+                    value_diff_pct = 1.0 if internal_value != 0 else 0.0
+
+                if value_diff_pct > self.tolerances['position_value_pct']:
+                    items.append(ReconciliationItem(
+                        item_type=ReconciliationType.POSITIONS,
+                        symbol=symbol,
+                        internal_value=internal_value,
+                        broker_value=broker_value,
+                        difference=internal_value - broker_value,
+                        difference_pct=value_diff_pct * 100,
+                        tolerance_exceeded=True,
+                        description=f"Position value mismatch for {symbol}"
+                    ))
+
+        return items
+
+    async def _reconcile_cash(self, account_id: str) -> List[ReconciliationItem]:
+        """Reconcile cash balances."""
+        items = []
+
+        broker_cash = await self.broker.get_cash_balance(account_id)
+        internal_cash = self.ledger.get_cash_balance(account_id)
+
+        # Compare total cash
+        broker_total = broker_cash['total_cash']
+        internal_total = internal_cash['total_cash']
+
+        if broker_total != 0:
+            cash_diff_pct = abs(internal_total - broker_total) / abs(broker_total)
+        else:
+            cash_diff_pct = 1.0 if internal_total != 0 else 0.0
+
+        if cash_diff_pct > self.tolerances['cash_pct']:
+            items.append(ReconciliationItem(
+                item_type=ReconciliationType.CASH,
+                symbol=None,
+                internal_value=internal_total,
+                broker_value=broker_total,
+                difference=internal_total - broker_total,
+                difference_pct=cash_diff_pct * 100,
+                tolerance_exceeded=True,
+                description="Total cash balance mismatch"
+            ))
+
+        # Compare buying power
+        broker_bp = broker_cash.get('buying_power', 0)
+        internal_bp = internal_cash.get('buying_power', 0)
+
+        if broker_bp != 0:
+            bp_diff_pct = abs(internal_bp - broker_bp) / abs(broker_bp)
+            if bp_diff_pct > self.tolerances['cash_pct'] * 2:  # More lenient for buying power
+                items.append(ReconciliationItem(
+                    item_type=ReconciliationType.CASH,
+                    symbol=None,
+                    internal_value=internal_bp,
+                    broker_value=broker_bp,
+                    difference=internal_bp - broker_bp,
+                    difference_pct=bp_diff_pct * 100,
+                    tolerance_exceeded=True,
+                    description="Buying power mismatch"
+                ))
+
+        return items
+
+    async def _reconcile_account_summary(self, account_id: str) -> List[ReconciliationItem]:
+        """Reconcile account-level summary values."""
+        items = []
+
+        broker_summary = await self.broker.get_account_summary(account_id)
+        internal_summary = self.ledger.get_account_summary(account_id)
+
+        # Key metrics to reconcile
+        metrics_to_check = [
+            ('net_liquidation_value', 'total_value_pct', "Net liquidation value mismatch"),
+            ('stock_market_value', 'position_value_pct', "Total stock value mismatch"),
+            ('unrealized_pnl', 'pnl_pct', "Unrealized PnL mismatch"),
+            ('realized_pnl', 'pnl_pct', "Realized PnL mismatch")
+        ]
+
+        for metric, tolerance_key, description in metrics_to_check:
+            broker_value = broker_summary.get(metric, 0)
+            internal_value = internal_summary.get(metric, 0)
+
+            if broker_value != 0:
+                diff_pct = abs(internal_value - broker_value) / abs(broker_value)
+            else:
+                diff_pct = 1.0 if internal_value != 0 else 0.0
+
+            if diff_pct > self.tolerances[tolerance_key]:
+                items.append(ReconciliationItem(
+                    item_type=ReconciliationType.PNL,
+                    symbol=None,
+                    internal_value=internal_value,
+                    broker_value=broker_value,
+                    difference=internal_value - broker_value,
+                    difference_pct=diff_pct * 100,
+                    tolerance_exceeded=True,
+                    description=description
+                ))
+
+        return items
+
+    def _create_summary(self, items: List[ReconciliationItem]) -> Dict[str, Any]:
+        """Create reconciliation summary."""
+        summary = {
+            'total_items': len(items),
+            'by_type': {},
+            'critical_items': [],
+            'largest_differences': []
+        }
+
+        # Group by type
+        for item in items:
+            item_type = item.item_type.value
+            if item_type not in summary['by_type']:
+                summary['by_type'][item_type] = {
+                    'count': 0,
+                    'total_difference': 0,
+                    'critical_count': 0
+                }
+
+            summary['by_type'][item_type]['count'] += 1
+            summary['by_type'][item_type]['total_difference'] += abs(item.difference)
+            if item.tolerance_exceeded:
+                summary['by_type'][item_type]['critical_count'] += 1
+
+        # Critical items
+        summary['critical_items'] = [
+            {
+                'type': item.item_type.value,
+                'symbol': item.symbol,
+                'difference': item.difference,
+                'difference_pct': item.difference_pct,
+                'description': item.description
+            }
+            for item in items if item.tolerance_exceeded
+        ]
+
+        # Largest differences
+        sorted_items = sorted(items, key=lambda x: abs(x.difference), reverse=True)
+        summary['largest_differences'] = [
+            {
+                'type': item.item_type.value,
+                'symbol': item.symbol,
+                'difference': item.difference,
+                'description': item.description
+            }
+            for item in sorted_items[:5]
+        ]
+
+        return summary
+
+    def update_tolerances(self, new_tolerances: Dict[str, float]):
+        """Update reconciliation tolerances."""
+        self.tolerances.update(new_tolerances)
+        logger.info(f"Updated reconciliation tolerances: {self.tolerances}")
+
+
+class ReconciliationReporter:
+    """Generates reconciliation reports and alerts."""
+
+    def __init__(self):
+        self.report_history = []
+
+    def generate_report(self, result: ReconciliationResult) -> str:
+        """Generate human-readable reconciliation report."""
+        report = f"""
+=== DAILY RECONCILIATION REPORT ===
+Date: {result.date.strftime('%Y-%m-%d %H:%M:%S')}
+Account: {result.account_id}
+Status: {'PASSED' if result.passed else 'FAILED'}
+
+SUMMARY:
+- Total Discrepancies: {result.total_discrepancies}
+- Critical Discrepancies: {result.critical_discrepancies}
+- Total Absolute Difference: ${result.total_difference_abs:,.2f}
+
+"""
+
+        if result.critical_discrepancies > 0:
+            report += "ðŸš¨ CRITICAL DISCREPANCIES:\n"
+            for item in result.items:
+                if item.tolerance_exceeded:
+                    symbol_str = f" ({item.symbol})" if item.symbol else ""
+                    report += f"- {item.item_type.value.upper()}{symbol_str}: "
+                    report += f"Internal=${item.internal_value:,.2f}, "
+                    report += f"Broker=${item.broker_value:,.2f}, "
+                    report += f"Diff=${item.difference:,.2f}"
+                    if item.difference_pct > 0:
+                        report += f" ({item.difference_pct:.2f}%)"
+                    report += f"\n  {item.description}\n"
+
+        if result.summary and 'by_type' in result.summary:
+            report += "\nBREAKDOWN BY TYPE:\n"
+            for item_type, type_summary in result.summary['by_type'].items():
+                report += f"- {item_type.upper()}: {type_summary['count']} items, "
+                report += f"{type_summary['critical_count']} critical, "
+                report += f"${type_summary['total_difference']:,.2f} total difference\n"
+
+        if not result.passed:
+            report += "\nâš ï¸  RECOMMENDED ACTIONS:\n"
+            report += "1. Halt automated trading until discrepancies resolved\n"
+            report += "2. Review trade execution logs for missing/duplicate trades\n"
+            report += "3. Check for pending corporate actions or settlements\n"
+            report += "4. Contact broker support if discrepancies persist\n"
+
+        return report
+
+    def should_halt_trading(self, result: ReconciliationResult) -> bool:
+        """Determine if trading should be halted based on reconciliation results."""
+        if not result.passed:
+            return True
+
+        # Additional halt conditions
+        if result.total_difference_abs > 10000:  # $10k total difference
+            return True
+
+        # Check for position quantity mismatches (always critical)
+        for item in result.items:
+            if (item.item_type == ReconciliationType.POSITIONS and
+                item.symbol and
+                abs(item.difference) > 0 and
+                item.description.lower().contains('quantity')):
+                return True
+
+        return False
+
+    def create_alert(self, result: ReconciliationResult) -> Dict[str, Any]:
+        """Create alert for failed reconciliation."""
+        severity = "critical" if self.should_halt_trading(result) else "warning"
+
+        return {
+            'timestamp': result.date,
+            'severity': severity,
+            'title': f"Reconciliation {'FAILED' if not result.passed else 'WARNING'}",
+            'message': f"Account {result.account_id} has {result.critical_discrepancies} critical discrepancies",
+            'should_halt_trading': self.should_halt_trading(result),
+            'account_id': result.account_id,
+            'discrepancy_count': result.critical_discrepancies,
+            'total_difference': result.total_difference_abs
+        }
+
+    def store_result(self, result: ReconciliationResult):
+        """Store reconciliation result for historical analysis."""
+        self.report_history.append(result)
+
+        # Keep only last 30 days of results
+        cutoff_date = datetime.now() - timedelta(days=30)
+        self.report_history = [
+            r for r in self.report_history
+            if r.date >= cutoff_date
+        ]
+
+    def get_reconciliation_trends(self, days: int = 7) -> Dict[str, Any]:
+        """Get reconciliation trends over specified period."""
+        cutoff_date = datetime.now() - timedelta(days=days)
+        recent_results = [
+            r for r in self.report_history
+            if r.date >= cutoff_date
+        ]
+
+        if not recent_results:
+            return {'error': 'No recent reconciliation data'}
+
+        trends = {
+            'period_days': days,
+            'total_reconciliations': len(recent_results),
+            'passed_rate': sum(1 for r in recent_results if r.passed) / len(recent_results),
+            'avg_discrepancies': np.mean([r.total_discrepancies for r in recent_results]),
+            'avg_critical_discrepancies': np.mean([r.critical_discrepancies for r in recent_results]),
+            'avg_difference_abs': np.mean([r.total_difference_abs for r in recent_results]),
+            'trend_direction': 'improving'  # Would calculate actual trend
+        }
+
+        return trends
\ No newline at end of file
Index: backend/validation/drift_monitor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/drift_monitor.py b/backend/validation/drift_monitor.py
new file mode 100644
--- /dev/null	(date 1758169658985)
+++ b/backend/validation/drift_monitor.py	(date 1758169658985)
@@ -0,0 +1,431 @@
+"""
+Live Drift Detection and Monitoring
+Implements CUSUM and PSR (Probabilistic Sharpe Ratio) for detecting
+when live performance deviates from backtest expectations.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Tuple, Any
+from dataclasses import dataclass, field
+from datetime import datetime, timedelta
+import logging
+from scipy import stats
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class DriftAlert:
+    """Drift detection alert."""
+    timestamp: datetime
+    alert_type: str  # 'cusum', 'psr', 'performance'
+    severity: str    # 'warning', 'critical'
+    message: str
+    current_value: float
+    threshold: float
+    recommended_action: str
+
+
+@dataclass
+class DriftMetrics:
+    """Current drift metrics."""
+    timestamp: datetime
+    cusum_positive: float
+    cusum_negative: float
+    psr_value: float
+    performance_deviation: float
+    days_since_reset: int
+    alert_active: bool
+
+
+class CUSUMDrift:
+    """
+    CUSUM (Cumulative Sum) drift detector for monitoring
+    when live performance deviates from expected.
+    """
+    
+    def __init__(self, k: float = 0.0, h: float = 3.0, reset_threshold: float = 5.0):
+        """
+        Initialize CUSUM detector.
+        
+        Args:
+            k: Reference value (typically 0 for zero-mean)
+            h: Decision threshold
+            reset_threshold: Threshold for resetting after alarm
+        """
+        self.k = k
+        self.h = h
+        self.reset_threshold = reset_threshold
+        self.gp = 0.0  # Positive CUSUM
+        self.gn = 0.0  # Negative CUSUM
+        self.last_reset = datetime.now()
+        self.alarm_count = 0
+        
+    def update(self, x: float) -> Tuple[bool, DriftAlert]:
+        """
+        Update CUSUM with new observation.
+        
+        Args:
+            x: New observation (e.g., daily edge: live_ret - modeled_ret)
+            
+        Returns:
+            Tuple of (alarm_triggered, alert_object)
+        """
+        # Update CUSUM statistics
+        self.gp = max(0.0, self.gp + (x - self.k))
+        self.gn = min(0.0, self.gn + (x + self.k))
+        
+        # Check for alarm
+        alarm = (self.gp > self.h) or (self.gn < -self.h)
+        
+        alert = None
+        if alarm:
+            self.alarm_count += 1
+            
+            # Determine severity and message
+            if abs(self.gp) > abs(self.gn):
+                severity = 'critical' if self.gp > self.h * 1.5 else 'warning'
+                message = f"Positive drift detected: CUSUM+ = {self.gp:.3f} > {self.h}"
+                recommended_action = "Reduce position size by 50% and investigate"
+            else:
+                severity = 'critical' if self.gn < -self.h * 1.5 else 'warning'
+                message = f"Negative drift detected: CUSUM- = {self.gn:.3f} < {-self.h}"
+                recommended_action = "Reduce position size by 50% and investigate"
+            
+            alert = DriftAlert(
+                timestamp=datetime.now(),
+                alert_type='cusum',
+                severity=severity,
+                message=message,
+                current_value=max(abs(self.gp), abs(self.gn)),
+                threshold=self.h,
+                recommended_action=recommended_action
+            )
+            
+            # Reset CUSUM after alarm
+            self.gp = self.gn = 0.0
+            self.last_reset = datetime.now()
+            
+        return alarm, alert
+    
+    def get_current_metrics(self) -> DriftMetrics:
+        """Get current CUSUM metrics."""
+        days_since_reset = (datetime.now() - self.last_reset).days
+        
+        return DriftMetrics(
+            timestamp=datetime.now(),
+            cusum_positive=self.gp,
+            cusum_negative=self.gn,
+            psr_value=0.0,  # Not applicable for CUSUM
+            performance_deviation=max(abs(self.gp), abs(self.gn)),
+            days_since_reset=days_since_reset,
+            alert_active=self.gp > self.h * 0.8 or self.gn < -self.h * 0.8
+        )
+
+
+class PSRDrift:
+    """
+    Probabilistic Sharpe Ratio (PSR) drift detector.
+    Monitors when live Sharpe ratio deviates significantly from backtest.
+    """
+    
+    def __init__(self, backtest_sharpe: float, backtest_observations: int,
+                 confidence_level: float = 0.95, min_observations: int = 30):
+        """
+        Initialize PSR detector.
+        
+        Args:
+            backtest_sharpe: Expected Sharpe ratio from backtest
+            backtest_observations: Number of observations in backtest
+            confidence_level: Confidence level for PSR calculation
+            min_observations: Minimum observations before PSR calculation
+        """
+        self.backtest_sharpe = backtest_sharpe
+        self.backtest_observations = backtest_observations
+        self.confidence_level = confidence_level
+        self.min_observations = min_observations
+        
+        self.live_returns = []
+        self.psr_history = []
+        
+    def update(self, live_return: float) -> Tuple[bool, Optional[DriftAlert]]:
+        """
+        Update PSR with new live return.
+        
+        Args:
+            live_return: New live return observation
+            
+        Returns:
+            Tuple of (alarm_triggered, alert_object)
+        """
+        self.live_returns.append(live_return)
+        
+        if len(self.live_returns) < self.min_observations:
+            return False, None
+            
+        # Calculate live Sharpe ratio
+        live_sharpe = self._calculate_sharpe_ratio(self.live_returns)
+        
+        # Calculate PSR
+        psr = self._calculate_psr(live_sharpe, len(self.live_returns))
+        self.psr_history.append(psr)
+        
+        # Check for significant deviation
+        threshold = 1 - self.confidence_level
+        alarm = psr < threshold
+        
+        alert = None
+        if alarm:
+            severity = 'critical' if psr < threshold * 0.5 else 'warning'
+            message = f"PSR drift detected: PSR = {psr:.3f} < {threshold:.3f}"
+            recommended_action = "Reduce position size and re-evaluate strategy"
+            
+            alert = DriftAlert(
+                timestamp=datetime.now(),
+                alert_type='psr',
+                severity=severity,
+                message=message,
+                current_value=psr,
+                threshold=threshold,
+                recommended_action=recommended_action
+            )
+            
+        return alarm, alert
+    
+    def _calculate_sharpe_ratio(self, returns: List[float]) -> float:
+        """Calculate Sharpe ratio from returns."""
+        if len(returns) < 2:
+            return 0.0
+            
+        returns_array = np.array(returns)
+        mean_return = np.mean(returns_array)
+        std_return = np.std(returns_array, ddof=1)
+        
+        if std_return == 0:
+            return 0.0
+            
+        return mean_return / std_return * np.sqrt(252)  # Annualized
+    
+    def _calculate_psr(self, live_sharpe: float, n_observations: int) -> float:
+        """
+        Calculate Probabilistic Sharpe Ratio.
+        
+        PSR = Prob(SR_live > SR_backtest | observed data)
+        """
+        # Estimate moments of Sharpe ratio distribution
+        # Using approximation from Bailey & LÃ³pez de Prado
+        
+        # Skewness and kurtosis estimates (simplified)
+        skew = 0.0  # Assume normal for simplicity
+        kurt = 3.0  # Normal kurtosis
+        
+        # Calculate PSR using normal approximation
+        z_score = (live_sharpe - self.backtest_sharpe) / np.sqrt(1 + skew * live_sharpe + (kurt - 1) / 4 * live_sharpe**2)
+        
+        # Convert to probability
+        psr = stats.norm.cdf(z_score)
+        
+        return psr
+
+
+class PerformanceDriftMonitor:
+    """
+    Comprehensive performance drift monitoring system.
+    Combines multiple drift detection methods.
+    """
+    
+    def __init__(self, backtest_metrics: Dict[str, float], 
+                 drift_thresholds: Optional[Dict[str, float]] = None):
+        """
+        Initialize performance drift monitor.
+        
+        Args:
+            backtest_metrics: Expected metrics from backtest
+            drift_thresholds: Custom drift thresholds
+        """
+        self.backtest_metrics = backtest_metrics
+        self.drift_thresholds = drift_thresholds or {
+            'sharpe_deviation': 0.5,
+            'return_deviation': 0.02,
+            'volatility_deviation': 0.01,
+            'max_drawdown_deviation': 0.05
+        }
+        
+        # Initialize detectors
+        self.cusum_detector = CUSUMDrift()
+        self.psr_detector = PSRDrift(
+            backtest_sharpe=backtest_metrics.get('sharpe_ratio', 1.0),
+            backtest_observations=backtest_metrics.get('observations', 252)
+        )
+        
+        self.alerts_history = []
+        self.performance_history = []
+        
+    def update(self, live_return: float, additional_metrics: Optional[Dict[str, float]] = None) -> List[DriftAlert]:
+        """
+        Update all drift detectors with new live data.
+        
+        Args:
+            live_return: New live return
+            additional_metrics: Additional performance metrics
+            
+        Returns:
+            List of triggered alerts
+        """
+        alerts = []
+        
+        # Update CUSUM (using return deviation from expected)
+        expected_return = self.backtest_metrics.get('daily_return', 0.0)
+        return_deviation = live_return - expected_return
+        
+        cusum_alarm, cusum_alert = self.cusum_detector.update(return_deviation)
+        if cusum_alarm and cusum_alert:
+            alerts.append(cusum_alert)
+            
+        # Update PSR
+        psr_alarm, psr_alert = self.psr_detector.update(live_return)
+        if psr_alarm and psr_alert:
+            alerts.append(psr_alert)
+            
+        # Check additional performance metrics
+        if additional_metrics:
+            perf_alerts = self._check_performance_metrics(additional_metrics)
+            alerts.extend(perf_alerts)
+            
+        # Store alerts and performance data
+        self.alerts_history.extend(alerts)
+        self.performance_history.append({
+            'timestamp': datetime.now(),
+            'live_return': live_return,
+            'return_deviation': return_deviation,
+            'alerts_count': len(alerts)
+        })
+        
+        return alerts
+    
+    def _check_performance_metrics(self, metrics: Dict[str, float]) -> List[DriftAlert]:
+        """Check additional performance metrics for drift."""
+        alerts = []
+        
+        for metric_name, current_value in metrics.items():
+            expected_value = self.backtest_metrics.get(metric_name, 0.0)
+            threshold = self.drift_thresholds.get(f'{metric_name}_deviation', 0.1)
+            
+            deviation = abs(current_value - expected_value)
+            
+            if deviation > threshold:
+                severity = 'critical' if deviation > threshold * 2 else 'warning'
+                message = f"{metric_name} drift: {current_value:.4f} vs expected {expected_value:.4f}"
+                
+                alert = DriftAlert(
+                    timestamp=datetime.now(),
+                    alert_type='performance',
+                    severity=severity,
+                    message=message,
+                    current_value=current_value,
+                    threshold=expected_value + threshold,
+                    recommended_action=f"Monitor {metric_name} closely"
+                )
+                alerts.append(alert)
+                
+        return alerts
+    
+    def get_drift_summary(self, days: int = 30) -> Dict[str, Any]:
+        """Get drift monitoring summary."""
+        cutoff_date = datetime.now() - timedelta(days=days)
+        
+        recent_alerts = [a for a in self.alerts_history if a.timestamp >= cutoff_date]
+        recent_performance = [p for p in self.performance_history if p['timestamp'] >= cutoff_date]
+        
+        # Count alerts by type and severity
+        alert_counts = {}
+        for alert in recent_alerts:
+            key = f"{alert.alert_type}_{alert.severity}"
+            alert_counts[key] = alert_counts.get(key, 0) + 1
+            
+        # Calculate performance statistics
+        if recent_performance:
+            returns = [p['live_return'] for p in recent_performance]
+            avg_return = np.mean(returns)
+            volatility = np.std(returns)
+            sharpe = avg_return / volatility * np.sqrt(252) if volatility > 0 else 0
+        else:
+            avg_return = volatility = sharpe = 0
+            
+        return {
+            'monitoring_period_days': days,
+            'total_alerts': len(recent_alerts),
+            'alert_counts': alert_counts,
+            'performance_summary': {
+                'avg_daily_return': avg_return,
+                'volatility': volatility,
+                'sharpe_ratio': sharpe,
+                'observations': len(recent_performance)
+            },
+            'cusum_status': self.cusum_detector.get_current_metrics(),
+            'drift_detected': len(recent_alerts) > 0,
+            'recommendation': self._get_recommendation(recent_alerts)
+        }
+    
+    def _get_recommendation(self, recent_alerts: List[DriftAlert]) -> str:
+        """Get recommendation based on recent alerts."""
+        if not recent_alerts:
+            return "Continue monitoring - no drift detected"
+            
+        critical_alerts = [a for a in recent_alerts if a.severity == 'critical']
+        if critical_alerts:
+            return "CRITICAL: Reduce position size by 50% and investigate immediately"
+            
+        warning_alerts = [a for a in recent_alerts if a.severity == 'warning']
+        if len(warning_alerts) >= 3:
+            return "WARNING: Multiple drift alerts - consider reducing position size"
+            
+        return "Monitor closely - drift alerts detected"
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    def test_drift_monitoring():
+        """Test the drift monitoring system."""
+        print("=== Drift Monitoring Test ===")
+        
+        # Simulate backtest metrics
+        backtest_metrics = {
+            'sharpe_ratio': 1.5,
+            'daily_return': 0.001,
+            'volatility': 0.02,
+            'max_drawdown': 0.10,
+            'observations': 252
+        }
+        
+        # Initialize monitor
+        monitor = PerformanceDriftMonitor(backtest_metrics)
+        
+        # Simulate live trading with some drift
+        np.random.seed(42)
+        
+        print("Simulating live trading...")
+        for day in range(50):
+            # Simulate returns - start normal, then introduce drift
+            if day < 30:
+                live_return = np.random.normal(0.001, 0.02)
+            else:
+                # Introduce negative drift
+                live_return = np.random.normal(-0.002, 0.025)
+                
+            alerts = monitor.update(live_return)
+            
+            if alerts:
+                print(f"Day {day}: {len(alerts)} alerts")
+                for alert in alerts:
+                    print(f"  {alert.severity.upper()}: {alert.message}")
+        
+        # Get summary
+        summary = monitor.get_drift_summary(days=30)
+        print("\nDrift Summary:")
+        print(f"Total alerts: {summary['total_alerts']}")
+        print(f"Alert counts: {summary['alert_counts']}")
+        print(f"Recommendation: {summary['recommendation']}")
+    
+    test_drift_monitoring()
Index: backend/validation/ensemble_evaluator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/ensemble_evaluator.py b/backend/validation/ensemble_evaluator.py
new file mode 100644
--- /dev/null	(date 1758168088188)
+++ b/backend/validation/ensemble_evaluator.py	(date 1758168088188)
@@ -0,0 +1,121 @@
+"""Ensemble & Correlation Analysis with Fixed Penalty + Real Portfolio."""
+
+import pandas as pd
+import numpy as np
+from typing import Dict, List, Tuple, Any
+from scipy.optimize import minimize
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+def _sharpe(r: pd.Series) -> float:
+    """Calculate Sharpe ratio safely."""
+    r = r.dropna()
+    if len(r) < 2 or r.std(ddof=1) == 0:
+        return 0.0
+    return float(np.sqrt(252) * r.mean() / r.std(ddof=1))
+
+
+class EnsembleValidator:
+    """Validates ensemble strategies with correlation analysis."""
+    
+    def __init__(self, corr_threshold: float = 0.7):
+        self.corr_threshold = corr_threshold
+
+    def analyze_strategy_correlations(self, strategy_returns_dict: Dict[str, pd.Series]) -> Dict[str, Any]:
+        """Analyze correlations between strategies and optimize ensemble."""
+        df = pd.DataFrame(strategy_returns_dict).dropna(how='all')
+        df = df.dropna()  # strict align
+        
+        if df.empty or df.shape[1] < 2:
+            return {
+                'correlation_matrix': pd.DataFrame(),
+                'strategy_clusters': [],
+                'redundant_strategies': [],
+                'ensemble_performance': {},
+                'diversification_ratio': 0.0
+            }
+
+        corr = df.corr()
+        redundant = self._find_redundant(corr)
+
+        # Equal weight portfolio
+        ew = np.ones(df.shape[1]) / df.shape[1]
+        ew_series = (df * ew).sum(axis=1)
+
+        # Optimized portfolio
+        opt = self._optimize_weights(df)
+        portfolio_series = (df * opt['weights']).sum(axis=1)
+
+        return {
+            'correlation_matrix': corr,
+            'strategy_clusters': self._clusters_from_corr(corr),
+            'redundant_strategies': redundant,
+            'ensemble_performance': {
+                'equal_weight_sharpe': _sharpe(ew_series),
+                'optimized_sharpe': _sharpe(portfolio_series),
+                'optimal_weights': dict(zip(df.columns, opt['weights'])),
+                'optimized_series': portfolio_series
+            },
+            'diversification_ratio': float((df.std(ddof=1).mean()) / (df.sum(axis=1).std(ddof=1) + 1e-9))
+        }
+
+    def _optimize_weights(self, df: pd.DataFrame) -> Dict[str, Any]:
+        """Optimize portfolio weights with correlation penalty."""
+        n = df.shape[1]
+
+        def neg_sharpe(w):
+            w = np.asarray(w)
+            port = (df * w).sum(axis=1)
+            return -_sharpe(port)
+
+        def corr_penalty(w):
+            # Compute average pairwise correlation for strategies with weight > 0.05
+            sel = np.where(np.asarray(w) > 0.05)[0]
+            if len(sel) < 2:
+                return 0.0
+            sub = df.iloc[:, sel].corr().values
+            iu = np.triu_indices_from(sub, k=1)
+            return float(max(0, sub[iu].mean() - 0.4))  # penalize avg corr above 0.4
+
+        def objective(w): 
+            return neg_sharpe(w) + 2.0 * corr_penalty(w)
+
+        cons = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0},)
+        bnds = tuple((0.0, 1.0) for _ in range(n))
+        x0 = np.ones(n) / n
+        
+        try:
+            res = minimize(objective, x0=x0, bounds=bnds, constraints=cons)
+            w = res.x if res.success else x0
+            return {'weights': w, 'success': bool(res.success)}
+        except Exception as e:
+            logger.warning(f"Optimization failed: {e}")
+            return {'weights': x0, 'success': False}
+
+    @staticmethod
+    def _find_redundant(corr: pd.DataFrame, thr: float = 0.9) -> List[str]:
+        """Find redundant strategies based on correlation."""
+        redundant = set()
+        cols = corr.columns.tolist()
+        for i in range(len(cols)):
+            for j in range(i+1, len(cols)):
+                if corr.iloc[i, j] >= thr:
+                    redundant.add(cols[j])
+        return sorted(redundant)
+
+    @staticmethod
+    def _clusters_from_corr(corr: pd.DataFrame) -> List[List[str]]:
+        """Simple threshold clustering."""
+        thr = 0.7
+        clusters = []
+        seen = set()
+        for c in corr.columns:
+            if c in seen: 
+                continue
+            group = [c] + [k for k in corr.columns if k != c and corr.loc[c, k] >= thr]
+            for g in group: 
+                seen.add(g)
+            clusters.append(sorted(set(group)))
+        return clusters
\ No newline at end of file
Index: backend/validation/example_usage.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/example_usage.py b/backend/validation/example_usage.py
new file mode 100644
--- /dev/null	(date 1758169658985)
+++ b/backend/validation/example_usage.py	(date 1758169658985)
@@ -0,0 +1,338 @@
+"""Example usage of the validation framework.
+
+Demonstrates how to use the validation framework to evaluate trading strategies
+and make deployment decisions.
+"""
+
+import numpy as np
+import pandas as pd
+from datetime import datetime, timedelta
+import logging
+
+# Configure logging
+logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+logger = logging.getLogger(__name__)
+
+from backend.validation import ValidationOrchestrator, ValidationCriteria
+
+
+class ExampleStrategy:
+    """Example strategy for demonstration purposes."""
+
+    def __init__(self, name: str = "Example Strategy", lookback: int = 20):
+        self.name = name
+        self.lookback = lookback
+        self.description = "Example momentum strategy for validation testing"
+        self.parameters = {"lookback": lookback}
+
+    def backtest(self, start: str = "2020-01-01", end: str = "2024-12-31"):
+        """Simulate strategy backtest with realistic returns."""
+        date_range = pd.date_range(start=start, end=end, freq='D')
+
+        # Generate realistic returns with momentum bias
+        np.random.seed(42)  # For reproducible results
+
+        # Base market returns
+        market_returns = np.random.normal(0.0005, 0.015, len(date_range))
+
+        # Add momentum factor
+        momentum_signal = np.random.normal(0.0002, 0.005, len(date_range))
+
+        # Strategy returns with some alpha
+        strategy_returns = market_returns + momentum_signal * 0.3
+
+        # Add some regime-dependent behavior
+        volatility_regime = np.random.choice([0, 1], len(date_range), p=[0.8, 0.2])
+        strategy_returns[volatility_regime == 1] *= 1.5  # Higher volatility periods
+
+        returns = pd.Series(strategy_returns, index=date_range)
+
+        # Create backtest results object
+        class BacktestResults:
+            def __init__(self, returns):
+                self.returns = returns
+                self.total_return = (1 + returns).prod() - 1
+                self.sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)
+                self.max_drawdown = self._calculate_max_drawdown(returns)
+
+            def _calculate_max_drawdown(self, returns):
+                cumulative = (1 + returns).cumprod()
+                running_max = cumulative.expanding().max()
+                drawdowns = (cumulative - running_max) / running_max
+                return drawdowns.min()
+
+        return BacktestResults(returns)
+
+    def backtest_with_capital(self, capital: float):
+        """Backtest with specific capital amount."""
+        results = self.backtest()
+        # Scale returns based on capital (simplified)
+        return results
+
+
+class PoorPerformingStrategy(ExampleStrategy):
+    """Example of a poorly performing strategy."""
+
+    def __init__(self):
+        super().__init__("Poor Strategy", lookback=10)
+        self.description = "Example strategy with poor performance"
+
+    def backtest(self, start: str = "2020-01-01", end: str = "2024-12-31"):
+        """Generate poor performing returns."""
+        date_range = pd.date_range(start=start, end=end, freq='D')
+
+        np.random.seed(123)
+        # Generate consistently poor returns
+        poor_returns = np.random.normal(-0.0002, 0.025, len(date_range))
+
+        # Add occasional large losses
+        large_loss_days = np.random.choice([0, 1], len(date_range), p=[0.95, 0.05])
+        poor_returns[large_loss_days == 1] -= 0.05
+
+        returns = pd.Series(poor_returns, index=date_range)
+
+        class BacktestResults:
+            def __init__(self, returns):
+                self.returns = returns
+                self.total_return = (1 + returns).prod() - 1
+                self.sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)
+
+        return BacktestResults(returns)
+
+
+def example_single_strategy_validation():
+    """Example of validating a single strategy."""
+    logger.info("=== Single Strategy Validation Example ===")
+
+    # Create a custom validation criteria
+    criteria = ValidationCriteria(
+        min_sharpe_ratio=0.8,  # Slightly lower threshold for demo
+        max_drawdown=0.20,     # 20% max drawdown
+        min_factor_adjusted_alpha=0.03  # 3% alpha
+    )
+
+    # Initialize orchestrator with custom criteria
+    orchestrator = ValidationOrchestrator(criteria)
+
+    # Create strategy
+    strategy = ExampleStrategy("Momentum Strategy")
+
+    # Run validation
+    try:
+        results = orchestrator.validate_strategy(
+            strategy,
+            start_date="2020-01-01",
+            end_date="2024-12-31"
+        )
+
+        # Print key results
+        print("\n" + "="*60)
+        print("VALIDATION RESULTS")
+        print("="*60)
+
+        # Strategy info
+        if 'strategy_info' in results:
+            print(f"Strategy: {results['strategy_info']['name']}")
+
+        # Deployment decision
+        if 'deployment_decision' in results:
+            decision = results['deployment_decision']
+            print(f"Recommendation: {decision['recommendation']}")
+            print(f"Confidence: {decision.get('confidence_score', 0):.2%}")
+
+            if 'decision_rationale' in decision:
+                print(f"Rationale: {decision['decision_rationale']}")
+
+        # Data quality
+        if 'data_quality' in results:
+            quality = results['data_quality']
+            if 'returns_statistics' in quality:
+                stats = quality['returns_statistics']
+                print("\nStrategy Performance:")
+                print(f"  - Annual Return: {stats['mean'] * 252:.2%}")
+                print(f"  - Volatility: {stats['std'] * np.sqrt(252):.2%}")
+                print(f"  - Sharpe Ratio: {stats['mean'] / stats['std'] * np.sqrt(252):.2f}")
+
+        # Factor analysis
+        if 'factor_analysis' in results and 'annualized_alpha' in results['factor_analysis']:
+            alpha_results = results['factor_analysis']
+            print("\nAlpha Analysis:")
+            print(f"  - Annualized Alpha: {alpha_results['annualized_alpha']:.2%}")
+            print(f"  - Alpha Significant: {alpha_results.get('alpha_significant', False)}")
+            print(f"  - Data Source: {alpha_results.get('data_source', 'Unknown')}")
+
+        # Regime testing
+        if 'regime_testing' in results and 'profitable_regime_rate' in results['regime_testing']:
+            regime_results = results['regime_testing']
+            print("\nRegime Analysis:")
+            print(f"  - Profitable Regime Rate: {regime_results['profitable_regime_rate']:.1%}")
+            print(f"  - Edge is Robust: {regime_results.get('edge_is_robust', False)}")
+
+        # Recommendations
+        if 'deployment_decision' in results and 'deployment_recommendations' in results['deployment_decision']:
+            recommendations = results['deployment_decision']['deployment_recommendations']
+            if recommendations.get('immediate_actions'):
+                print("\nImmediate Actions:")
+                for action in recommendations['immediate_actions'][:3]:
+                    print(f"  - {action}")
+
+        # Save results
+        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+        filename = f"validation_results_{timestamp}.json"
+        orchestrator.save_validation_results(results, filename)
+        print(f"\nResults saved to: {filename}")
+
+    except Exception as e:
+        logger.error(f"Validation failed: {e}")
+        print(f"Validation failed: {e}")
+
+
+def example_multiple_strategy_validation():
+    """Example of validating multiple strategies."""
+    logger.info("=== Multiple Strategy Validation Example ===")
+
+    # Create orchestrator
+    orchestrator = ValidationOrchestrator()
+
+    # Create multiple strategies
+    strategies = {
+        "Momentum_20": ExampleStrategy("Momentum 20-day", lookback=20),
+        "Momentum_50": ExampleStrategy("Momentum 50-day", lookback=50),
+        "Poor_Strategy": PoorPerformingStrategy()
+    }
+
+    try:
+        # Run multi-strategy validation
+        results = orchestrator.validate_multiple_strategies(
+            strategies,
+            start_date="2020-01-01",
+            end_date="2024-12-31"
+        )
+
+        print("\n" + "="*60)
+        print("MULTI-STRATEGY VALIDATION RESULTS")
+        print("="*60)
+
+        # Validation summary
+        if 'validation_summary' in results:
+            summary = results['validation_summary']
+            print(f"Total Strategies: {summary['total_strategies']}")
+
+            if 'overall_assessment' in summary:
+                assessment = summary['overall_assessment']
+                ready_pct = assessment.get('deployment_ready_percentage', 0)
+                print(f"Deployment Ready: {ready_pct:.1%}")
+
+        # Strategy rankings
+        if 'comparative_analysis' in results:
+            comparative = results['comparative_analysis']
+
+            if 'strategy_rankings' in comparative:
+                rankings = comparative['strategy_rankings']
+
+                if 'sharpe_ratio' in rankings:
+                    print("\nSharpe Ratio Rankings:")
+                    for i, (name, sharpe) in enumerate(rankings['sharpe_ratio'][:3], 1):
+                        print(f"  {i}. {name}: {sharpe:.2f}")
+
+            if 'best_performers' in comparative:
+                best = comparative['best_performers']
+                print("\nBest Performers:")
+                if best.get('highest_sharpe'):
+                    print(f"  - Highest Sharpe: {best['highest_sharpe']}")
+                if best.get('most_deployable'):
+                    print(f"  - Most Deployable: {best['most_deployable']}")
+
+            if 'deployment_readiness' in comparative:
+                readiness = comparative['deployment_readiness']
+                print("\nDeployment Readiness:")
+                for status, count in readiness.items():
+                    if count > 0:
+                        print(f"  - {status}: {count}")
+
+        # Ensemble analysis
+        if 'ensemble_analysis' in results and 'portfolio_comparison' in results['ensemble_analysis']:
+            ensemble = results['ensemble_analysis']
+            comparison = ensemble['portfolio_comparison']
+
+            print("\nEnsemble Analysis:")
+            print(f"  - Equal Weight Sharpe: {comparison.get('equal_weight_sharpe', 0):.2f}")
+            print(f"  - Optimized Sharpe: {comparison.get('optimized_sharpe', 0):.2f}")
+            print(f"  - Diversification Benefit: {comparison.get('diversification_benefit', 0):.2f}")
+
+            if 'optimal_ensemble' in ensemble:
+                optimal = ensemble['optimal_ensemble']
+                if 'selected_strategies' in optimal:
+                    print(f"  - Optimal Strategies: {', '.join(optimal['selected_strategies'])}")
+
+    except Exception as e:
+        logger.error(f"Multi-strategy validation failed: {e}")
+        print(f"Multi-strategy validation failed: {e}")
+
+
+def example_custom_validation_criteria():
+    """Example of using custom validation criteria."""
+    logger.info("=== Custom Validation Criteria Example ===")
+
+    # Create strict criteria for institutional deployment
+    strict_criteria = ValidationCriteria(
+        min_sharpe_ratio=1.5,           # Higher Sharpe requirement
+        max_drawdown=0.10,              # Lower drawdown tolerance
+        min_win_rate=0.55,              # Higher win rate requirement
+        min_factor_adjusted_alpha=0.08, # Higher alpha requirement
+        alpha_t_stat_threshold=2.5,     # Stricter significance
+        min_regime_consistency=0.8,     # Must work in 80% of regimes
+        min_cross_market_success=0.7,   # Higher cross-market success
+        min_return_on_capital=0.20,     # Higher ROC requirement
+        max_margin_calls=0              # Zero tolerance for margin calls
+    )
+
+    orchestrator = ValidationOrchestrator(strict_criteria)
+    strategy = ExampleStrategy("High-Bar Strategy")
+
+    try:
+        results = orchestrator.validate_strategy(strategy)
+
+        print("\n" + "="*60)
+        print("STRICT CRITERIA VALIDATION")
+        print("="*60)
+
+        if 'deployment_decision' in results:
+            decision = results['deployment_decision']
+            print(f"Recommendation: {decision['recommendation']}")
+            print(f"Confidence: {decision.get('confidence_score', 0):.2%}")
+
+            # Show which criteria failed
+            if 'component_scores' in decision:
+                print("\nCriteria Performance:")
+                for component, scores in decision['component_scores'].items():
+                    print(f"  {component}:")
+                    for metric, data in scores.items():
+                        status = "âœ“" if data['passed'] else "âœ—"
+                        print(f"    {status} {metric}: {data['value']:.3f} (threshold: {data['threshold']})")
+
+    except Exception as e:
+        logger.error(f"Strict validation failed: {e}")
+        print(f"Strict validation failed: {e}")
+
+
+if __name__ == "__main__":
+    print("WallStreetBots Validation Framework Examples")
+    print("=" * 50)
+
+    # Run examples
+    example_single_strategy_validation()
+    print("\n" + "="*80 + "\n")
+
+    example_multiple_strategy_validation()
+    print("\n" + "="*80 + "\n")
+
+    example_custom_validation_criteria()
+
+    print("\n" + "="*50)
+    print("Examples completed!")
+    print("\nTo use the validation framework in your code:")
+    print("1. Import: from backend.validation import ValidationOrchestrator")
+    print("2. Create orchestrator: orchestrator = ValidationOrchestrator()")
+    print("3. Validate strategy: results = orchestrator.validate_strategy(your_strategy)")
+    print("4. Check recommendation: results['deployment_decision']['recommendation']")
\ No newline at end of file
Index: backend/validation/factor_analysis.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/factor_analysis.py b/backend/validation/factor_analysis.py
new file mode 100644
--- /dev/null	(date 1758169876674)
+++ b/backend/validation/factor_analysis.py	(date 1758169876674)
@@ -0,0 +1,294 @@
+"""
+Correct Factor Analysis with OLS + Newey-West HAC
+Implements proper factor regression using statsmodels with HAC standard errors.
+"""
+
+from __future__ import annotations
+import pandas as pd
+import numpy as np
+from dataclasses import dataclass
+from typing import Dict, Optional, List, Tuple, Any
+import statsmodels.api as sm
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class FactorResult:
+    """Results from factor regression analysis."""
+    daily_alpha: float
+    annualized_alpha: float
+    alpha_t_stat: float
+    alpha_p_value: float
+    factor_exposures: Dict[str, float]
+    factor_t_stats: Dict[str, float]
+    factor_p_values: Dict[str, float]
+    r_squared: float
+    alpha_significant: bool
+    n_obs: int
+    hac_lags: int
+    regression_summary: str
+
+
+class AlphaFactorAnalyzer:
+    """
+    Regress strategy daily returns on factors using OLS with HAC (Newey-West) std errors.
+    Accepts either true Fama-French dataframe (recommended) or ETF proxies (fallback).
+    Columns expected in factor_df: ['mkt', 'smb', 'hml', 'umd'] (or subset).
+    Strategy returns should be daily arithmetic returns aligned to factor_df.index.
+    """
+
+    def __init__(self, min_obs: int = 126, hac_lags: int = 5, alpha_sig_t: float = 2.0):
+        """
+        Initialize factor analyzer.
+        
+        Args:
+            min_obs: Minimum observations required
+            hac_lags: Number of lags for HAC standard errors
+            alpha_sig_t: T-statistic threshold for alpha significance
+        """
+        self.min_obs = min_obs
+        self.hac_lags = hac_lags
+        self.alpha_sig_t = alpha_sig_t
+
+    def run_factor_regression(
+        self,
+        strategy_returns: pd.Series,
+        factor_df: pd.DataFrame,
+        factor_cols: Optional[List[str]] = None,
+        risk_free_rate: float = 0.0
+    ) -> FactorResult:
+        """
+        Run factor regression with HAC standard errors.
+        
+        Args:
+            strategy_returns: Strategy daily returns
+            factor_df: Factor returns dataframe
+            factor_cols: Specific factor columns to use
+            risk_free_rate: Risk-free rate for excess returns
+            
+        Returns:
+            FactorResult with regression analysis
+        """
+        if factor_cols is None:
+            # Try common set; fallback to whatever is available
+            default_cols = ['mkt', 'smb', 'hml', 'umd']
+            factor_cols = [c for c in default_cols if c in factor_df.columns]
+            
+        if not factor_cols:
+            raise ValueError("No valid factor columns found")
+
+        # Align and drop NaNs
+        df = pd.concat([
+            strategy_returns.rename('ret'), 
+            factor_df[factor_cols]
+        ], axis=1).dropna()
+        
+        if len(df) < self.min_obs:
+            raise ValueError(f"Insufficient observations ({len(df)}), need â‰¥ {self.min_obs}")
+
+        # Calculate excess returns
+        y = (df['ret'] - risk_free_rate).astype(float)
+        X = sm.add_constant(df[factor_cols].astype(float))
+        
+        # Fit OLS model with HAC standard errors
+        model = sm.OLS(y, X, missing='drop')
+        res = model.fit(cov_type='HAC', cov_kwds={'maxlags': self.hac_lags})
+
+        # Extract results
+        alpha = res.params['const']
+        alpha_t_stat = res.tvalues['const']
+        alpha_p_value = res.pvalues['const']
+        
+        # Factor exposures and statistics
+        factor_exposures = {k: float(res.params[k]) for k in factor_cols}
+        factor_t_stats = {k: float(res.tvalues[k]) for k in factor_cols}
+        factor_p_values = {k: float(res.pvalues[k]) for k in factor_cols}
+        
+        r_squared = float(res.rsquared)
+        alpha_significant = abs(alpha_t_stat) >= self.alpha_sig_t
+
+        return FactorResult(
+            daily_alpha=float(alpha),
+            annualized_alpha=float(alpha * 252.0),
+            alpha_t_stat=float(alpha_t_stat),
+            alpha_p_value=float(alpha_p_value),
+            factor_exposures=factor_exposures,
+            factor_t_stats=factor_t_stats,
+            factor_p_values=factor_p_values,
+            r_squared=r_squared,
+            alpha_significant=alpha_significant,
+            n_obs=len(df),
+            hac_lags=self.hac_lags,
+            regression_summary=str(res.summary())
+        )
+
+    def isolate_components_alpha(
+        self,
+        component_returns: Dict[str, pd.Series],
+        factor_df: pd.DataFrame,
+        risk_free_rate: float = 0.0
+    ) -> Dict[str, FactorResult]:
+        """
+        Analyze alpha for individual strategy components.
+        
+        Args:
+            component_returns: Dictionary of component returns
+            factor_df: Factor returns dataframe
+            risk_free_rate: Risk-free rate
+            
+        Returns:
+            Dictionary of FactorResult for each component
+        """
+        results = {}
+        for name, series in component_returns.items():
+            try:
+                results[name] = self.run_factor_regression(
+                    series, factor_df, risk_free_rate=risk_free_rate
+                )
+            except Exception as e:
+                logger.warning(f"Failed to analyze component {name}: {e}")
+                # Return safe default
+                results[name] = FactorResult(
+                    daily_alpha=0.0,
+                    annualized_alpha=0.0,
+                    alpha_t_stat=0.0,
+                    alpha_p_value=1.0,
+                    factor_exposures={},
+                    factor_t_stats={},
+                    factor_p_values={},
+                    r_squared=0.0,
+                    alpha_significant=False,
+                    n_obs=0,
+                    hac_lags=self.hac_lags,
+                    regression_summary="Analysis failed"
+                )
+        return results
+
+    def create_factor_proxies(self, market_data: pd.DataFrame) -> pd.DataFrame:
+        """
+        Create factor proxies from market data if true factors unavailable.
+        
+        Args:
+            market_data: DataFrame with columns like ['SPY', 'IWM', 'EFA', 'EEM']
+            
+        Returns:
+            DataFrame with factor proxies
+        """
+        proxies = pd.DataFrame(index=market_data.index)
+        
+        # Market factor (SPY)
+        if 'SPY' in market_data.columns:
+            proxies['mkt'] = market_data['SPY'].pct_change()
+        
+        # Size factor (IWM - SPY)
+        if 'IWM' in market_data.columns and 'SPY' in market_data.columns:
+            proxies['smb'] = market_data['IWM'].pct_change() - market_data['SPY'].pct_change()
+        
+        # Value factor (simplified - would need HML data)
+        if 'EFA' in market_data.columns and 'SPY' in market_data.columns:
+            proxies['hml'] = market_data['EFA'].pct_change() - market_data['SPY'].pct_change()
+        
+        # Momentum factor (SPY 12-1 month)
+        if 'SPY' in market_data.columns:
+            spy_returns = market_data['SPY'].pct_change()
+            proxies['umd'] = spy_returns.rolling(252).apply(
+                lambda x: (1 + x).prod() - (1 + x.tail(21)).prod()
+            ) if len(spy_returns) >= 252 else pd.Series(index=spy_returns.index)
+        
+        return proxies.dropna()
+
+    def analyze_factor_attribution(self, strategy_returns: pd.Series, 
+                                 factor_df: pd.DataFrame) -> Dict[str, Any]:
+        """
+        Comprehensive factor attribution analysis.
+        
+        Args:
+            strategy_returns: Strategy returns
+            factor_df: Factor returns
+            
+        Returns:
+            Comprehensive attribution analysis
+        """
+        try:
+            # Run main regression
+            main_result = self.run_factor_regression(strategy_returns, factor_df)
+            
+            # Calculate factor contributions
+            factor_contributions = {}
+            for factor, exposure in main_result.factor_exposures.items():
+                if factor in factor_df.columns:
+                    factor_returns = factor_df[factor].dropna()
+                    contribution = exposure * factor_returns.mean() * 252  # Annualized
+                    factor_contributions[factor] = contribution
+            
+            # Calculate residual (unexplained) return
+            total_factor_contribution = sum(factor_contributions.values())
+            residual_contribution = main_result.annualized_alpha
+            
+            return {
+                'alpha_analysis': main_result,
+                'factor_contributions': factor_contributions,
+                'total_factor_contribution': total_factor_contribution,
+                'residual_contribution': residual_contribution,
+                'attribution_summary': {
+                    'alpha_dominates': abs(residual_contribution) > abs(total_factor_contribution),
+                    'factor_explanation': main_result.r_squared,
+                    'significant_alpha': main_result.alpha_significant
+                }
+            }
+            
+        except Exception as e:
+            logger.error(f"Factor attribution analysis failed: {e}")
+            return {'error': str(e)}
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    def test_factor_analysis():
+        """Test the factor analysis implementation."""
+        print("=== Factor Analysis Test ===")
+        
+        # Generate test data
+        np.random.seed(42)
+        dates = pd.date_range('2023-01-01', periods=500, freq='B')
+        
+        # Generate factor returns
+        factors = pd.DataFrame({
+            'mkt': np.random.normal(0.0005, 0.015, len(dates)),
+            'smb': np.random.normal(0.0002, 0.008, len(dates)),
+            'hml': np.random.normal(0.0001, 0.006, len(dates)),
+            'umd': np.random.normal(0.0003, 0.010, len(dates))
+        }, index=dates)
+        
+        # Generate strategy with true alpha
+        strategy_returns = pd.Series(
+            0.0008 + 0.7 * factors['mkt'] + 0.3 * factors['smb'] + 
+            np.random.normal(0, 0.012, len(dates)),
+            index=dates
+        )
+        
+        # Run analysis
+        analyzer = AlphaFactorAnalyzer()
+        result = analyzer.run_factor_regression(strategy_returns, factors)
+        
+        print(f"Alpha (annualized): {result.annualized_alpha:.4f}")
+        print(f"Alpha t-stat: {result.alpha_t_stat:.2f}")
+        print(f"Alpha significant: {result.alpha_significant}")
+        print(f"R-squared: {result.r_squared:.3f}")
+        print(f"Factor exposures: {result.factor_exposures}")
+        
+        # Test component analysis
+        components = {
+            'timing': strategy_returns * 0.6,
+            'selection': strategy_returns * 0.4
+        }
+        
+        component_results = analyzer.isolate_components_alpha(components, factors)
+        print("\nComponent Analysis:")
+        for name, comp_result in component_results.items():
+            print(f"  {name}: Alpha = {comp_result.annualized_alpha:.4f}, "
+                  f"Significant = {comp_result.alpha_significant}")
+    
+    test_factor_analysis()
\ No newline at end of file
Index: backend/validation/statistical_rigor/leakage_detection.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/statistical_rigor/leakage_detection.py b/backend/validation/statistical_rigor/leakage_detection.py
new file mode 100644
--- /dev/null	(date 1758164061117)
+++ b/backend/validation/statistical_rigor/leakage_detection.py	(date 1758164061117)
@@ -0,0 +1,487 @@
+"""Look-ahead bias and data leakage detection system.
+
+Prevents strategies from accidentally using future information including
+survivorship bias, corporate actions, and post-event data.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Any, Set, Tuple
+import logging
+from datetime import datetime, timedelta
+from dataclasses import dataclass
+import inspect
+import ast
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class LeakageViolation:
+    """Represents a detected data leakage violation."""
+    violation_type: str
+    description: str
+    severity: str  # 'critical', 'warning', 'info'
+    detected_at: datetime
+    code_location: Optional[str] = None
+    suggested_fix: Optional[str] = None
+
+
+class TemporalGuard:
+    """Guards against temporal data leakage in strategy code."""
+
+    def __init__(self):
+        self.current_time = None
+        self.violations = []
+        self.monitored_functions = set()
+
+    def set_current_time(self, current_time: pd.Timestamp):
+        """Set the current simulation time."""
+        self.current_time = current_time
+
+    def check_data_access(self, data: pd.Series, requested_time: pd.Timestamp) -> bool:
+        """Check if data access violates temporal constraints."""
+        if self.current_time is None:
+            logger.warning("TemporalGuard: current_time not set")
+            return True
+
+        # Check for look-ahead bias
+        if requested_time > self.current_time:
+            violation = LeakageViolation(
+                violation_type='look_ahead_bias',
+                description=f"Attempted to access data from {requested_time} while current time is {self.current_time}",
+                severity='critical',
+                detected_at=datetime.now(),
+                suggested_fix="Ensure data access is limited to current_time or earlier"
+            )
+            self.violations.append(violation)
+            return False
+
+        return True
+
+    def monitor_function(self, func):
+        """Decorator to monitor function for data leakage."""
+        def wrapper(*args, **kwargs):
+            # Record function call
+            self.monitored_functions.add(func.__name__)
+
+            # Analyze function for potential leakage
+            self._analyze_function_code(func)
+
+            # Execute function
+            return func(*args, **kwargs)
+
+        return wrapper
+
+    def _analyze_function_code(self, func):
+        """Analyze function source code for potential leakage patterns."""
+        try:
+            source = inspect.getsource(func)
+            tree = ast.parse(source)
+
+            # Look for suspicious patterns
+            for node in ast.walk(tree):
+                if isinstance(node, ast.Attribute):
+                    # Check for dangerous pandas operations
+                    if hasattr(node, 'attr'):
+                        if node.attr in ['shift', 'pct_change'] and hasattr(node, 'value'):
+                            # Check if shift has negative values (future data)
+                            self._check_shift_operations(node, func.__name__)
+
+                elif isinstance(node, ast.Call):
+                    # Check for rolling operations that might use future data
+                    if hasattr(node, 'func') and hasattr(node.func, 'attr'):
+                        if node.func.attr in ['rolling', 'ewm']:
+                            self._check_rolling_operations(node, func.__name__)
+
+        except Exception as e:
+            logger.debug(f"Could not analyze function {func.__name__}: {e}")
+
+    def _check_shift_operations(self, node: ast.AST, func_name: str):
+        """Check shift operations for look-ahead bias."""
+        # This is a simplified check - in production you'd want more sophisticated AST analysis
+        pass
+
+    def _check_rolling_operations(self, node: ast.AST, func_name: str):
+        """Check rolling operations for potential leakage."""
+        # This is a simplified check - in production you'd want more sophisticated AST analysis
+        pass
+
+
+class CorporateActionsGuard:
+    """Prevents leakage from corporate actions and earnings data."""
+
+    def __init__(self):
+        self.corporate_actions = {}
+        self.earnings_calendar = {}
+        self.violations = []
+
+    def register_corporate_action(self, symbol: str, action_date: pd.Timestamp,
+                                action_type: str, effective_date: pd.Timestamp):
+        """Register a corporate action."""
+        if symbol not in self.corporate_actions:
+            self.corporate_actions[symbol] = []
+
+        self.corporate_actions[symbol].append({
+            'action_date': action_date,
+            'effective_date': effective_date,
+            'action_type': action_type
+        })
+
+    def register_earnings_date(self, symbol: str, earnings_date: pd.Timestamp,
+                             announcement_time: str = 'after_market'):
+        """Register an earnings announcement date."""
+        if symbol not in self.earnings_calendar:
+            self.earnings_calendar[symbol] = []
+
+        self.earnings_calendar[symbol].append({
+            'earnings_date': earnings_date,
+            'announcement_time': announcement_time
+        })
+
+    def check_price_data_validity(self, symbol: str, data: pd.Series,
+                                current_time: pd.Timestamp) -> Tuple[bool, List[str]]:
+        """Check if price data is valid given corporate actions."""
+        warnings = []
+        is_valid = True
+
+        # Check for unadjusted data around corporate actions
+        if symbol in self.corporate_actions:
+            for action in self.corporate_actions[symbol]:
+                action_date = action['action_date']
+                effective_date = action['effective_date']
+
+                # Check if we're using data around corporate action dates
+                if action_date <= current_time:
+                    # We should know about this action
+                    data_around_action = data[
+                        (data.index >= action_date - timedelta(days=5)) &
+                        (data.index <= action_date + timedelta(days=5))
+                    ]
+
+                    if len(data_around_action) > 0:
+                        # Check for suspicious price jumps that might indicate unadjusted data
+                        returns = data_around_action.pct_change().dropna()
+                        if len(returns) > 0 and (abs(returns) > 0.2).any():
+                            if action['action_type'] in ['split', 'dividend']:
+                                warnings.append(
+                                    f"Large price movement around {action['action_type']} "
+                                    f"on {action_date} - check if data is properly adjusted"
+                                )
+
+        return is_valid, warnings
+
+    def check_earnings_blackout(self, symbol: str, current_time: pd.Timestamp,
+                              trade_time: str = 'market_hours') -> bool:
+        """Check if current time violates earnings blackout rules."""
+        if symbol not in self.earnings_calendar:
+            return True  # No earnings data, assume OK
+
+        for earnings in self.earnings_calendar[symbol]:
+            earnings_date = earnings['earnings_date']
+            announcement_time = earnings['announcement_time']
+
+            # Define blackout window
+            if announcement_time == 'before_market':
+                blackout_start = earnings_date - timedelta(days=1)
+                blackout_end = earnings_date
+            else:  # after_market or during_market
+                blackout_start = earnings_date - timedelta(days=1)
+                blackout_end = earnings_date + timedelta(days=1)
+
+            # Check if current time falls in blackout window
+            if blackout_start <= current_time <= blackout_end:
+                violation = LeakageViolation(
+                    violation_type='earnings_blackout_violation',
+                    description=f"Trading {symbol} during earnings blackout period: {blackout_start} to {blackout_end}",
+                    severity='warning',
+                    detected_at=datetime.now(),
+                    suggested_fix="Implement earnings blackout rules or pre-earnings exit logic"
+                )
+                self.violations.append(violation)
+                return False
+
+        return True
+
+
+class SurvivorshipGuard:
+    """Detects survivorship bias in strategy backtests."""
+
+    def __init__(self):
+        self.universe_history = {}
+        self.violations = []
+
+    def register_universe_snapshot(self, date: pd.Timestamp, universe: List[str]):
+        """Register universe composition at a specific date."""
+        self.universe_history[date] = set(universe)
+
+    def check_survivorship_bias(self, strategy_universe: List[str],
+                              backtest_start: pd.Timestamp,
+                              backtest_end: pd.Timestamp) -> Tuple[bool, List[str]]:
+        """Check for survivorship bias in strategy universe."""
+        warnings = []
+        has_bias = False
+
+        # Check if universe uses only symbols that survived to the end
+        if not self.universe_history:
+            warnings.append(
+                "No universe history available - cannot check for survivorship bias. "
+                "Consider using point-in-time universe selection."
+            )
+            return True, warnings  # Can't check, assume OK but warn
+
+        # Find universe snapshots within backtest period
+        relevant_dates = [date for date in self.universe_history.keys()
+                         if backtest_start <= date <= backtest_end]
+
+        if not relevant_dates:
+            warnings.append(
+                f"No universe snapshots found between {backtest_start} and {backtest_end}"
+            )
+            return True, warnings
+
+        # Check if strategy universe includes delisted stocks
+        strategy_set = set(strategy_universe)
+        earliest_universe = self.universe_history[min(relevant_dates)]
+        latest_universe = self.universe_history[max(relevant_dates)]
+
+        # Stocks that were in universe at start but not at end (potentially delisted)
+        potentially_delisted = earliest_universe - latest_universe
+
+        # Check if strategy completely excludes potentially delisted stocks
+        if potentially_delisted and not (strategy_set & potentially_delisted):
+            has_bias = True
+            violation = LeakageViolation(
+                violation_type='survivorship_bias',
+                description=(
+                    f"Strategy universe excludes all {len(potentially_delisted)} stocks "
+                    f"that may have been delisted during backtest period"
+                ),
+                severity='critical',
+                detected_at=datetime.now(),
+                suggested_fix=(
+                    "Include delisted stocks in backtest or use point-in-time "
+                    "universe selection to avoid survivorship bias"
+                )
+            )
+            self.violations.append(violation)
+
+        # Check for other potential bias indicators
+        if len(strategy_set & latest_universe) / len(strategy_set) > 0.95:
+            warnings.append(
+                "Strategy universe heavily biased toward stocks that survived to end of period"
+            )
+
+        return not has_bias, warnings
+
+
+class ParameterGuard:
+    """Ensures parameter immutability and prevents optimization leakage."""
+
+    def __init__(self):
+        self.parameter_registry = {}
+        self.frozen_parameters = {}
+        self.violations = []
+
+    def register_parameters(self, strategy_name: str, parameters: Dict[str, Any],
+                          optimization_period: Tuple[pd.Timestamp, pd.Timestamp],
+                          git_hash: Optional[str] = None):
+        """Register strategy parameters for a specific optimization period."""
+        param_id = f"{strategy_name}_{optimization_period[0]}_{optimization_period[1]}"
+
+        self.parameter_registry[param_id] = {
+            'strategy_name': strategy_name,
+            'parameters': parameters.copy(),
+            'optimization_period': optimization_period,
+            'registered_at': datetime.now(),
+            'git_hash': git_hash,
+            'frozen': False
+        }
+
+        logger.info(f"Registered parameters for {strategy_name}: {param_id}")
+
+    def freeze_parameters(self, strategy_name: str,
+                        optimization_period: Tuple[pd.Timestamp, pd.Timestamp]) -> str:
+        """Freeze parameters after optimization period ends."""
+        param_id = f"{strategy_name}_{optimization_period[0]}_{optimization_period[1]}"
+
+        if param_id not in self.parameter_registry:
+            raise ValueError(f"Parameters not found for {param_id}")
+
+        # Mark as frozen
+        self.parameter_registry[param_id]['frozen'] = True
+        self.parameter_registry[param_id]['frozen_at'] = datetime.now()
+
+        # Store in frozen registry
+        self.frozen_parameters[param_id] = self.parameter_registry[param_id].copy()
+
+        logger.info(f"Frozen parameters for {param_id}")
+        return param_id
+
+    def validate_parameters(self, strategy_name: str, current_parameters: Dict[str, Any],
+                          oos_period: Tuple[pd.Timestamp, pd.Timestamp]) -> bool:
+        """Validate that current parameters match frozen parameters for OOS period."""
+        # Find the relevant frozen parameter set
+        relevant_param_id = None
+
+        for param_id, param_data in self.frozen_parameters.items():
+            if (param_data['strategy_name'] == strategy_name and
+                param_data['optimization_period'][1] <= oos_period[0]):
+                relevant_param_id = param_id
+                break
+
+        if relevant_param_id is None:
+            violation = LeakageViolation(
+                violation_type='parameter_leakage',
+                description=f"No frozen parameters found for {strategy_name} OOS period {oos_period}",
+                severity='critical',
+                detected_at=datetime.now(),
+                suggested_fix="Ensure parameters are frozen before OOS testing begins"
+            )
+            self.violations.append(violation)
+            return False
+
+        # Compare parameters
+        frozen_params = self.frozen_parameters[relevant_param_id]['parameters']
+        params_match = self._deep_compare_parameters(frozen_params, current_parameters)
+
+        if not params_match:
+            violation = LeakageViolation(
+                violation_type='parameter_drift',
+                description=f"Current parameters for {strategy_name} don't match frozen parameters {relevant_param_id}",
+                severity='critical',
+                detected_at=datetime.now(),
+                suggested_fix="Revert to frozen parameters or re-run optimization if justified"
+            )
+            self.violations.append(violation)
+
+        return params_match
+
+    def _deep_compare_parameters(self, params1: Dict[str, Any], params2: Dict[str, Any]) -> bool:
+        """Deep comparison of parameter dictionaries."""
+        if set(params1.keys()) != set(params2.keys()):
+            return False
+
+        for key in params1.keys():
+            val1, val2 = params1[key], params2[key]
+
+            if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
+                if abs(val1 - val2) > 1e-10:  # Numerical tolerance
+                    return False
+            elif val1 != val2:
+                return False
+
+        return True
+
+
+class LeakageDetectionSuite:
+    """Comprehensive data leakage detection system."""
+
+    def __init__(self):
+        self.temporal_guard = TemporalGuard()
+        self.corporate_actions_guard = CorporateActionsGuard()
+        self.survivorship_guard = SurvivorshipGuard()
+        self.parameter_guard = ParameterGuard()
+
+    def validate_strategy(self, strategy, backtest_start: pd.Timestamp,
+                         backtest_end: pd.Timestamp,
+                         strategy_universe: List[str]) -> Dict[str, Any]:
+        """Comprehensive validation of strategy for data leakage."""
+        validation_results = {
+            'passed': True,
+            'violations': [],
+            'warnings': [],
+            'checks_performed': []
+        }
+
+        # Survivorship bias check
+        try:
+            survivorship_ok, survivorship_warnings = self.survivorship_guard.check_survivorship_bias(
+                strategy_universe, backtest_start, backtest_end
+            )
+            validation_results['checks_performed'].append('survivorship_bias')
+            validation_results['warnings'].extend(survivorship_warnings)
+
+            if not survivorship_ok:
+                validation_results['passed'] = False
+
+        except Exception as e:
+            logger.error(f"Survivorship check failed: {e}")
+            validation_results['warnings'].append(f"Survivorship check failed: {e}")
+
+        # Corporate actions validation
+        try:
+            for symbol in strategy_universe:
+                ca_valid, ca_warnings = self.corporate_actions_guard.check_price_data_validity(
+                    symbol, pd.Series(), backtest_end  # Placeholder data
+                )
+                validation_results['warnings'].extend(ca_warnings)
+
+            validation_results['checks_performed'].append('corporate_actions')
+
+        except Exception as e:
+            logger.error(f"Corporate actions check failed: {e}")
+            validation_results['warnings'].append(f"Corporate actions check failed: {e}")
+
+        # Collect all violations
+        all_violations = (
+            self.temporal_guard.violations +
+            self.corporate_actions_guard.violations +
+            self.survivorship_guard.violations +
+            self.parameter_guard.violations
+        )
+
+        validation_results['violations'] = [
+            {
+                'type': v.violation_type,
+                'description': v.description,
+                'severity': v.severity,
+                'suggested_fix': v.suggested_fix
+            }
+            for v in all_violations
+        ]
+
+        # Update overall pass/fail status
+        critical_violations = [v for v in all_violations if v.severity == 'critical']
+        if critical_violations:
+            validation_results['passed'] = False
+
+        return validation_results
+
+    def create_leakage_report(self) -> str:
+        """Create a comprehensive leakage detection report."""
+        all_violations = (
+            self.temporal_guard.violations +
+            self.corporate_actions_guard.violations +
+            self.survivorship_guard.violations +
+            self.parameter_guard.violations
+        )
+
+        report = "=== DATA LEAKAGE DETECTION REPORT ===\n\n"
+
+        if not all_violations:
+            report += "âœ… No data leakage violations detected.\n"
+        else:
+            report += f"âš ï¸ {len(all_violations)} potential violations detected:\n\n"
+
+            for i, violation in enumerate(all_violations, 1):
+                report += f"{i}. {violation.violation_type.upper()} ({violation.severity})\n"
+                report += f"   Description: {violation.description}\n"
+                if violation.suggested_fix:
+                    report += f"   Suggested Fix: {violation.suggested_fix}\n"
+                report += f"   Detected: {violation.detected_at}\n\n"
+
+        # Summary recommendations
+        critical_count = len([v for v in all_violations if v.severity == 'critical'])
+        warning_count = len([v for v in all_violations if v.severity == 'warning'])
+
+        report += "=== RECOMMENDATIONS ===\n"
+        if critical_count > 0:
+            report += f"ðŸš¨ {critical_count} critical violations must be fixed before deployment.\n"
+        if warning_count > 0:
+            report += f"âš ï¸ {warning_count} warnings should be reviewed and addressed.\n"
+
+        if critical_count == 0 and warning_count == 0:
+            report += "âœ… Strategy passes data leakage validation.\n"
+
+        return report
\ No newline at end of file
Index: backend/validation/fast_edge/liquid_anomaly_detector.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/fast_edge/liquid_anomaly_detector.py b/backend/validation/fast_edge/liquid_anomaly_detector.py
new file mode 100644
--- /dev/null	(date 1758165426501)
+++ b/backend/validation/fast_edge/liquid_anomaly_detector.py	(date 1758165426501)
@@ -0,0 +1,580 @@
+"""
+Liquid Anomaly Detection for Fast-Edge Implementation
+Detects simple, liquid market anomalies for rapid deployment and profit capture.
+"""
+
+import asyncio
+import logging
+import time
+from datetime import datetime, timedelta
+from typing import Dict, List, Optional, Any, Tuple, Set
+from dataclasses import dataclass, field
+from enum import Enum
+import numpy as np
+import pandas as pd
+from collections import defaultdict, deque
+import yfinance as yf
+import threading
+
+
+class AnomalyType(Enum):
+    VOLUME_SPIKE = "volume_spike"
+    PRICE_GAP = "price_gap"
+    SPREAD_COMPRESSION = "spread_compression"
+    ORDER_BOOK_IMBALANCE = "order_book_imbalance"
+    MOMENTUM_REVERSAL = "momentum_reversal"
+    EARNINGS_DRIFT = "earnings_drift"
+    ETF_ARBITRAGE = "etf_arbitrage"
+    OPTIONS_SKEW = "options_skew"
+
+
+class LiquidityTier(Enum):
+    ULTRA_LIQUID = "ultra_liquid"  # SPY, QQQ, AAPL, etc.
+    HIGH_LIQUID = "high_liquid"    # S&P 500 components
+    MEDIUM_LIQUID = "medium_liquid" # Russell 1000
+    LOW_LIQUID = "low_liquid"      # Small caps
+
+
+@dataclass
+class AnomalySignal:
+    """Represents a detected market anomaly."""
+    symbol: str
+    anomaly_type: AnomalyType
+    strength: float  # 0-1 confidence score
+    timestamp: datetime
+    expected_duration_minutes: int
+    target_profit_bps: int  # Basis points
+    max_position_size: float
+    entry_price: float
+    stop_loss: float
+    take_profit: float
+    metadata: Dict[str, Any] = field(default_factory=dict)
+    liquidity_tier: LiquidityTier = LiquidityTier.MEDIUM_LIQUID
+
+
+@dataclass
+class MarketState:
+    """Current market state for anomaly detection."""
+    symbol: str
+    timestamp: datetime
+    price: float
+    volume: int
+    bid: float
+    ask: float
+    spread_bps: float
+    volume_ratio_1m: float  # Volume vs 1-month average
+    momentum_5m: float      # 5-minute momentum
+    momentum_1h: float      # 1-hour momentum
+    rsi_14: float          # 14-period RSI
+    volatility_20d: float  # 20-day realized volatility
+
+
+class LiquidityClassifier:
+    """Classifies securities by liquidity for appropriate anomaly detection."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.ultra_liquid_symbols = {
+            'SPY', 'QQQ', 'IWM', 'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'TSLA',
+            'NVDA', 'META', 'BRK.B', 'UNH', 'JNJ', 'V', 'PG', 'HD', 'MA',
+            'BAC', 'XOM', 'DIS', 'ADBE', 'CRM', 'NFLX', 'KO', 'PEP'
+        }
+        self.liquidity_cache: Dict[str, Tuple[LiquidityTier, datetime]] = {}
+
+    def classify_liquidity(self, symbol: str, market_state: MarketState) -> LiquidityTier:
+        """Classify symbol liquidity tier."""
+        # Check cache first
+        if symbol in self.liquidity_cache:
+            tier, timestamp = self.liquidity_cache[symbol]
+            if datetime.now() - timestamp < timedelta(hours=1):
+                return tier
+
+        # Ultra liquid predefined list
+        if symbol in self.ultra_liquid_symbols:
+            tier = LiquidityTier.ULTRA_LIQUID
+        else:
+            # Classify based on metrics
+            tier = self._classify_by_metrics(symbol, market_state)
+
+        # Cache result
+        self.liquidity_cache[symbol] = (tier, datetime.now())
+        return tier
+
+    def _classify_by_metrics(self, symbol: str, market_state: MarketState) -> LiquidityTier:
+        """Classify liquidity based on market metrics."""
+        try:
+            # Spread-based classification
+            spread_bps = market_state.spread_bps
+
+            # Volume-based classification
+            avg_volume = market_state.volume * (1 / max(0.1, market_state.volume_ratio_1m))
+
+            if spread_bps <= 2 and avg_volume >= 1_000_000:
+                return LiquidityTier.HIGH_LIQUID
+            elif spread_bps <= 5 and avg_volume >= 100_000:
+                return LiquidityTier.MEDIUM_LIQUID
+            else:
+                return LiquidityTier.LOW_LIQUID
+
+        except Exception as e:
+            self.logger.warning(f"Error classifying liquidity for {symbol}: {e}")
+            return LiquidityTier.LOW_LIQUID
+
+
+class VolumeAnomalyDetector:
+    """Detects unusual volume patterns."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.volume_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
+
+    def detect_volume_spike(self, market_state: MarketState) -> Optional[AnomalySignal]:
+        """Detect volume spikes indicating potential breakouts."""
+        symbol = market_state.symbol
+        volume_ratio = market_state.volume_ratio_1m
+
+        # Volume spike thresholds by liquidity
+        liquidity_classifier = LiquidityClassifier()
+        liquidity_tier = liquidity_classifier.classify_liquidity(symbol, market_state)
+
+        thresholds = {
+            LiquidityTier.ULTRA_LIQUID: 3.0,   # 3x normal volume
+            LiquidityTier.HIGH_LIQUID: 2.5,    # 2.5x normal volume
+            LiquidityTier.MEDIUM_LIQUID: 2.0,  # 2x normal volume
+            LiquidityTier.LOW_LIQUID: 1.8      # 1.8x normal volume
+        }
+
+        threshold = thresholds[liquidity_tier]
+
+        if volume_ratio >= threshold:
+            # Calculate strength based on how much above threshold
+            strength = min(1.0, (volume_ratio - threshold) / threshold)
+
+            # Determine position sizing based on liquidity and strength
+            max_position_sizes = {
+                LiquidityTier.ULTRA_LIQUID: 1_000_000,
+                LiquidityTier.HIGH_LIQUID: 500_000,
+                LiquidityTier.MEDIUM_LIQUID: 100_000,
+                LiquidityTier.LOW_LIQUID: 25_000
+            }
+
+            max_position = max_position_sizes[liquidity_tier] * strength
+
+            # Set stops and targets based on momentum direction
+            momentum = market_state.momentum_5m
+            if momentum > 0:  # Upward momentum
+                stop_loss = market_state.price * 0.98    # 2% stop
+                take_profit = market_state.price * 1.04  # 4% target
+            else:  # Downward momentum
+                stop_loss = market_state.price * 1.02    # 2% stop
+                take_profit = market_state.price * 0.96  # 4% target
+
+            return AnomalySignal(
+                symbol=symbol,
+                anomaly_type=AnomalyType.VOLUME_SPIKE,
+                strength=strength,
+                timestamp=market_state.timestamp,
+                expected_duration_minutes=30,
+                target_profit_bps=200,  # 200 bps target
+                max_position_size=max_position,
+                entry_price=market_state.price,
+                stop_loss=stop_loss,
+                take_profit=take_profit,
+                liquidity_tier=liquidity_tier,
+                metadata={
+                    'volume_ratio': volume_ratio,
+                    'threshold': threshold,
+                    'momentum_5m': momentum
+                }
+            )
+
+        return None
+
+
+class SpreadAnomalyDetector:
+    """Detects bid-ask spread anomalies."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.spread_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
+
+    def detect_spread_compression(self, market_state: MarketState) -> Optional[AnomalySignal]:
+        """Detect unusual spread compression indicating liquidity influx."""
+        symbol = market_state.symbol
+        current_spread = market_state.spread_bps
+
+        # Store historical spreads
+        self.spread_history[symbol].append((market_state.timestamp, current_spread))
+
+        if len(self.spread_history[symbol]) < 20:
+            return None  # Need history
+
+        # Calculate average spread over last 20 periods
+        recent_spreads = [spread for _, spread in list(self.spread_history[symbol])[-20:]]
+        avg_spread = np.mean(recent_spreads)
+        std_spread = np.std(recent_spreads)
+
+        # Detect compression (current spread significantly below average)
+        if std_spread > 0 and current_spread < (avg_spread - 2 * std_spread):
+            compression_strength = min(1.0, (avg_spread - current_spread) / avg_spread)
+
+            # Spread compression often precedes volatility
+            return AnomalySignal(
+                symbol=symbol,
+                anomaly_type=AnomalyType.SPREAD_COMPRESSION,
+                strength=compression_strength,
+                timestamp=market_state.timestamp,
+                expected_duration_minutes=15,
+                target_profit_bps=100,
+                max_position_size=50_000,  # Smaller position for volatility plays
+                entry_price=market_state.price,
+                stop_loss=market_state.price * 0.995,  # Tight stop
+                take_profit=market_state.price * 1.01,  # Quick profit
+                metadata={
+                    'current_spread_bps': current_spread,
+                    'avg_spread_bps': avg_spread,
+                    'compression_strength': compression_strength
+                }
+            )
+
+        return None
+
+
+class MomentumAnomalyDetector:
+    """Detects momentum-based anomalies."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+
+    def detect_momentum_reversal(self, market_state: MarketState) -> Optional[AnomalySignal]:
+        """Detect potential momentum reversals."""
+        momentum_5m = market_state.momentum_5m
+        momentum_1h = market_state.momentum_1h
+        rsi = market_state.rsi_14
+
+        # Look for divergence between short and long momentum
+        momentum_divergence = abs(momentum_5m - momentum_1h)
+
+        # RSI extremes suggest reversal potential
+        rsi_extreme = rsi <= 30 or rsi >= 70
+
+        if momentum_divergence > 0.02 and rsi_extreme:  # 2% divergence
+            # Determine reversal direction
+            if rsi <= 30 and momentum_5m > momentum_1h:
+                # Oversold with recent uptick
+                direction = "long"
+                stop_loss = market_state.price * 0.97
+                take_profit = market_state.price * 1.05
+            elif rsi >= 70 and momentum_5m < momentum_1h:
+                # Overbought with recent downtick
+                direction = "short"
+                stop_loss = market_state.price * 1.03
+                take_profit = market_state.price * 0.95
+            else:
+                return None
+
+            strength = min(1.0, momentum_divergence / 0.05)  # Scale to 5% max
+
+            return AnomalySignal(
+                symbol=market_state.symbol,
+                anomaly_type=AnomalyType.MOMENTUM_REVERSAL,
+                strength=strength,
+                timestamp=market_state.timestamp,
+                expected_duration_minutes=60,
+                target_profit_bps=300,
+                max_position_size=100_000,
+                entry_price=market_state.price,
+                stop_loss=stop_loss,
+                take_profit=take_profit,
+                metadata={
+                    'direction': direction,
+                    'momentum_5m': momentum_5m,
+                    'momentum_1h': momentum_1h,
+                    'rsi': rsi,
+                    'divergence': momentum_divergence
+                }
+            )
+
+        return None
+
+
+class PriceGapDetector:
+    """Detects and analyzes price gaps."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.previous_close: Dict[str, float] = {}
+
+    def detect_gap_fill_opportunity(self, market_state: MarketState) -> Optional[AnomalySignal]:
+        """Detect gaps that are likely to fill."""
+        symbol = market_state.symbol
+        current_price = market_state.price
+
+        if symbol not in self.previous_close:
+            self.previous_close[symbol] = current_price
+            return None
+
+        prev_close = self.previous_close[symbol]
+        gap_percent = (current_price - prev_close) / prev_close
+
+        # Look for significant gaps (>1% for liquid stocks)
+        if abs(gap_percent) > 0.01:
+            # Gap up or gap down
+            gap_direction = "up" if gap_percent > 0 else "down"
+
+            # Gap fill probability based on volume and momentum
+            volume_confirmation = market_state.volume_ratio_1m > 1.5
+            momentum_alignment = (gap_percent > 0 and market_state.momentum_5m > 0) or \
+                               (gap_percent < 0 and market_state.momentum_5m < 0)
+
+            if not momentum_alignment and volume_confirmation:
+                # Gap without momentum support - likely to fill
+                strength = min(1.0, abs(gap_percent) / 0.05)  # Scale to 5% max gap
+
+                if gap_direction == "up":
+                    # Gap up likely to fill down
+                    target_price = prev_close
+                    stop_loss = current_price * 1.02
+                else:
+                    # Gap down likely to fill up
+                    target_price = prev_close
+                    stop_loss = current_price * 0.98
+
+                return AnomalySignal(
+                    symbol=symbol,
+                    anomaly_type=AnomalyType.PRICE_GAP,
+                    strength=strength,
+                    timestamp=market_state.timestamp,
+                    expected_duration_minutes=120,
+                    target_profit_bps=int(abs(gap_percent) * 10000 * 0.8),  # 80% of gap
+                    max_position_size=200_000,
+                    entry_price=current_price,
+                    stop_loss=stop_loss,
+                    take_profit=target_price,
+                    metadata={
+                        'gap_percent': gap_percent,
+                        'gap_direction': gap_direction,
+                        'previous_close': prev_close,
+                        'volume_confirmation': volume_confirmation
+                    }
+                )
+
+        # Update previous close for next check
+        self.previous_close[symbol] = current_price
+        return None
+
+
+class LiquidAnomalyEngine:
+    """Main engine for detecting liquid market anomalies."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.detectors = {
+            'volume': VolumeAnomalyDetector(),
+            'spread': SpreadAnomalyDetector(),
+            'momentum': MomentumAnomalyDetector(),
+            'gap': PriceGapDetector()
+        }
+        self.liquidity_classifier = LiquidityClassifier()
+        self.detected_signals: List[AnomalySignal] = []
+        self.watchlist = self._build_liquid_watchlist()
+
+    def _build_liquid_watchlist(self) -> List[str]:
+        """Build watchlist of liquid securities for monitoring."""
+        return [
+            # Major ETFs
+            'SPY', 'QQQ', 'IWM', 'VTI', 'VEA', 'VWO', 'GLD', 'SLV', 'TLT', 'HYG',
+
+            # Mega caps
+            'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'TSLA', 'META', 'NVDA', 'BRK.B',
+
+            # High volume stocks
+            'BAC', 'JPM', 'WFC', 'GE', 'F', 'AMD', 'INTC', 'XOM', 'CVX',
+            'JNJ', 'PFE', 'UNH', 'V', 'MA', 'DIS', 'KO', 'PEP', 'WMT', 'HD'
+        ]
+
+    async def scan_for_anomalies(self, market_states: Dict[str, MarketState]) -> List[AnomalySignal]:
+        """Scan market states for anomalies across all detectors."""
+        new_signals = []
+
+        for symbol, market_state in market_states.items():
+            # Only scan liquid securities
+            liquidity_tier = self.liquidity_classifier.classify_liquidity(symbol, market_state)
+            if liquidity_tier == LiquidityTier.LOW_LIQUID:
+                continue
+
+            # Run all detectors
+            for detector_name, detector in self.detectors.items():
+                try:
+                    if detector_name == 'volume':
+                        signal = detector.detect_volume_spike(market_state)
+                    elif detector_name == 'spread':
+                        signal = detector.detect_spread_compression(market_state)
+                    elif detector_name == 'momentum':
+                        signal = detector.detect_momentum_reversal(market_state)
+                    elif detector_name == 'gap':
+                        signal = detector.detect_gap_fill_opportunity(market_state)
+                    else:
+                        continue
+
+                    if signal and self._validate_signal(signal):
+                        new_signals.append(signal)
+                        self.logger.info(f"Detected {signal.anomaly_type.value} in {symbol}: {signal.strength:.2f}")
+
+                except Exception as e:
+                    self.logger.error(f"Detector {detector_name} failed for {symbol}: {e}")
+
+        # Filter and rank signals
+        filtered_signals = self._filter_and_rank_signals(new_signals)
+        self.detected_signals.extend(filtered_signals)
+
+        return filtered_signals
+
+    def _validate_signal(self, signal: AnomalySignal) -> bool:
+        """Validate signal meets minimum criteria."""
+        # Minimum strength threshold
+        if signal.strength < 0.3:
+            return False
+
+        # Minimum profit target
+        if signal.target_profit_bps < 50:  # 50 bps minimum
+            return False
+
+        # Risk/reward ratio check
+        entry_price = signal.entry_price
+        stop_distance = abs(entry_price - signal.stop_loss) / entry_price
+        profit_distance = abs(signal.take_profit - entry_price) / entry_price
+
+        if profit_distance / stop_distance < 1.5:  # Min 1.5:1 reward/risk
+            return False
+
+        return True
+
+    def _filter_and_rank_signals(self, signals: List[AnomalySignal]) -> List[AnomalySignal]:
+        """Filter duplicate signals and rank by attractiveness."""
+        # Remove duplicates (same symbol, same type within 5 minutes)
+        filtered = []
+        seen = set()
+
+        for signal in signals:
+            key = (signal.symbol, signal.anomaly_type.value)
+            if key not in seen:
+                filtered.append(signal)
+                seen.add(key)
+
+        # Rank by combined score: strength * liquidity_weight * profit_target
+        liquidity_weights = {
+            LiquidityTier.ULTRA_LIQUID: 1.0,
+            LiquidityTier.HIGH_LIQUID: 0.8,
+            LiquidityTier.MEDIUM_LIQUID: 0.6,
+            LiquidityTier.LOW_LIQUID: 0.3
+        }
+
+        for signal in filtered:
+            liquidity_weight = liquidity_weights[signal.liquidity_tier]
+            signal.metadata['rank_score'] = (
+                signal.strength *
+                liquidity_weight *
+                (signal.target_profit_bps / 100)
+            )
+
+        # Sort by rank score descending
+        filtered.sort(key=lambda s: s.metadata.get('rank_score', 0), reverse=True)
+
+        return filtered[:10]  # Return top 10 signals
+
+    def get_current_signals(self, max_age_minutes: int = 30) -> List[AnomalySignal]:
+        """Get currently active signals."""
+        cutoff_time = datetime.now() - timedelta(minutes=max_age_minutes)
+        return [s for s in self.detected_signals if s.timestamp >= cutoff_time]
+
+    async def generate_market_states(self) -> Dict[str, MarketState]:
+        """Generate market states for watchlist symbols."""
+        market_states = {}
+
+        # In production, this would connect to real market data
+        # For demo, generate realistic market states
+        for symbol in self.watchlist[:10]:  # Limit for demo
+            try:
+                # Simulate market data
+                price = 100 + np.random.normal(0, 5)
+                volume = int(np.random.lognormal(13, 1))  # ~300K average
+                spread_bps = max(1, np.random.exponential(3))
+
+                market_state = MarketState(
+                    symbol=symbol,
+                    timestamp=datetime.now(),
+                    price=price,
+                    volume=volume,
+                    bid=price - spread_bps/10000 * price / 2,
+                    ask=price + spread_bps/10000 * price / 2,
+                    spread_bps=spread_bps,
+                    volume_ratio_1m=np.random.lognormal(0, 0.5),
+                    momentum_5m=np.random.normal(0, 0.02),
+                    momentum_1h=np.random.normal(0, 0.05),
+                    rsi_14=np.random.uniform(20, 80),
+                    volatility_20d=np.random.uniform(0.15, 0.35)
+                )
+
+                market_states[symbol] = market_state
+
+            except Exception as e:
+                self.logger.error(f"Failed to generate market state for {symbol}: {e}")
+
+        return market_states
+
+    def get_signal_summary(self) -> Dict[str, Any]:
+        """Get summary of detected signals."""
+        active_signals = self.get_current_signals()
+
+        by_type = defaultdict(int)
+        by_liquidity = defaultdict(int)
+
+        for signal in active_signals:
+            by_type[signal.anomaly_type.value] += 1
+            by_liquidity[signal.liquidity_tier.value] += 1
+
+        return {
+            'total_active_signals': len(active_signals),
+            'by_anomaly_type': dict(by_type),
+            'by_liquidity_tier': dict(by_liquidity),
+            'avg_strength': np.mean([s.strength for s in active_signals]) if active_signals else 0,
+            'avg_profit_target_bps': np.mean([s.target_profit_bps for s in active_signals]) if active_signals else 0,
+            'total_potential_position_size': sum([s.max_position_size for s in active_signals])
+        }
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    async def demo_liquid_anomaly_detection():
+        print("=== Liquid Anomaly Detection Demo ===")
+
+        engine = LiquidAnomalyEngine()
+
+        # Generate market states
+        print("Generating market states...")
+        market_states = await engine.generate_market_states()
+        print(f"Generated states for {len(market_states)} symbols")
+
+        # Scan for anomalies
+        print("\nScanning for anomalies...")
+        signals = await engine.scan_for_anomalies(market_states)
+
+        print(f"Detected {len(signals)} anomaly signals:")
+        for signal in signals[:5]:  # Show top 5
+            print(f"  {signal.symbol}: {signal.anomaly_type.value}")
+            print(f"    Strength: {signal.strength:.2f}")
+            print(f"    Target: {signal.target_profit_bps} bps")
+            print(f"    Max Size: ${signal.max_position_size:,.0f}")
+            print(f"    Risk/Reward: 1:{signal.target_profit_bps/200:.1f}")
+            print()
+
+        # Get summary
+        summary = engine.get_signal_summary()
+        print("Signal Summary:")
+        print(f"  Active signals: {summary['total_active_signals']}")
+        print(f"  Avg strength: {summary['avg_strength']:.2f}")
+        print(f"  Avg profit target: {summary['avg_profit_target_bps']:.0f} bps")
+        print(f"  Total position size: ${summary['total_potential_position_size']:,.0f}")
+
+    asyncio.run(demo_liquid_anomaly_detection())
\ No newline at end of file
Index: backend/validation/execution_reality/order_type_testing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/execution_reality/order_type_testing.py b/backend/validation/execution_reality/order_type_testing.py
new file mode 100644
--- /dev/null	(date 1758164685792)
+++ b/backend/validation/execution_reality/order_type_testing.py	(date 1758164685792)
@@ -0,0 +1,568 @@
+"""Order type policy testing and optimization.
+
+Tests different order types (market, limit, IOC) to optimize execution
+quality across different market conditions.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Tuple, Any
+import logging
+from datetime import datetime, timedelta
+from dataclasses import dataclass
+from enum import Enum
+import asyncio
+
+logger = logging.getLogger(__name__)
+
+
+class OrderType(Enum):
+    """Supported order types for testing."""
+    MARKET = "market"
+    LIMIT = "limit"
+    IOC = "ioc"  # Immediate or Cancel
+    FOK = "fok"  # Fill or Kill
+    MARKETABLE_LIMIT = "marketable_limit"  # Limit at/through market
+
+
+@dataclass
+class OrderTest:
+    """Configuration for order type testing."""
+    symbol: str
+    side: str  # 'buy' or 'sell'
+    quantity: int
+    order_types: List[OrderType]
+    market_conditions: Dict[str, float]
+    test_duration_seconds: int = 60
+    max_orders_per_type: int = 10
+
+
+@dataclass
+class OrderResult:
+    """Result of a single order execution."""
+    timestamp: datetime
+    symbol: str
+    order_type: OrderType
+    side: str
+    quantity: int
+    fill_quantity: int
+    fill_price: Optional[float]
+    market_price_at_order: float
+    spread_at_order: float
+    latency_ms: float
+    filled: bool
+    partial_fill: bool
+    adverse_selection_bps: float
+    order_id: str
+
+
+class OrderTypeOptimizer:
+    """Optimizes order type selection based on market conditions."""
+
+    def __init__(self):
+        self.test_results = []
+        self.performance_matrix = {}  # (order_type, market_regime) -> performance metrics
+        self.current_policies = {}  # symbol -> order_type
+
+    def run_order_type_test(self, test_config: OrderTest) -> Dict[str, Any]:
+        """Run comprehensive order type testing.
+
+        Args:
+            test_config: Test configuration
+
+        Returns:
+            Dictionary with test results and recommendations
+        """
+        logger.info(f"Starting order type test for {test_config.symbol}")
+
+        test_results = {
+            'symbol': test_config.symbol,
+            'test_start': datetime.now(),
+            'market_conditions': test_config.market_conditions,
+            'order_results': [],
+            'performance_summary': {},
+            'recommendations': {}
+        }
+
+        # Run tests for each order type
+        for order_type in test_config.order_types:
+            logger.info(f"Testing {order_type.value} orders")
+
+            try:
+                type_results = self._test_single_order_type(
+                    test_config, order_type
+                )
+                test_results['order_results'].extend(type_results)
+
+            except Exception as e:
+                logger.error(f"Failed to test {order_type.value}: {e}")
+                continue
+
+        # Analyze results
+        test_results['performance_summary'] = self._analyze_performance(
+            test_results['order_results']
+        )
+
+        test_results['recommendations'] = self._generate_recommendations(
+            test_results['performance_summary'],
+            test_config.market_conditions
+        )
+
+        # Store results for future optimization
+        self.test_results.append(test_results)
+
+        return test_results
+
+    def _test_single_order_type(self, test_config: OrderTest,
+                               order_type: OrderType) -> List[OrderResult]:
+        """Test a single order type with multiple orders."""
+        results = []
+
+        for i in range(min(test_config.max_orders_per_type, 10)):
+            try:
+                # Simulate order placement and execution
+                result = self._simulate_order_execution(
+                    test_config, order_type, order_sequence=i
+                )
+                results.append(result)
+
+                # Add delay between orders to avoid market impact
+                if i < test_config.max_orders_per_type - 1:
+                    delay = test_config.test_duration_seconds / test_config.max_orders_per_type
+                    # In real implementation, this would be actual time delay
+                    # For simulation, we just adjust the timestamp
+                    pass
+
+            except Exception as e:
+                logger.error(f"Order {i} failed for {order_type.value}: {e}")
+                continue
+
+        return results
+
+    def _simulate_order_execution(self, test_config: OrderTest,
+                                 order_type: OrderType,
+                                 order_sequence: int) -> OrderResult:
+        """Simulate order execution based on order type and market conditions.
+
+        This is a simulation - in production this would interface with actual broker APIs.
+        """
+        symbol = test_config.symbol
+        side = test_config.side
+        quantity = test_config.quantity
+        market_conditions = test_config.market_conditions
+
+        # Generate realistic market data
+        base_price = 100.0  # Placeholder
+        spread_bps = market_conditions.get('spread_bps', 5)
+        volatility = market_conditions.get('volatility', 0.02)
+
+        # Current bid/ask
+        spread_dollars = base_price * spread_bps / 10000
+        if side == 'buy':
+            market_price = base_price + spread_dollars / 2  # Ask price
+            opposite_price = base_price - spread_dollars / 2  # Bid price
+        else:
+            market_price = base_price - spread_dollars / 2  # Bid price
+            opposite_price = base_price + spread_dollars / 2  # Ask price
+
+        # Simulate latency (varies by order type)
+        latency_ms = self._simulate_latency(order_type, market_conditions)
+
+        # Simulate execution based on order type
+        timestamp = datetime.now() + timedelta(seconds=order_sequence * 10)
+        fill_result = self._simulate_fill_behavior(
+            order_type, quantity, market_price, market_conditions
+        )
+
+        # Calculate adverse selection (price movement during latency)
+        adverse_selection_bps = self._calculate_adverse_selection(
+            latency_ms, volatility, market_conditions
+        )
+
+        return OrderResult(
+            timestamp=timestamp,
+            symbol=symbol,
+            order_type=order_type,
+            side=side,
+            quantity=quantity,
+            fill_quantity=fill_result['fill_quantity'],
+            fill_price=fill_result['fill_price'],
+            market_price_at_order=market_price,
+            spread_at_order=spread_dollars,
+            latency_ms=latency_ms,
+            filled=fill_result['filled'],
+            partial_fill=fill_result['partial_fill'],
+            adverse_selection_bps=adverse_selection_bps,
+            order_id=f"{symbol}_{order_type.value}_{order_sequence}_{timestamp.strftime('%H%M%S')}"
+        )
+
+    def _simulate_latency(self, order_type: OrderType,
+                         market_conditions: Dict[str, float]) -> float:
+        """Simulate order latency based on order type."""
+        base_latency = {
+            OrderType.MARKET: 80,
+            OrderType.LIMIT: 50,
+            OrderType.IOC: 90,
+            OrderType.FOK: 95,
+            OrderType.MARKETABLE_LIMIT: 85
+        }
+
+        latency = base_latency.get(order_type, 100)
+
+        # Add market condition adjustments
+        if market_conditions.get('volatility', 0) > 0.03:
+            latency *= 1.2  # Higher latency in volatile markets
+
+        if market_conditions.get('volume', 1000) < 500:
+            latency *= 1.1  # Higher latency in low volume
+
+        # Add random variation
+        latency += np.random.normal(0, 10)
+
+        return max(20, latency)  # Minimum 20ms
+
+    def _simulate_fill_behavior(self, order_type: OrderType, quantity: int,
+                               market_price: float,
+                               market_conditions: Dict[str, float]) -> Dict[str, Any]:
+        """Simulate order fill behavior based on order type."""
+        volume_available = market_conditions.get('volume', 1000)
+        spread_bps = market_conditions.get('spread_bps', 5)
+
+        if order_type == OrderType.MARKET:
+            # Market orders usually fill but may have slippage
+            fill_probability = 0.98
+            if np.random.random() < fill_probability:
+                # Calculate slippage
+                slippage_bps = spread_bps * 0.3 + np.random.exponential(2)
+                slippage_dollars = market_price * slippage_bps / 10000
+                fill_price = market_price + slippage_dollars
+
+                # Check for partial fills in large orders
+                if quantity > volume_available * 0.1:
+                    fill_quantity = min(quantity, int(volume_available * 0.8))
+                    partial_fill = fill_quantity < quantity
+                else:
+                    fill_quantity = quantity
+                    partial_fill = False
+
+                return {
+                    'filled': True,
+                    'fill_quantity': fill_quantity,
+                    'fill_price': fill_price,
+                    'partial_fill': partial_fill
+                }
+            else:
+                return {'filled': False, 'fill_quantity': 0, 'fill_price': None, 'partial_fill': False}
+
+        elif order_type == OrderType.LIMIT:
+            # Limit orders have lower fill probability but better price
+            fill_probability = 0.7
+            if np.random.random() < fill_probability:
+                # Limit orders get better execution
+                improvement_bps = spread_bps * 0.4
+                improvement_dollars = market_price * improvement_bps / 10000
+                fill_price = market_price - improvement_dollars
+
+                # Lower chance of partial fills for limit orders
+                if quantity > volume_available * 0.2:
+                    fill_quantity = min(quantity, int(volume_available * 0.6))
+                    partial_fill = fill_quantity < quantity
+                else:
+                    fill_quantity = quantity
+                    partial_fill = False
+
+                return {
+                    'filled': True,
+                    'fill_quantity': fill_quantity,
+                    'fill_price': fill_price,
+                    'partial_fill': partial_fill
+                }
+            else:
+                return {'filled': False, 'fill_quantity': 0, 'fill_price': None, 'partial_fill': False}
+
+        elif order_type == OrderType.IOC:
+            # IOC fills immediately or not at all
+            fill_probability = 0.85
+            if np.random.random() < fill_probability:
+                # IOC gets reasonable execution but may have more slippage than limit
+                slippage_bps = spread_bps * 0.2 + np.random.exponential(1)
+                slippage_dollars = market_price * slippage_bps / 10000
+                fill_price = market_price + slippage_dollars
+
+                # Higher chance of partial fills for IOC
+                if quantity > volume_available * 0.15:
+                    fill_quantity = min(quantity, int(volume_available * 0.7))
+                    partial_fill = fill_quantity < quantity
+                else:
+                    fill_quantity = quantity
+                    partial_fill = False
+
+                return {
+                    'filled': True,
+                    'fill_quantity': fill_quantity,
+                    'fill_price': fill_price,
+                    'partial_fill': partial_fill
+                }
+            else:
+                return {'filled': False, 'fill_quantity': 0, 'fill_price': None, 'partial_fill': False}
+
+        elif order_type == OrderType.FOK:
+            # FOK fills completely or not at all
+            fill_probability = 0.75
+            can_fill_completely = quantity <= volume_available * 0.5
+
+            if np.random.random() < fill_probability and can_fill_completely:
+                slippage_bps = spread_bps * 0.25
+                slippage_dollars = market_price * slippage_bps / 10000
+                fill_price = market_price + slippage_dollars
+
+                return {
+                    'filled': True,
+                    'fill_quantity': quantity,  # FOK fills completely or not at all
+                    'fill_price': fill_price,
+                    'partial_fill': False
+                }
+            else:
+                return {'filled': False, 'fill_quantity': 0, 'fill_price': None, 'partial_fill': False}
+
+        elif order_type == OrderType.MARKETABLE_LIMIT:
+            # Marketable limit orders behave like aggressive limit orders
+            fill_probability = 0.92
+            if np.random.random() < fill_probability:
+                # Good execution, similar to aggressive limit
+                slippage_bps = spread_bps * 0.1
+                slippage_dollars = market_price * slippage_bps / 10000
+                fill_price = market_price + slippage_dollars
+
+                if quantity > volume_available * 0.12:
+                    fill_quantity = min(quantity, int(volume_available * 0.75))
+                    partial_fill = fill_quantity < quantity
+                else:
+                    fill_quantity = quantity
+                    partial_fill = False
+
+                return {
+                    'filled': True,
+                    'fill_quantity': fill_quantity,
+                    'fill_price': fill_price,
+                    'partial_fill': partial_fill
+                }
+            else:
+                return {'filled': False, 'fill_quantity': 0, 'fill_price': None, 'partial_fill': False}
+
+        # Default fallback
+        return {'filled': False, 'fill_quantity': 0, 'fill_price': None, 'partial_fill': False}
+
+    def _calculate_adverse_selection(self, latency_ms: float, volatility: float,
+                                   market_conditions: Dict[str, float]) -> float:
+        """Calculate adverse selection cost due to latency."""
+        # Convert latency to fraction of day
+        latency_fraction = latency_ms / (1000 * 60 * 60 * 24)
+
+        # Estimate price movement during latency
+        price_std = volatility * np.sqrt(latency_fraction)
+
+        # Adverse selection is directional - market moves against us
+        adverse_selection = np.random.normal(0, price_std) * 0.5  # 50% of movement is adverse
+
+        return abs(adverse_selection) * 10000  # Convert to basis points
+
+    def _analyze_performance(self, order_results: List[OrderResult]) -> Dict[str, Any]:
+        """Analyze performance across different order types."""
+        if not order_results:
+            return {'error': 'No order results to analyze'}
+
+        # Group results by order type
+        results_by_type = {}
+        for result in order_results:
+            order_type = result.order_type.value
+            if order_type not in results_by_type:
+                results_by_type[order_type] = []
+            results_by_type[order_type].append(result)
+
+        # Calculate metrics for each order type
+        performance_summary = {}
+        for order_type, results in results_by_type.items():
+            if not results:
+                continue
+
+            filled_results = [r for r in results if r.filled]
+
+            metrics = {
+                'total_orders': len(results),
+                'filled_orders': len(filled_results),
+                'fill_rate': len(filled_results) / len(results) if results else 0,
+                'avg_latency_ms': np.mean([r.latency_ms for r in results]),
+                'avg_adverse_selection_bps': np.mean([r.adverse_selection_bps for r in results]),
+                'partial_fill_rate': np.mean([r.partial_fill for r in results]),
+                'avg_fill_ratio': 0,
+                'execution_quality_score': 0
+            }
+
+            if filled_results:
+                # Calculate slippage for filled orders
+                slippages = []
+                for result in filled_results:
+                    if result.fill_price and result.market_price_at_order:
+                        if result.side == 'buy':
+                            slippage_bps = (result.fill_price - result.market_price_at_order) / result.market_price_at_order * 10000
+                        else:
+                            slippage_bps = (result.market_price_at_order - result.fill_price) / result.market_price_at_order * 10000
+                        slippages.append(slippage_bps)
+
+                metrics['avg_slippage_bps'] = np.mean(slippages) if slippages else 0
+                metrics['slippage_std_bps'] = np.std(slippages) if slippages else 0
+
+                # Average fill ratio for filled orders
+                fill_ratios = [r.fill_quantity / r.quantity for r in filled_results if r.quantity > 0]
+                metrics['avg_fill_ratio'] = np.mean(fill_ratios) if fill_ratios else 0
+
+                # Calculate execution quality score (0-1, higher is better)
+                quality_score = 1.0
+                quality_score -= min(metrics['avg_slippage_bps'] / 20, 0.3)  # Penalize slippage
+                quality_score -= min(metrics['avg_latency_ms'] / 200, 0.2)  # Penalize latency
+                quality_score -= min(metrics['partial_fill_rate'] * 0.5, 0.2)  # Penalize partial fills
+                quality_score -= min(metrics['avg_adverse_selection_bps'] / 10, 0.2)  # Penalize adverse selection
+                quality_score += (metrics['fill_rate'] - 0.8) * 0.5  # Bonus for high fill rate
+
+                metrics['execution_quality_score'] = max(0, quality_score)
+
+            performance_summary[order_type] = metrics
+
+        return performance_summary
+
+    def _generate_recommendations(self, performance_summary: Dict[str, Any],
+                                market_conditions: Dict[str, float]) -> Dict[str, Any]:
+        """Generate order type recommendations based on test results."""
+        if not performance_summary:
+            return {'error': 'No performance data to analyze'}
+
+        recommendations = {
+            'primary_recommendation': None,
+            'alternative_recommendation': None,
+            'market_condition_based': {},
+            'reasoning': []
+        }
+
+        # Find best performing order type overall
+        best_order_type = None
+        best_score = -1
+
+        for order_type, metrics in performance_summary.items():
+            if 'execution_quality_score' in metrics:
+                score = metrics['execution_quality_score']
+                if score > best_score:
+                    best_score = score
+                    best_order_type = order_type
+
+        if best_order_type:
+            recommendations['primary_recommendation'] = best_order_type
+            recommendations['reasoning'].append(
+                f"{best_order_type} achieved highest execution quality score: {best_score:.3f}"
+            )
+
+        # Market condition based recommendations
+        volatility = market_conditions.get('volatility', 0.02)
+        spread_bps = market_conditions.get('spread_bps', 5)
+        volume = market_conditions.get('volume', 1000)
+
+        if volatility > 0.03:  # High volatility
+            recommendations['market_condition_based']['high_volatility'] = 'ioc'
+            recommendations['reasoning'].append(
+                "High volatility detected - IOC recommended for immediate execution"
+            )
+
+        if spread_bps > 15:  # Wide spreads
+            recommendations['market_condition_based']['wide_spreads'] = 'limit'
+            recommendations['reasoning'].append(
+                "Wide spreads detected - Limit orders recommended to avoid excessive slippage"
+            )
+
+        if volume < 500:  # Low volume
+            recommendations['market_condition_based']['low_volume'] = 'limit'
+            recommendations['reasoning'].append(
+                "Low volume detected - Limit orders recommended to reduce market impact"
+            )
+
+        # Find alternative recommendation (second best)
+        sorted_types = sorted(
+            [(ot, metrics.get('execution_quality_score', 0)) for ot, metrics in performance_summary.items()],
+            key=lambda x: x[1], reverse=True
+        )
+
+        if len(sorted_types) > 1:
+            recommendations['alternative_recommendation'] = sorted_types[1][0]
+
+        return recommendations
+
+    def update_policies(self, symbol: str, test_results: Dict[str, Any]):
+        """Update order type policies based on test results."""
+        if 'recommendations' not in test_results:
+            return
+
+        recommendations = test_results['recommendations']
+        primary_rec = recommendations.get('primary_recommendation')
+
+        if primary_rec:
+            self.current_policies[symbol] = OrderType(primary_rec)
+            logger.info(f"Updated order type policy for {symbol}: {primary_rec}")
+
+    def get_recommended_order_type(self, symbol: str,
+                                 market_conditions: Dict[str, float]) -> OrderType:
+        """Get recommended order type for current market conditions."""
+        # Check if we have a current policy for this symbol
+        if symbol in self.current_policies:
+            base_recommendation = self.current_policies[symbol]
+        else:
+            base_recommendation = OrderType.LIMIT  # Default to limit orders
+
+        # Override based on current market conditions
+        volatility = market_conditions.get('volatility', 0.02)
+        spread_bps = market_conditions.get('spread_bps', 5)
+
+        if volatility > 0.04 and spread_bps < 10:
+            return OrderType.IOC  # Fast execution in volatile but liquid markets
+
+        if spread_bps > 20:
+            return OrderType.LIMIT  # Patient execution in wide spread markets
+
+        return base_recommendation
+
+    def get_performance_history(self, hours: int = 24) -> Dict[str, Any]:
+        """Get performance history for order types."""
+        cutoff_time = datetime.now() - timedelta(hours=hours)
+
+        recent_tests = [
+            test for test in self.test_results
+            if test['test_start'] >= cutoff_time
+        ]
+
+        if not recent_tests:
+            return {'error': 'No recent test data available'}
+
+        # Aggregate performance across all recent tests
+        aggregated_performance = {}
+        for test in recent_tests:
+            if 'performance_summary' in test:
+                for order_type, metrics in test['performance_summary'].items():
+                    if order_type not in aggregated_performance:
+                        aggregated_performance[order_type] = []
+                    aggregated_performance[order_type].append(metrics)
+
+        # Calculate averages
+        summary = {}
+        for order_type, metrics_list in aggregated_performance.items():
+            if metrics_list:
+                summary[order_type] = {
+                    'avg_fill_rate': np.mean([m.get('fill_rate', 0) for m in metrics_list]),
+                    'avg_latency_ms': np.mean([m.get('avg_latency_ms', 0) for m in metrics_list]),
+                    'avg_slippage_bps': np.mean([m.get('avg_slippage_bps', 0) for m in metrics_list]),
+                    'avg_quality_score': np.mean([m.get('execution_quality_score', 0) for m in metrics_list]),
+                    'test_count': len(metrics_list)
+                }
+
+        return {
+            'period_hours': hours,
+            'tests_analyzed': len(recent_tests),
+            'order_type_performance': summary
+        }
\ No newline at end of file
Index: backend/validation/statistical_rigor/reality_check.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/statistical_rigor/reality_check.py b/backend/validation/statistical_rigor/reality_check.py
new file mode 100644
--- /dev/null	(date 1758169876676)
+++ b/backend/validation/statistical_rigor/reality_check.py	(date 1758169876676)
@@ -0,0 +1,448 @@
+"""White's Reality Check and SPA Test for multiple hypothesis control.
+
+Prevents false discovery of edges when testing multiple strategies by controlling
+for data mining bias and multiple testing issues.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Tuple, Any
+import logging
+from scipy import stats
+from dataclasses import dataclass
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class RealityCheckResult:
+    """Results from White's Reality Check test."""
+    test_statistic: float
+    p_value: float
+    bootstrap_p_value: float
+    significant: bool
+    benchmark_performance: float
+    best_strategy_performance: float
+    best_strategy_name: str
+    num_strategies_tested: int
+    num_bootstrap_samples: int
+
+
+class WhitesRealityCheck:
+    """Implementation of White's Reality Check for multiple strategy testing.
+
+    Tests the null hypothesis that the best performing strategy is not
+    significantly better than a benchmark after accounting for data mining bias.
+    """
+
+    def __init__(self, num_bootstrap_samples: int = 1000, block_size: Optional[int] = None):
+        self.num_bootstrap_samples = num_bootstrap_samples
+        self.block_size = block_size  # For block bootstrap if data is serially correlated
+
+    def run_reality_check(self,
+                         strategy_returns: Dict[str, pd.Series],
+                         benchmark_returns: pd.Series,
+                         alpha: float = 0.05) -> RealityCheckResult:
+        """Run White's Reality Check test.
+
+        Args:
+            strategy_returns: Dictionary of strategy name -> return series
+            benchmark_returns: Benchmark return series
+            alpha: Significance level
+
+        Returns:
+            RealityCheckResult with test statistics and conclusions
+        """
+        if not strategy_returns:
+            raise ValueError("No strategy returns provided")
+
+        # Align all returns to common dates
+        aligned_data = self._align_returns(strategy_returns, benchmark_returns)
+        strategy_matrix = aligned_data['strategies']
+        benchmark_series = aligned_data['benchmark']
+
+        if len(benchmark_series) < 30:
+            raise ValueError("Insufficient data for Reality Check (minimum 30 observations)")
+
+        # Calculate excess returns over benchmark
+        excess_returns = strategy_matrix.subtract(benchmark_series, axis=0)
+
+        # Find best performing strategy
+        mean_excess = excess_returns.mean()
+        best_strategy_idx = mean_excess.idxmax()
+        best_strategy_performance = mean_excess[best_strategy_idx]
+
+        # Calculate test statistic (t-statistic of best strategy)
+        best_excess_returns = excess_returns[best_strategy_idx]
+        test_statistic = self._calculate_t_statistic(best_excess_returns)
+
+        # Standard p-value (without multiple testing correction)
+        standard_p_value = 2 * (1 - stats.t.cdf(abs(test_statistic), len(best_excess_returns) - 1))
+
+        # Bootstrap p-value accounting for multiple testing
+        bootstrap_p_value = self._bootstrap_p_value(excess_returns, test_statistic)
+
+        # Test significance
+        significant = bootstrap_p_value <= alpha
+
+        return RealityCheckResult(
+            test_statistic=test_statistic,
+            p_value=standard_p_value,
+            bootstrap_p_value=bootstrap_p_value,
+            significant=significant,
+            benchmark_performance=benchmark_series.mean(),
+            best_strategy_performance=strategy_matrix[best_strategy_idx].mean(),
+            best_strategy_name=best_strategy_idx,
+            num_strategies_tested=len(strategy_returns),
+            num_bootstrap_samples=self.num_bootstrap_samples
+        )
+
+    def _align_returns(self, strategy_returns: Dict[str, pd.Series],
+                      benchmark_returns: pd.Series) -> Dict[str, Any]:
+        """Align all return series to common dates."""
+        # Combine all series
+        all_series = {**strategy_returns, 'benchmark': benchmark_returns}
+        combined_df = pd.DataFrame(all_series)
+
+        # Drop rows with any NaN values
+        clean_df = combined_df.dropna()
+
+        if len(clean_df) == 0:
+            raise ValueError("No common dates found across all return series")
+
+        benchmark_clean = clean_df['benchmark']
+        strategies_clean = clean_df.drop('benchmark', axis=1)
+
+        return {
+            'strategies': strategies_clean,
+            'benchmark': benchmark_clean
+        }
+
+    def _calculate_t_statistic(self, returns: pd.Series) -> float:
+        """Calculate t-statistic for return series."""
+        if len(returns) <= 1:
+            return 0.0
+
+        mean_return = returns.mean()
+        std_return = returns.std(ddof=1)
+
+        if std_return == 0:
+            return 0.0 if mean_return == 0 else np.inf * np.sign(mean_return)
+
+        return mean_return / (std_return / np.sqrt(len(returns)))
+
+    def _bootstrap_p_value(self, excess_returns: pd.DataFrame,
+                          observed_test_stat: float) -> float:
+        """Calculate bootstrap p-value for multiple testing correction."""
+        bootstrap_stats = []
+
+        for _ in range(self.num_bootstrap_samples):
+            # Generate bootstrap sample
+            if self.block_size:
+                bootstrap_sample = self._block_bootstrap(excess_returns)
+            else:
+                bootstrap_sample = self._iid_bootstrap(excess_returns)
+
+            # Calculate test statistic for this bootstrap sample
+            # Under null hypothesis, center the bootstrap sample at zero
+            centered_sample = bootstrap_sample - bootstrap_sample.mean()
+
+            # Find maximum t-statistic across all strategies in bootstrap sample
+            max_t_stat = -np.inf
+            for strategy in centered_sample.columns:
+                strategy_returns = centered_sample[strategy]
+                t_stat = self._calculate_t_statistic(strategy_returns)
+                max_t_stat = max(max_t_stat, t_stat)
+
+            bootstrap_stats.append(max_t_stat)
+
+        # Calculate p-value
+        bootstrap_stats = np.array(bootstrap_stats)
+        p_value = np.mean(bootstrap_stats >= observed_test_stat)
+
+        return p_value
+
+    def _iid_bootstrap(self, data: pd.DataFrame) -> pd.DataFrame:
+        """Generate IID bootstrap sample."""
+        n = len(data)
+        bootstrap_indices = np.random.choice(n, size=n, replace=True)
+        return data.iloc[bootstrap_indices].reset_index(drop=True)
+
+    def _block_bootstrap(self, data: pd.DataFrame) -> pd.DataFrame:
+        """Generate block bootstrap sample for time series data."""
+        n = len(data)
+        if self.block_size is None:
+            self.block_size = int(np.sqrt(n))  # Rule of thumb
+
+        num_blocks = int(np.ceil(n / self.block_size))
+        bootstrap_sample = []
+
+        for _ in range(num_blocks):
+            start_idx = np.random.randint(0, n - self.block_size + 1)
+            block = data.iloc[start_idx:start_idx + self.block_size]
+            bootstrap_sample.append(block)
+
+        # Concatenate blocks and trim to original length
+        bootstrap_df = pd.concat(bootstrap_sample, ignore_index=True)
+        return bootstrap_df.iloc[:n]
+
+
+class SPATest:
+    """Superior Predictive Ability (SPA) Test by Hansen (2005).
+
+    Extension of White's Reality Check with improved power and finite sample properties.
+    """
+
+    def __init__(self, num_bootstrap_samples: int = 1000):
+        self.num_bootstrap_samples = num_bootstrap_samples
+
+    def run_spa_test(self,
+                    strategy_returns: Dict[str, pd.Series],
+                    benchmark_returns: pd.Series,
+                    alpha: float = 0.05) -> Dict[str, Any]:
+        """Run SPA test for superior predictive ability.
+
+        Args:
+            strategy_returns: Dictionary of strategy returns
+            benchmark_returns: Benchmark returns
+            alpha: Significance level
+
+        Returns:
+            Dictionary with SPA test results
+        """
+        # Align returns
+        reality_check = WhitesRealityCheck(self.num_bootstrap_samples)
+        aligned_data = reality_check._align_returns(strategy_returns, benchmark_returns)
+
+        strategy_matrix = aligned_data['strategies']
+        benchmark_series = aligned_data['benchmark']
+
+        # Calculate excess returns
+        excess_returns = strategy_matrix.subtract(benchmark_series, axis=0)
+
+        # Calculate performance measures for all strategies
+        strategy_performance = self._calculate_spa_statistics(excess_returns)
+
+        # Run bootstrap test
+        spa_p_values = self._spa_bootstrap(excess_returns, strategy_performance)
+
+        # Find strategies with significant outperformance
+        significant_strategies = []
+        for strategy_name, p_value in spa_p_values.items():
+            if p_value <= alpha:
+                significant_strategies.append({
+                    'name': strategy_name,
+                    'p_value': p_value,
+                    'performance': strategy_performance[strategy_name]['mean'],
+                    't_statistic': strategy_performance[strategy_name]['t_stat']
+                })
+
+        return {
+            'significant_strategies': significant_strategies,
+            'all_p_values': spa_p_values,
+            'all_performance': strategy_performance,
+            'num_strategies_tested': len(strategy_returns),
+            'alpha': alpha,
+            'test_conclusion': 'significant_outperformance' if significant_strategies else 'no_significant_outperformance'
+        }
+
+    def _calculate_spa_statistics(self, excess_returns: pd.DataFrame) -> Dict[str, Dict]:
+        """Calculate performance statistics for SPA test."""
+        performance = {}
+
+        for strategy in excess_returns.columns:
+            returns = excess_returns[strategy]
+            mean_return = returns.mean()
+
+            # Calculate HAC-robust standard error
+            std_error = self._calculate_hac_std_error(returns)
+
+            t_statistic = mean_return / std_error if std_error > 0 else 0
+
+            performance[strategy] = {
+                'mean': mean_return,
+                'std_error': std_error,
+                't_stat': t_statistic,
+                'observations': len(returns)
+            }
+
+        return performance
+
+    def _calculate_hac_std_error(self, returns: pd.Series, max_lags: int | None = None) -> float:
+        """Calculate HAC (Heteroskedasticity and Autocorrelation Consistent) standard error."""
+        n = len(returns)
+        if max_lags is None:
+            max_lags = int(4 * (n / 100) ** (2/9))  # Newey-West lag selection
+
+        mean_return = returns.mean()
+        centered_returns = returns - mean_return
+
+        # Calculate variance with Newey-West correction
+        gamma_0 = np.mean(centered_returns ** 2)
+
+        gamma_sum = 0
+        for lag in range(1, min(max_lags + 1, n)):
+            if n - lag <= 0:
+                break
+
+            gamma_lag = np.mean(centered_returns[:-lag] * centered_returns[lag:])
+            weight = 1 - lag / (max_lags + 1)  # Bartlett kernel
+            gamma_sum += 2 * weight * gamma_lag
+
+        robust_variance = (gamma_0 + gamma_sum) / n
+        return np.sqrt(max(robust_variance, 1e-10))  # Avoid numerical issues
+
+    def _spa_bootstrap(self, excess_returns: pd.DataFrame,
+                      strategy_performance: Dict[str, Dict]) -> Dict[str, float]:
+        """Calculate SPA bootstrap p-values."""
+        p_values = {}
+
+        for strategy_name in excess_returns.columns:
+            observed_t_stat = strategy_performance[strategy_name]['t_stat']
+
+            # Bootstrap distribution under null hypothesis
+            bootstrap_t_stats = []
+
+            for _ in range(self.num_bootstrap_samples):
+                # Resample with replacement
+                bootstrap_sample = excess_returns.sample(n=len(excess_returns), replace=True)
+
+                # Calculate t-statistic for this strategy in bootstrap sample
+                bootstrap_returns = bootstrap_sample[strategy_name]
+                bootstrap_mean = bootstrap_returns.mean()
+                bootstrap_std_error = self._calculate_hac_std_error(bootstrap_returns)
+
+                if bootstrap_std_error > 0:
+                    bootstrap_t_stat = bootstrap_mean / bootstrap_std_error
+                else:
+                    bootstrap_t_stat = 0
+
+                bootstrap_t_stats.append(bootstrap_t_stat)
+
+            # Calculate p-value
+            bootstrap_t_stats = np.array(bootstrap_t_stats)
+            p_value = np.mean(bootstrap_t_stats >= observed_t_stat)
+            p_values[strategy_name] = p_value
+
+        return p_values
+
+
+class MultipleTestingController:
+    """Controller for multiple hypothesis testing in strategy validation."""
+
+    def __init__(self):
+        self.reality_check = WhitesRealityCheck()
+        self.spa_test = SPATest()
+
+    def validate_strategy_universe(self,
+                                 strategy_returns: Dict[str, pd.Series],
+                                 benchmark_returns: pd.Series,
+                                 alpha: float = 0.05) -> Dict[str, Any]:
+        """Comprehensive multiple testing validation.
+
+        Args:
+            strategy_returns: Dictionary of strategy returns
+            benchmark_returns: Benchmark returns (e.g., SPY)
+            alpha: Significance level
+
+        Returns:
+            Comprehensive multiple testing results
+        """
+        logger.info(f"Running multiple testing validation on {len(strategy_returns)} strategies")
+
+        results = {
+            'summary': {},
+            'reality_check': {},
+            'spa_test': {},
+            'recommendations': {}
+        }
+
+        try:
+            # White's Reality Check
+            logger.info("Running White's Reality Check")
+            reality_result = self.reality_check.run_reality_check(
+                strategy_returns, benchmark_returns, alpha
+            )
+            results['reality_check'] = {
+                'test_statistic': reality_result.test_statistic,
+                'p_value': reality_result.p_value,
+                'bootstrap_p_value': reality_result.bootstrap_p_value,
+                'significant': reality_result.significant,
+                'best_strategy': reality_result.best_strategy_name,
+                'best_performance': reality_result.best_strategy_performance,
+                'benchmark_performance': reality_result.benchmark_performance
+            }
+
+            # SPA Test
+            logger.info("Running SPA Test")
+            spa_result = self.spa_test.run_spa_test(
+                strategy_returns, benchmark_returns, alpha
+            )
+            results['spa_test'] = spa_result
+
+            # Summary and recommendations
+            results['summary'] = self._create_summary(reality_result, spa_result)
+            results['recommendations'] = self._create_recommendations(reality_result, spa_result)
+
+        except Exception as e:
+            logger.error(f"Multiple testing validation failed: {e}")
+            results['error'] = str(e)
+
+        return results
+
+    def _create_summary(self, reality_result: RealityCheckResult,
+                       spa_result: Dict[str, Any]) -> Dict[str, Any]:
+        """Create summary of multiple testing results."""
+        return {
+            'total_strategies_tested': reality_result.num_strategies_tested,
+            'reality_check_passed': reality_result.significant,
+            'spa_significant_strategies': len(spa_result.get('significant_strategies', [])),
+            'best_strategy_name': reality_result.best_strategy_name,
+            'best_strategy_outperformance': reality_result.best_strategy_performance - reality_result.benchmark_performance,
+            'data_mining_bias_detected': not reality_result.significant and reality_result.p_value < 0.05,
+            'robust_edge_found': reality_result.significant and len(spa_result.get('significant_strategies', [])) > 0
+        }
+
+    def _create_recommendations(self, reality_result: RealityCheckResult,
+                              spa_result: Dict[str, Any]) -> List[str]:
+        """Create actionable recommendations based on test results."""
+        recommendations = []
+
+        if not reality_result.significant:
+            if reality_result.p_value < 0.05:
+                recommendations.append(
+                    "âš ï¸ Data mining bias detected: Best strategy shows significance in naive test "
+                    "but fails Reality Check. Likely false discovery."
+                )
+            else:
+                recommendations.append(
+                    "âŒ No significant outperformance found. All strategies may be due to luck."
+                )
+
+            recommendations.append(
+                "ðŸ”§ Consider: Reduce strategy universe, increase data sample, or improve strategy design"
+            )
+
+        else:
+            recommendations.append(
+                f"âœ… Reality Check passed: {reality_result.best_strategy_name} shows robust outperformance"
+            )
+
+            significant_strategies = spa_result.get('significant_strategies', [])
+            if len(significant_strategies) > 1:
+                recommendations.append(
+                    f"ðŸŽ¯ {len(significant_strategies)} strategies show significant SPA: "
+                    f"Consider ensemble or diversified approach"
+                )
+            elif len(significant_strategies) == 1:
+                recommendations.append(
+                    "ðŸŽ¯ Single strategy validation: Focus resources on validated edge"
+                )
+
+        # Data quality recommendations
+        if reality_result.num_bootstrap_samples < 1000:
+            recommendations.append(
+                "ðŸ“Š Increase bootstrap samples to 1000+ for more reliable p-values"
+            )
+
+        return recommendations
\ No newline at end of file
Index: backend/validation/reality_check.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/reality_check.py b/backend/validation/reality_check.py
new file mode 100644
--- /dev/null	(date 1758169876676)
+++ b/backend/validation/reality_check.py	(date 1758169876676)
@@ -0,0 +1,268 @@
+"""
+White Reality Check / SPA Test for Multiple Hypothesis Control
+Prevents false discoveries from testing many strategies simultaneously.
+"""
+
+import numpy as np
+from numpy.random import default_rng
+from typing import Dict, List, Tuple, Any
+import pandas as pd
+from dataclasses import dataclass
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class SPAResult:
+    """Results from SPA test."""
+    strategy_name: str
+    p_value: float
+    test_statistic: float
+    is_significant: bool
+    bootstrap_samples: int
+
+
+class RealityCheckValidator:
+    """
+    Implements Hansen's Superior Predictive Ability (SPA) test to control
+    for multiple hypothesis testing when evaluating many strategies.
+    """
+    
+    def __init__(self, bootstrap_samples: int = 2000, significance_level: float = 0.05):
+        self.bootstrap_samples = bootstrap_samples
+        self.significance_level = significance_level
+        
+    def stationary_bootstrap(self, series: np.ndarray, p: float = 0.1, 
+                           B: int | None = None, rng: np.random.Generator | None = None) -> np.ndarray:
+        """
+        Stationary bootstrap for dependent data.
+        
+        Args:
+            series: Time series data
+            p: Probability of starting new block
+            B: Number of bootstrap samples
+            rng: Random number generator
+            
+        Returns:
+            Bootstrap samples of shape (B, len(series))
+        """
+        if B is None:
+            B = self.bootstrap_samples
+        if rng is None:
+            rng = default_rng(42)
+            
+        n = len(series)
+        idx = np.arange(n)
+        out = np.empty((B, n), dtype=int)
+        
+        for b in range(B):
+            pos = rng.integers(0, n)
+            for t in range(n):
+                out[b, t] = pos
+                if rng.random() < p:
+                    pos = rng.integers(0, n)
+                else:
+                    pos = (pos + 1) % n
+                    
+        return series[out]
+    
+    def spa_test(self, candidate_returns: Dict[str, np.ndarray], 
+                 benchmark: np.ndarray, B: int | None = None) -> Dict[str, SPAResult]:
+        """
+        Implements Hansen's SPA test (simplified).
+        
+        Args:
+            candidate_returns: Dictionary of strategy returns
+            benchmark: Benchmark returns (e.g., buy-and-hold)
+            B: Number of bootstrap samples
+            
+        Returns:
+            Dictionary of SPA results for each strategy
+        """
+        if B is None:
+            B = self.bootstrap_samples
+            
+        results = {}
+        
+        # Calculate excess returns (strategy - benchmark)
+        X = {k: (v - benchmark) for k, v in candidate_returns.items()}
+        means = {k: np.mean(v) for k, v in X.items()}
+        
+        # Pool all excess returns for bootstrap
+        pool = np.column_stack(list(X.values()))
+        
+        # Stationary bootstrap
+        boot = self.stationary_bootstrap(pool, p=0.1, B=B)
+        
+        # Calculate bootstrap means
+        boot_means = boot.mean(axis=1, keepdims=True)
+        
+        # Calculate p-values for each strategy
+        for i, (strategy_name, excess_returns) in enumerate(X.items()):
+            t0 = means[strategy_name]  # Original test statistic
+            tb = boot[:, i] - boot_means[:, 0]  # Bootstrap test statistics
+            
+            # P-value is fraction of bootstrap statistics >= original
+            p_value = float(np.mean(tb >= t0))
+            
+            results[strategy_name] = SPAResult(
+                strategy_name=strategy_name,
+                p_value=p_value,
+                test_statistic=t0,
+                is_significant=p_value <= self.significance_level,
+                bootstrap_samples=B
+            )
+            
+        return results
+    
+    def benjamini_hochberg_correction(self, p_values: Dict[str, float]) -> Dict[str, bool]:
+        """
+        Apply Benjamini-Hochberg correction for multiple testing.
+        
+        Args:
+            p_values: Dictionary of strategy names to p-values
+            
+        Returns:
+            Dictionary of strategy names to significance flags
+        """
+        strategies = list(p_values.keys())
+        p_vals = list(p_values.values())
+        
+        # Sort by p-value
+        sorted_indices = np.argsort(p_vals)
+        sorted_p = np.array(p_vals)[sorted_indices]
+        
+        # Calculate critical values
+        m = len(p_vals)
+        critical_values = np.arange(1, m + 1) * self.significance_level / m
+        
+        # Find largest k such that p(k) <= critical_value(k)
+        significant_indices = []
+        for i in range(m - 1, -1, -1):
+            if sorted_p[i] <= critical_values[i]:
+                significant_indices = sorted_indices[:i + 1]
+                break
+                
+        # Create results dictionary
+        results = dict.fromkeys(strategies, False)
+        for idx in significant_indices:
+            results[strategies[idx]] = True
+            
+        return results
+    
+    def validate_strategy_universe(self, strategy_returns: Dict[str, pd.Series], 
+                                 benchmark_returns: pd.Series) -> Dict[str, Any]:
+        """
+        Comprehensive validation of strategy universe with multiple testing control.
+        
+        Args:
+            strategy_returns: Dictionary of strategy returns
+            benchmark_returns: Benchmark returns
+            
+        Returns:
+            Validation results with multiple testing corrections
+        """
+        # Align all series
+        aligned_data = pd.concat([benchmark_returns.rename('benchmark')] + 
+                                [sr.rename(k) for k, sr in strategy_returns.items()], 
+                                axis=1).dropna()
+        
+        if len(aligned_data) < 100:
+            logger.warning(f"Insufficient data for validation: {len(aligned_data)} observations")
+            return {'error': 'Insufficient data'}
+        
+        # Convert to numpy arrays
+        benchmark = aligned_data['benchmark'].values
+        candidates = {k: aligned_data[k].values for k in strategy_returns.keys()}
+        
+        # Run SPA test
+        spa_results = self.spa_test(candidates, benchmark)
+        
+        # Extract p-values for BH correction
+        p_values = {k: v.p_value for k, v in spa_results.items()}
+        bh_significant = self.benjamini_hochberg_correction(p_values)
+        
+        # Calculate summary statistics
+        total_strategies = len(strategy_returns)
+        spa_significant = sum(1 for r in spa_results.values() if r.is_significant)
+        bh_significant_count = sum(1 for sig in bh_significant.values() if sig)
+        
+        # Identify best strategies
+        significant_strategies = [k for k, sig in bh_significant.items() if sig]
+        
+        return {
+            'total_strategies_tested': total_strategies,
+            'spa_significant_count': spa_significant,
+            'bh_significant_count': bh_significant_count,
+            'significant_strategies': significant_strategies,
+            'spa_results': spa_results,
+            'bh_correction': bh_significant,
+            'recommendation': 'PROCEED' if bh_significant_count > 0 else 'REJECT_ALL',
+            'false_discovery_rate': spa_significant / max(total_strategies, 1),
+            'validation_passed': bh_significant_count > 0
+        }
+
+
+def run_reality_check_validation(strategy_returns: Dict[str, pd.Series], 
+                                benchmark_returns: pd.Series,
+                                bootstrap_samples: int = 2000) -> Dict[str, Any]:
+    """
+    Convenience function to run reality check validation.
+    
+    Args:
+        strategy_returns: Dictionary of strategy returns
+        benchmark_returns: Benchmark returns
+        bootstrap_samples: Number of bootstrap samples
+        
+    Returns:
+        Validation results
+    """
+    validator = RealityCheckValidator(bootstrap_samples=bootstrap_samples)
+    return validator.validate_strategy_universe(strategy_returns, benchmark_returns)
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    def test_reality_check():
+        """Test the reality check implementation."""
+        print("=== Reality Check Validation Test ===")
+        
+        # Generate test data
+        np.random.seed(42)
+        dates = pd.date_range('2020-01-01', periods=500, freq='D')
+        
+        # Benchmark (random walk)
+        benchmark = pd.Series(np.random.normal(0.0005, 0.02, len(dates)), index=dates)
+        
+        # Generate strategies with varying alpha
+        strategies = {}
+        
+        # Strategy 1: True alpha
+        strategies['true_alpha'] = benchmark + np.random.normal(0.001, 0.015, len(dates))
+        
+        # Strategy 2: No alpha (just noise)
+        strategies['no_alpha'] = benchmark + np.random.normal(0, 0.02, len(dates))
+        
+        # Strategy 3: Negative alpha
+        strategies['negative_alpha'] = benchmark + np.random.normal(-0.0005, 0.02, len(dates))
+        
+        # Strategy 4: Another true alpha
+        strategies['another_alpha'] = benchmark + np.random.normal(0.0008, 0.018, len(dates))
+        
+        # Run validation
+        validator = RealityCheckValidator()
+        results = validator.validate_strategy_universe(strategies, benchmark)
+        
+        print(f"Total strategies tested: {results['total_strategies_tested']}")
+        print(f"SPA significant: {results['spa_significant_count']}")
+        print(f"BH significant: {results['bh_significant_count']}")
+        print(f"Significant strategies: {results['significant_strategies']}")
+        print(f"Recommendation: {results['recommendation']}")
+        
+        # Show individual results
+        print("\nIndividual SPA Results:")
+        for strategy, result in results['spa_results'].items():
+            print(f"  {strategy}: p-value={result.p_value:.4f}, significant={result.is_significant}")
+    
+    test_reality_check()
Index: backend/validation/regime_testing.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/regime_testing.py b/backend/validation/regime_testing.py
new file mode 100644
--- /dev/null	(date 1758168088178)
+++ b/backend/validation/regime_testing.py	(date 1758168088178)
@@ -0,0 +1,385 @@
+"""
+Regime Testing with Proper Alignment and Drawdown Calculation
+Tests strategy performance across different market regimes.
+"""
+
+from __future__ import annotations
+import pandas as pd
+import numpy as np
+from typing import Dict, Callable, List, Optional, Any
+from dataclasses import dataclass
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class RegimeResult:
+    """Results from regime testing."""
+    regime_name: str
+    sharpe_ratio: float
+    win_rate: float
+    avg_return: float
+    max_drawdown: float
+    sample_size: int
+    start_date: pd.Timestamp
+    end_date: pd.Timestamp
+    regime_periods: int
+
+
+def _max_drawdown(returns: pd.Series) -> float:
+    """Calculate maximum drawdown from returns."""
+    if returns.empty:
+        return 0.0
+    
+    equity = (1 + returns).cumprod()
+    peak = equity.cummax()
+    dd = (equity / peak) - 1.0
+    return float(dd.min())
+
+
+def _sharpe(returns: pd.Series, rf_daily: float = 0.0) -> float:
+    """Calculate Sharpe ratio."""
+    ex = returns - rf_daily
+    if ex.std(ddof=1) == 0 or len(ex) < 2:
+        return 0.0
+    return float(np.sqrt(252) * ex.mean() / ex.std(ddof=1))
+
+
+class RegimeValidator:
+    """
+    Tests strategy performance across different market regimes.
+    
+    market_data columns used: ['SPY','VIX','DGS10'] daily close or levels as appropriate.
+    strategy_returns: daily returns aligned to market_data.index
+    """
+    
+    def __init__(self, rf_daily: float = 0.0):
+        """
+        Initialize regime validator.
+        
+        Args:
+            rf_daily: Daily risk-free rate
+        """
+        self.rf_daily = rf_daily
+        self.regimes: Dict[str, Callable[[pd.DataFrame], pd.Series]] = {
+            'bull_market': lambda d: (d['SPY'].pct_change(20).rolling(60).mean() > 0.02),
+            'bear_market': lambda d: (d['SPY'].pct_change(20).rolling(60).mean() < -0.02),
+            'high_vol': lambda d: (d['VIX'] > 20),
+            'low_vol': lambda d: (d['VIX'] < 15),
+            'rate_hiking': lambda d: (d['DGS10'].diff(60) > 0.5),
+            'rate_cutting': lambda d: (d['DGS10'].diff(60) < -0.5),
+            'trending_up': lambda d: (d['SPY'].pct_change(5).rolling(20).mean() > 0.001),
+            'trending_down': lambda d: (d['SPY'].pct_change(5).rolling(20).mean() < -0.001),
+            'sideways': lambda d: (abs(d['SPY'].pct_change(5).rolling(20).mean()) < 0.001),
+        }
+
+    def test_edge_persistence(self, strategy_returns: pd.Series, 
+                            market_data: pd.DataFrame, 
+                            min_n: int = 30) -> Dict[str, Any]:
+        """
+        Test strategy performance across different market regimes.
+        
+        Args:
+            strategy_returns: Strategy daily returns
+            market_data: Market data with regime indicators
+            min_n: Minimum observations per regime
+            
+        Returns:
+            Dictionary with regime test results
+        """
+        # Align data
+        aligned = pd.concat([
+            strategy_returns.rename('ret'), 
+            market_data
+        ], axis=1).dropna()
+        
+        if len(aligned) < min_n * 2:
+            logger.warning(f"Insufficient data for regime testing: {len(aligned)} observations")
+            return {
+                'regime_results': {}, 
+                'edge_is_robust': False, 
+                'weakest_regime': None, 
+                'strongest_regime': None,
+                'error': 'Insufficient data'
+            }
+        
+        results = {}
+        regime_periods = {}
+        
+        for name, regime_func in self.regimes.items():
+            try:
+                # Calculate regime mask
+                mask = regime_func(aligned).fillna(False)
+                
+                if not mask.any():
+                    logger.debug(f"No observations for regime: {name}")
+                    continue
+                
+                # Get strategy returns for this regime
+                regime_returns = aligned.loc[mask, 'ret']
+                
+                if len(regime_returns) < min_n:
+                    logger.debug(f"Insufficient observations for regime {name}: {len(regime_returns)}")
+                    continue
+                
+                # Calculate regime statistics
+                sharpe_ratio = _sharpe(regime_returns, self.rf_daily)
+                win_rate = float((regime_returns > 0).mean())
+                avg_return = float(regime_returns.mean())
+                max_dd = _max_drawdown(regime_returns)
+                
+                # Get regime period information
+                regime_dates = aligned.loc[mask].index
+                start_date = regime_dates.min()
+                end_date = regime_dates.max()
+                
+                # Count regime periods (consecutive days)
+                regime_periods_count = self._count_regime_periods(mask)
+                
+                results[name] = RegimeResult(
+                    regime_name=name,
+                    sharpe_ratio=sharpe_ratio,
+                    win_rate=win_rate,
+                    avg_return=avg_return,
+                    max_drawdown=max_dd,
+                    sample_size=len(regime_returns),
+                    start_date=start_date,
+                    end_date=end_date,
+                    regime_periods=regime_periods_count
+                )
+                
+            except Exception as e:
+                logger.warning(f"Failed to analyze regime {name}: {e}")
+                continue
+        
+        if not results:
+            return {
+                'regime_results': {}, 
+                'edge_is_robust': False, 
+                'weakest_regime': None, 
+                'strongest_regime': None,
+                'error': 'No valid regimes found'
+            }
+        
+        # Determine robustness
+        sharpe_ratios = [r.sharpe_ratio for r in results.values()]
+        edge_is_robust = all(sr > 0.5 for sr in sharpe_ratios) and len(sharpe_ratios) >= 3
+        
+        # Find weakest and strongest regimes
+        weakest = min(results.keys(), key=lambda k: results[k].sharpe_ratio)
+        strongest = max(results.keys(), key=lambda k: results[k].sharpe_ratio)
+        
+        return {
+            'regime_results': results,
+            'edge_is_robust': edge_is_robust,
+            'weakest_regime': weakest,
+            'strongest_regime': strongest,
+            'total_regimes_tested': len(results),
+            'robust_regimes_count': sum(1 for r in results.values() if r.sharpe_ratio > 0.5)
+        }
+    
+    def _count_regime_periods(self, mask: pd.Series) -> int:
+        """Count number of distinct regime periods."""
+        if not mask.any():
+            return 0
+        
+        # Find changes in regime
+        changes = mask.astype(int).diff().fillna(0)
+        regime_starts = (changes == 1).sum()
+        
+        # If mask starts with True, add 1
+        if mask.iloc[0]:
+            regime_starts += 1
+            
+        return int(regime_starts)
+    
+    def add_custom_regime(self, name: str, regime_func: Callable[[pd.DataFrame], pd.Series]):
+        """Add a custom regime definition."""
+        self.regimes[name] = regime_func
+        logger.info(f"Added custom regime: {name}")
+    
+    def get_regime_summary(self, results: Dict[str, RegimeResult]) -> Dict[str, Any]:
+        """Get summary statistics across regimes."""
+        if not results:
+            return {'error': 'No regime results'}
+        
+        sharpe_ratios = [r.sharpe_ratio for r in results.values()]
+        win_rates = [r.win_rate for r in results.values()]
+        max_drawdowns = [r.max_drawdown for r in results.values()]
+        
+        return {
+            'regime_count': len(results),
+            'avg_sharpe': float(np.mean(sharpe_ratios)),
+            'min_sharpe': float(np.min(sharpe_ratios)),
+            'max_sharpe': float(np.max(sharpe_ratios)),
+            'sharpe_std': float(np.std(sharpe_ratios)),
+            'avg_win_rate': float(np.mean(win_rates)),
+            'avg_max_drawdown': float(np.mean(max_drawdowns)),
+            'worst_drawdown': float(np.min(max_drawdowns)),
+            'regime_consistency': float(np.std(sharpe_ratios) / (np.mean(sharpe_ratios) + 1e-9))
+        }
+
+
+class RegimeRobustnessTester:
+    """
+    Advanced regime robustness testing with rolling windows and stress tests.
+    """
+    
+    def __init__(self, regime_validator: RegimeValidator):
+        self.regime_validator = regime_validator
+    
+    def rolling_regime_test(self, strategy_returns: pd.Series, 
+                           market_data: pd.DataFrame,
+                           window_size: int = 252,
+                           step_size: int = 21) -> Dict[str, Any]:
+        """
+        Test regime robustness using rolling windows.
+        
+        Args:
+            strategy_returns: Strategy returns
+            market_data: Market data
+            window_size: Rolling window size in days
+            step_size: Step size between windows
+            
+        Returns:
+            Rolling regime test results
+        """
+        results = []
+        start_dates = []
+        
+        for start_idx in range(0, len(strategy_returns) - window_size + 1, step_size):
+            end_idx = start_idx + window_size
+            
+            window_returns = strategy_returns.iloc[start_idx:end_idx]
+            window_market = market_data.iloc[start_idx:end_idx]
+            
+            try:
+                regime_results = self.regime_validator.test_edge_persistence(
+                    window_returns, window_market
+                )
+                
+                if regime_results['regime_results']:
+                    results.append(regime_results)
+                    start_dates.append(window_returns.index[0])
+                    
+            except Exception as e:
+                logger.warning(f"Rolling window test failed at {start_idx}: {e}")
+                continue
+        
+        if not results:
+            return {'error': 'No valid rolling windows'}
+        
+        # Analyze consistency across windows
+        robustness_scores = [r['edge_is_robust'] for r in results]
+        consistency_rate = sum(robustness_scores) / len(robustness_scores)
+        
+        return {
+            'total_windows': len(results),
+            'robust_windows': sum(robustness_scores),
+            'consistency_rate': consistency_rate,
+            'window_results': results,
+            'start_dates': start_dates,
+            'overall_robust': consistency_rate >= 0.7
+        }
+    
+    def stress_test_regimes(self, strategy_returns: pd.Series, 
+                          market_data: pd.DataFrame) -> Dict[str, Any]:
+        """
+        Stress test strategy in extreme market conditions.
+        
+        Args:
+            strategy_returns: Strategy returns
+            market_data: Market data
+            
+        Returns:
+            Stress test results
+        """
+        aligned = pd.concat([
+            strategy_returns.rename('ret'), 
+            market_data
+        ], axis=1).dropna()
+        
+        stress_tests = {
+            'market_crash': aligned['SPY'].pct_change(5) < -0.05,
+            'volatility_spike': aligned['VIX'] > 30,
+            'rate_shock': abs(aligned['DGS10'].diff(5)) > 0.3,
+            'trend_reversal': (
+                (aligned['SPY'].pct_change(5) > 0.02) & 
+                (aligned['SPY'].pct_change(5).shift(5) < -0.02)
+            )
+        }
+        
+        stress_results = {}
+        for test_name, stress_mask in stress_tests.items():
+            if not stress_mask.any():
+                continue
+                
+            stress_returns = aligned.loc[stress_mask, 'ret']
+            if len(stress_returns) < 5:
+                continue
+            
+            stress_results[test_name] = {
+                'sample_size': len(stress_returns),
+                'avg_return': float(stress_returns.mean()),
+                'sharpe_ratio': _sharpe(stress_returns),
+                'max_drawdown': _max_drawdown(stress_returns),
+                'survival_rate': float((stress_returns > -0.05).mean())
+            }
+        
+        return {
+            'stress_tests': stress_results,
+            'total_stress_periods': sum(len(r) for r in stress_tests.values()),
+            'strategy_survives_stress': all(
+                r['survival_rate'] > 0.7 for r in stress_results.values()
+            )
+        }
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    def test_regime_validation():
+        """Test regime validation implementation."""
+        print("=== Regime Validation Test ===")
+        
+        # Generate test data
+        np.random.seed(42)
+        dates = pd.date_range('2020-01-01', periods=1000, freq='D')
+        
+        # Generate market data
+        spy_returns = np.random.normal(0.0005, 0.02, len(dates))
+        spy_prices = (1 + pd.Series(spy_returns, index=dates)).cumprod()
+        
+        market_data = pd.DataFrame({
+            'SPY': spy_prices,
+            'VIX': np.random.uniform(10, 40, len(dates)),
+            'DGS10': np.random.uniform(1.5, 4.5, len(dates))
+        }, index=dates)
+        
+        # Generate strategy returns (performs better in low vol)
+        strategy_returns = pd.Series(
+            np.random.normal(0.001, 0.015, len(dates)) + 
+            np.where(market_data['VIX'] < 20, 0.0005, -0.0005),
+            index=dates
+        )
+        
+        # Test regime validation
+        validator = RegimeValidator()
+        results = validator.test_edge_persistence(strategy_returns, market_data)
+        
+        print(f"Edge is robust: {results['edge_is_robust']}")
+        print(f"Total regimes tested: {results['total_regimes_tested']}")
+        print(f"Robust regimes: {results['robust_regimes_count']}")
+        
+        if results['regime_results']:
+            print(f"Weakest regime: {results['weakest_regime']}")
+            print(f"Strongest regime: {results['strongest_regime']}")
+            
+            # Show individual regime results
+            print("\nRegime Results:")
+            for name, regime_result in results['regime_results'].items():
+                print(f"  {name}: Sharpe={regime_result.sharpe_ratio:.2f}, "
+                      f"Win Rate={regime_result.win_rate:.2f}, "
+                      f"Max DD={regime_result.max_drawdown:.2f}")
+    
+    test_regime_validation()
\ No newline at end of file
Index: backend/validation/advanced_risk/risk_of_ruin.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/advanced_risk/risk_of_ruin.py b/backend/validation/advanced_risk/risk_of_ruin.py
new file mode 100644
--- /dev/null	(date 1758169876676)
+++ b/backend/validation/advanced_risk/risk_of_ruin.py	(date 1758169876676)
@@ -0,0 +1,536 @@
+"""Risk of ruin calculations and capital allocation optimization.
+
+Provides analytical upper bounds for risk of ruin and optimizes
+position sizing to maintain acceptable ruin probabilities.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Tuple, Any
+import logging
+from datetime import datetime, timedelta
+from dataclasses import dataclass
+from scipy import stats, optimize
+import warnings
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class RiskOfRuinResult:
+    """Results from risk of ruin calculation."""
+    probability_of_ruin: float
+    analytical_upper_bound: float
+    monte_carlo_estimate: float
+    time_to_ruin_median: Optional[float]
+    safe_position_size: float
+    current_position_size: float
+    recommendation: str
+    confidence_interval: Tuple[float, float]
+
+
+@dataclass
+class TailRiskMetrics:
+    """Tail risk analysis results."""
+    var_95: float
+    var_99: float
+    var_999: float
+    cvar_95: float
+    cvar_99: float
+    expected_shortfall: float
+    tail_ratio: float
+    max_drawdown_estimate: float
+
+
+class RiskOfRuinCalculator:
+    """Calculate risk of ruin using multiple methods."""
+
+    def __init__(self, initial_capital: float):
+        self.initial_capital = initial_capital
+        self.target_ruin_probability = 0.02  # 2% maximum acceptable ruin risk
+
+    def calculate_risk_of_ruin(self, returns: pd.Series,
+                             position_size: float,
+                             confidence_level: float = 0.95) -> RiskOfRuinResult:
+        """Calculate comprehensive risk of ruin analysis.
+
+        Args:
+            returns: Historical strategy returns
+            position_size: Current position size (fraction of capital)
+            confidence_level: Confidence level for estimates
+
+        Returns:
+            RiskOfRuinResult with multiple ruin probability estimates
+        """
+        if len(returns) < 30:
+            raise ValueError("Need at least 30 return observations")
+
+        # Calculate key statistics
+        win_rate = (returns > 0).mean()
+        avg_win = returns[returns > 0].mean() if (returns > 0).any() else 0
+        avg_loss = returns[returns <= 0].mean() if (returns <= 0).any() else 0
+
+        # Analytical approximation (Gambler's Ruin formula adaptation)
+        analytical_ruin = self._analytical_risk_of_ruin(
+            win_rate, avg_win, avg_loss, position_size
+        )
+
+        # Monte Carlo simulation
+        mc_ruin, mc_confidence = self._monte_carlo_risk_of_ruin(
+            returns, position_size, confidence_level
+        )
+
+        # Time to ruin analysis
+        median_time_to_ruin = self._estimate_time_to_ruin(
+            returns, position_size
+        )
+
+        # Calculate safe position size
+        safe_size = self._calculate_safe_position_size(
+            returns, self.target_ruin_probability
+        )
+
+        # Generate recommendation
+        recommendation = self._generate_recommendation(
+            analytical_ruin, mc_ruin, position_size, safe_size
+        )
+
+        return RiskOfRuinResult(
+            probability_of_ruin=mc_ruin,
+            analytical_upper_bound=analytical_ruin,
+            monte_carlo_estimate=mc_ruin,
+            time_to_ruin_median=median_time_to_ruin,
+            safe_position_size=safe_size,
+            current_position_size=position_size,
+            recommendation=recommendation,
+            confidence_interval=mc_confidence
+        )
+
+    def _analytical_risk_of_ruin(self, win_rate: float, avg_win: float,
+                               avg_loss: float, position_size: float) -> float:
+        """Calculate analytical risk of ruin approximation."""
+        if avg_loss == 0:
+            return 0.0  # No losses = no ruin
+
+        # Calculate probability parameters
+        p = win_rate  # Probability of win
+        q = 1 - p     # Probability of loss
+
+        # Average gain/loss per bet as fraction of capital
+        a = avg_win * position_size    # Average gain when winning
+        b = abs(avg_loss) * position_size  # Average loss when losing
+
+        if a <= 0 or b <= 0:
+            return 1.0  # Invalid parameters lead to certain ruin
+
+        # Expected value per bet
+        expected_return = p * a - q * b
+
+        if expected_return <= 0:
+            return 1.0  # Negative expectancy leads to certain ruin
+
+        # Simplified gambler's ruin adaptation
+        # This assumes discrete bets and uses the classic formula
+        # R = (q*b / p*a)^(capital / unit_bet_size)
+
+        # Risk ratio
+        risk_ratio = (q * b) / (p * a)
+
+        if risk_ratio >= 1:
+            return 1.0  # Unfavorable game
+
+        # Number of bets before ruin (simplified)
+        # Assumes each bet risks fixed fraction of remaining capital
+        ruin_threshold = 0.1  # 10% of initial capital remaining = "ruin"
+
+        # Use logarithmic approximation for continuous betting
+        # P(ruin) â‰ˆ exp(-2Î¼N/oÂ²) where Î¼ is drift, o is volatility, N is number of periods
+
+        if risk_ratio > 0:
+            # Approximate using exponential decay
+            ruin_prob = min(1.0, risk_ratio ** (1.0 / position_size))
+        else:
+            ruin_prob = 0.0
+
+        return min(1.0, max(0.0, ruin_prob))
+
+    def _monte_carlo_risk_of_ruin(self, returns: pd.Series, position_size: float,
+                                confidence_level: float, num_simulations: int = 10000) -> Tuple[float, Tuple[float, float]]:
+        """Monte Carlo simulation of risk of ruin."""
+        np.random.seed(42)  # For reproducible results
+
+        # Parameters for return distribution
+        mean_return = returns.mean()
+        std_return = returns.std()
+
+        # Number of periods to simulate (1 year)
+        periods = 252
+
+        # Track ruin events
+        ruin_events = 0
+        time_to_ruin = []
+
+        for _ in range(num_simulations):
+            capital = 1.0  # Start with normalized capital
+
+            for period in range(periods):
+                # Generate return for this period
+                period_return = np.random.normal(mean_return, std_return)
+
+                # Apply position sizing
+                portfolio_return = period_return * position_size
+
+                # Update capital
+                capital *= (1 + portfolio_return)
+
+                # Check for ruin (capital below 10% of initial)
+                if capital <= 0.1:
+                    ruin_events += 1
+                    time_to_ruin.append(period)
+                    break
+
+        # Calculate ruin probability
+        ruin_probability = ruin_events / num_simulations
+
+        # Calculate confidence interval using Wilson score interval
+        if ruin_events > 0:
+            z = stats.norm.ppf((1 + confidence_level) / 2)
+            n = num_simulations
+            p_hat = ruin_probability
+
+            denominator = 1 + z**2 / n
+            center = (p_hat + z**2 / (2*n)) / denominator
+            margin = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4*n)) / n) / denominator
+
+            ci_lower = max(0, center - margin)
+            ci_upper = min(1, center + margin)
+        else:
+            ci_lower = 0
+            ci_upper = 3 / num_simulations  # Rule of three for zero events
+
+        return ruin_probability, (ci_lower, ci_upper)
+
+    def _estimate_time_to_ruin(self, returns: pd.Series, position_size: float) -> Optional[float]:
+        """Estimate median time to ruin if ruin occurs."""
+        # Simple approximation based on negative drift
+        mean_return = returns.mean()
+        std_return = returns.std()
+
+        if mean_return >= 0:
+            return None  # Positive expectancy = no expected ruin
+
+        # Portfolio level statistics
+        portfolio_mean = mean_return * position_size
+        portfolio_std = std_return * position_size
+
+        # Time to hit 10% of capital (ln(0.1) = -2.3)
+        # Using first passage time approximation
+        barrier = np.log(0.1)  # Log of ruin level
+
+        if portfolio_mean < 0:
+            # Expected time to hit barrier (simplified)
+            expected_time = -barrier / portfolio_mean
+            return max(1, expected_time)  # At least 1 period
+
+        return None
+
+    def _calculate_safe_position_size(self, returns: pd.Series,
+                                    target_ruin_prob: float) -> float:
+        """Calculate maximum safe position size for target ruin probability."""
+
+        def ruin_objective(position_size):
+            """Objective function: minimize difference from target ruin probability."""
+            if position_size <= 0 or position_size > 1:
+                return 1.0  # Penalty for invalid sizes
+
+            try:
+                _, mc_confidence = self._monte_carlo_risk_of_ruin(
+                    returns, position_size, 0.95, num_simulations=1000  # Faster for optimization
+                )
+                estimated_ruin = mc_confidence[1]  # Use upper bound of confidence interval
+                return abs(estimated_ruin - target_ruin_prob)
+            except Exception:
+                return 1.0
+
+        # Binary search for optimal position size
+        try:
+            result = optimize.minimize_scalar(
+                ruin_objective,
+                bounds=(0.01, 0.5),  # Search between 1% and 50% position size
+                method='bounded'
+            )
+
+            optimal_size = result.x if result.success else 0.1
+
+            # Verify the result makes sense
+            if optimal_size > 0.5:
+                optimal_size = 0.1  # Conservative fallback
+
+            return optimal_size
+
+        except Exception as e:
+            logger.warning(f"Safe position size calculation failed: {e}")
+            return 0.1  # Conservative default
+
+    def _generate_recommendation(self, analytical_ruin: float, mc_ruin: float,
+                               current_size: float, safe_size: float) -> str:
+        """Generate risk management recommendation."""
+        max_ruin = max(analytical_ruin, mc_ruin)
+
+        if max_ruin > 0.05:  # 5% threshold
+            return f"HIGH RISK: Reduce position size from {current_size:.1%} to {safe_size:.1%}"
+        elif max_ruin > 0.02:  # 2% threshold
+            return f"MODERATE RISK: Consider reducing position size to {safe_size:.1%}"
+        elif current_size < safe_size * 0.8:  # Significantly under-leveraged
+            return f"CONSERVATIVE: Could increase position size to {safe_size:.1%}"
+        else:
+            return "ACCEPTABLE RISK: Current position size is appropriate"
+
+
+class TailRiskAnalyzer:
+    """Analyzes tail risk and extreme loss scenarios."""
+
+    def __init__(self):
+        self.confidence_levels = [0.95, 0.99, 0.999]
+
+    def analyze_tail_risk(self, returns: pd.Series, position_size: float = 1.0) -> TailRiskMetrics:
+        """Comprehensive tail risk analysis.
+
+        Args:
+            returns: Strategy returns
+            position_size: Position sizing factor
+
+        Returns:
+            TailRiskMetrics with VaR, CVaR, and other tail measures
+        """
+        if len(returns) < 30:
+            raise ValueError("Need at least 30 return observations")
+
+        # Scale returns by position size
+        scaled_returns = returns * position_size
+
+        # Calculate VaR at different confidence levels
+        var_95 = self._calculate_var(scaled_returns, 0.95)
+        var_99 = self._calculate_var(scaled_returns, 0.99)
+        var_999 = self._calculate_var(scaled_returns, 0.999)
+
+        # Calculate Conditional VaR (Expected Shortfall)
+        cvar_95 = self._calculate_cvar(scaled_returns, 0.95)
+        cvar_99 = self._calculate_cvar(scaled_returns, 0.99)
+
+        # Expected shortfall (average of worst 5% outcomes)
+        expected_shortfall = scaled_returns.quantile(0.05)
+
+        # Tail ratio (90th percentile / 10th percentile)
+        tail_ratio = scaled_returns.quantile(0.9) / abs(scaled_returns.quantile(0.1))
+
+        # Maximum drawdown estimate
+        max_dd_estimate = self._estimate_max_drawdown(scaled_returns)
+
+        return TailRiskMetrics(
+            var_95=var_95,
+            var_99=var_99,
+            var_999=var_999,
+            cvar_95=cvar_95,
+            cvar_99=cvar_99,
+            expected_shortfall=expected_shortfall,
+            tail_ratio=tail_ratio,
+            max_drawdown_estimate=max_dd_estimate
+        )
+
+    def _calculate_var(self, returns: pd.Series, confidence_level: float) -> float:
+        """Calculate Value at Risk."""
+        return -returns.quantile(1 - confidence_level)
+
+    def _calculate_cvar(self, returns: pd.Series, confidence_level: float) -> float:
+        """Calculate Conditional Value at Risk (Expected Shortfall)."""
+        var_threshold = returns.quantile(1 - confidence_level)
+        tail_returns = returns[returns <= var_threshold]
+
+        if len(tail_returns) == 0:
+            return self._calculate_var(returns, confidence_level)
+
+        return -tail_returns.mean()
+
+    def _estimate_max_drawdown(self, returns: pd.Series) -> float:
+        """Estimate maximum drawdown using Monte Carlo."""
+        np.random.seed(42)
+
+        # Parameters for return distribution
+        mean_return = returns.mean()
+        std_return = returns.std()
+
+        max_drawdowns = []
+
+        # Run multiple simulations
+        for _ in range(1000):
+            # Generate return path
+            simulated_returns = np.random.normal(mean_return, std_return, len(returns))
+
+            # Calculate cumulative returns
+            cumulative = (1 + pd.Series(simulated_returns)).cumprod()
+
+            # Calculate drawdowns
+            running_max = cumulative.expanding().max()
+            drawdowns = (cumulative - running_max) / running_max
+
+            max_drawdowns.append(drawdowns.min())
+
+        # Return 95th percentile of maximum drawdowns (conservative estimate)
+        return -np.percentile(max_drawdowns, 95)
+
+
+class DynamicRiskManager:
+    """Dynamic risk management with real-time adjustments."""
+
+    def __init__(self, initial_capital: float):
+        self.initial_capital = initial_capital
+        self.current_capital = initial_capital
+        self.risk_calculator = RiskOfRuinCalculator(initial_capital)
+        self.tail_analyzer = TailRiskAnalyzer()
+
+        # Risk limits
+        self.risk_limits = {
+            'max_ruin_probability': 0.02,  # 2%
+            'max_var_95': 0.05,           # 5% daily VaR
+            'max_drawdown': 0.20,         # 20% max drawdown
+            'min_sharpe_ratio': 1.0       # Minimum Sharpe ratio
+        }
+
+        # Position sizing parameters
+        self.base_position_size = 0.1     # 10% base allocation
+        self.max_position_size = 0.3      # 30% maximum allocation
+        self.min_position_size = 0.01     # 1% minimum allocation
+
+    def calculate_optimal_position_size(self, strategy_returns: pd.Series,
+                                      current_market_regime: str = 'normal') -> Dict[str, Any]:
+        """Calculate optimal position size considering multiple risk factors.
+
+        Args:
+            strategy_returns: Historical strategy returns
+            current_market_regime: Current market regime ('bull', 'bear', 'normal', 'volatile')
+
+        Returns:
+            Dictionary with position sizing recommendation and analysis
+        """
+
+        # Start with base position size
+        recommended_size = self.base_position_size
+
+        # Risk of ruin analysis
+        ruin_analysis = self.risk_calculator.calculate_risk_of_ruin(
+            strategy_returns, recommended_size
+        )
+
+        # Tail risk analysis
+        tail_analysis = self.tail_analyzer.analyze_tail_risk(
+            strategy_returns, recommended_size
+        )
+
+        # Adjust for risk of ruin
+        if ruin_analysis.probability_of_ruin > self.risk_limits['max_ruin_probability']:
+            recommended_size = ruin_analysis.safe_position_size
+
+        # Adjust for VaR limits
+        if tail_analysis.var_95 > self.risk_limits['max_var_95']:
+            var_adjustment = self.risk_limits['max_var_95'] / tail_analysis.var_95
+            recommended_size *= var_adjustment
+
+        # Adjust for market regime
+        regime_multiplier = self._get_regime_multiplier(current_market_regime)
+        recommended_size *= regime_multiplier
+
+        # Apply bounds
+        recommended_size = max(self.min_position_size,
+                             min(self.max_position_size, recommended_size))
+
+        # Calculate expected metrics with recommended size
+        final_ruin = self.risk_calculator.calculate_risk_of_ruin(
+            strategy_returns, recommended_size
+        )
+
+        final_tail = self.tail_analyzer.analyze_tail_risk(
+            strategy_returns, recommended_size
+        )
+
+        return {
+            'recommended_position_size': recommended_size,
+            'base_size': self.base_position_size,
+            'regime_adjustment': regime_multiplier,
+            'risk_of_ruin': final_ruin.probability_of_ruin,
+            'var_95_percent': final_tail.var_95 * 100,
+            'expected_sharpe': self._estimate_sharpe(strategy_returns, recommended_size),
+            'max_drawdown_estimate': final_tail.max_drawdown_estimate,
+            'recommendation_reason': self._explain_sizing_decision(
+                ruin_analysis, tail_analysis, regime_multiplier
+            ),
+            'risk_metrics': {
+                'ruin_probability': final_ruin.probability_of_ruin,
+                'var_95': final_tail.var_95,
+                'cvar_95': final_tail.cvar_95,
+                'tail_ratio': final_tail.tail_ratio
+            }
+        }
+
+    def _get_regime_multiplier(self, regime: str) -> float:
+        """Get position size multiplier based on market regime."""
+        multipliers = {
+            'bull': 1.2,       # Increase size in bull markets
+            'normal': 1.0,     # Normal size
+            'bear': 0.7,       # Reduce size in bear markets
+            'volatile': 0.6,   # Significantly reduce in volatile markets
+            'crisis': 0.3      # Minimal size in crisis
+        }
+
+        return multipliers.get(regime, 1.0)
+
+    def _estimate_sharpe(self, returns: pd.Series, position_size: float) -> float:
+        """Estimate Sharpe ratio with given position size."""
+        scaled_returns = returns * position_size
+
+        if scaled_returns.std() == 0:
+            return 0.0
+
+        return scaled_returns.mean() / scaled_returns.std() * np.sqrt(252)
+
+    def _explain_sizing_decision(self, ruin_analysis: RiskOfRuinResult,
+                               tail_analysis: TailRiskMetrics,
+                               regime_multiplier: float) -> str:
+        """Explain the position sizing decision."""
+        reasons = []
+
+        if ruin_analysis.probability_of_ruin > 0.01:
+            reasons.append(f"Risk of ruin ({ruin_analysis.probability_of_ruin:.1%}) considered")
+
+        if tail_analysis.var_95 > 0.03:
+            reasons.append(f"High VaR ({tail_analysis.var_95:.1%}) factored in")
+
+        if regime_multiplier != 1.0:
+            if regime_multiplier > 1.0:
+                reasons.append("Increased for favorable market regime")
+            else:
+                reasons.append("Reduced for unfavorable market regime")
+
+        if not reasons:
+            return "Standard position sizing based on risk limits"
+
+        return "; ".join(reasons)
+
+    def update_capital(self, new_capital: float):
+        """Update current capital level."""
+        self.current_capital = new_capital
+        self.risk_calculator = RiskOfRuinCalculator(new_capital)
+
+    def get_risk_dashboard(self) -> Dict[str, Any]:
+        """Get current risk dashboard metrics."""
+        return {
+            'current_capital': self.current_capital,
+            'capital_change_pct': (self.current_capital - self.initial_capital) / self.initial_capital,
+            'risk_limits': self.risk_limits,
+            'position_size_range': {
+                'min': self.min_position_size,
+                'max': self.max_position_size,
+                'base': self.base_position_size
+            },
+            'last_updated': datetime.now().isoformat()
+        }
\ No newline at end of file
Index: backend/validation/operations/secrets_iam_hardening.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/operations/secrets_iam_hardening.py b/backend/validation/operations/secrets_iam_hardening.py
new file mode 100644
--- /dev/null	(date 1758169876676)
+++ b/backend/validation/operations/secrets_iam_hardening.py	(date 1758169876676)
@@ -0,0 +1,875 @@
+"""
+Secrets Management & IAM Hardening for Trading Systems
+Implements production-grade secrets management and identity/access controls.
+"""
+
+import os
+import hashlib
+import hmac
+import base64
+import json
+import logging
+from datetime import datetime, timedelta
+from typing import Dict, List, Optional, Any, Tuple, Set
+from dataclasses import dataclass, field
+from enum import Enum
+import asyncio
+from pathlib import Path
+import boto3
+from cryptography.fernet import Fernet
+from cryptography.hazmat.primitives import hashes
+from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
+import jwt
+
+
+class SecretType(Enum):
+    API_KEY = "api_key"
+    DATABASE_PASSWORD = "database_password"  # noqa: S105
+    BROKER_CREDENTIALS = "broker_credentials"
+    ENCRYPTION_KEY = "encryption_key"
+    WEBHOOK_SECRET = "webhook_secret"  # noqa: S105
+    CERTIFICATE = "certificate"
+    PRIVATE_KEY = "private_key"
+
+
+class AccessLevel(Enum):
+    READ_ONLY = "read_only"
+    READ_WRITE = "read_write"
+    ADMIN = "admin"
+    SERVICE = "service"
+
+
+@dataclass
+class Secret:
+    """Represents a managed secret."""
+    name: str
+    secret_type: SecretType
+    value: str
+    description: str = ""
+    created_at: datetime = field(default_factory=datetime.now)
+    expires_at: Optional[datetime] = None
+    last_accessed: Optional[datetime] = None
+    access_count: int = 0
+    tags: Dict[str, str] = field(default_factory=dict)
+    rotation_required: bool = False
+
+
+@dataclass
+class AccessPolicy:
+    """Defines access control policy."""
+    name: str
+    description: str
+    allowed_secrets: Set[str] = field(default_factory=set)
+    allowed_operations: Set[str] = field(default_factory=set)
+    conditions: Dict[str, Any] = field(default_factory=dict)
+    expires_at: Optional[datetime] = None
+
+
+@dataclass
+class Principal:
+    """Represents an identity (user, service, etc.)."""
+    id: str
+    name: str
+    principal_type: str  # user, service, system
+    access_level: AccessLevel
+    policies: List[str] = field(default_factory=list)
+    created_at: datetime = field(default_factory=datetime.now)
+    last_login: Optional[datetime] = None
+    mfa_enabled: bool = False
+    api_key_hash: Optional[str] = None
+
+
+class SecretEncryption:
+    """Handles encryption/decryption of secrets."""
+
+    def __init__(self, master_key: Optional[str] = None):
+        self.logger = logging.getLogger(__name__)
+        if master_key:
+            self.key = master_key.encode()
+        else:
+            self.key = self._generate_key_from_environment()
+        self.fernet = Fernet(base64.urlsafe_b64encode(self.key[:32]))
+
+    def _generate_key_from_environment(self) -> bytes:
+        """Generate encryption key from environment variables."""
+        # Use multiple environment variables for key derivation
+        sources = [
+            os.environ.get('WALLSTREETBOTS_MASTER_KEY', ''),
+            os.environ.get('HOSTNAME', ''),
+            os.environ.get('USER', ''),
+            'wallstreetbots_default_salt'
+        ]
+
+        combined = ''.join(sources).encode()
+        kdf = PBKDF2HMAC(
+            algorithm=hashes.SHA256(),
+            length=32,
+            salt=b'wallstreetbots_salt_2024',
+            iterations=100000,
+        )
+        return kdf.derive(combined)
+
+    def encrypt(self, plaintext: str) -> str:
+        """Encrypt plaintext value."""
+        try:
+            encrypted = self.fernet.encrypt(plaintext.encode())
+            return base64.urlsafe_b64encode(encrypted).decode()
+        except Exception as e:
+            self.logger.error(f"Encryption failed: {e}")
+            raise
+
+    def decrypt(self, ciphertext: str) -> str:
+        """Decrypt ciphertext value."""
+        try:
+            encrypted_data = base64.urlsafe_b64decode(ciphertext.encode())
+            decrypted = self.fernet.decrypt(encrypted_data)
+            return decrypted.decode()
+        except Exception as e:
+            self.logger.error(f"Decryption failed: {e}")
+            raise
+
+
+class SecretsManager:
+    """Manages secrets with encryption and access control."""
+
+    def __init__(self, storage_path: str | None = None):
+        self.logger = logging.getLogger(__name__)
+        self.encryption = SecretEncryption()
+        self.storage_path = Path(storage_path or os.environ.get('SECRETS_STORAGE_PATH', '/tmp/wallstreetbots_secrets'))
+        self.storage_path.mkdir(parents=True, exist_ok=True)
+        self.secrets: Dict[str, Secret] = {}
+        self.audit_log: List[Dict[str, Any]] = []
+        self._load_secrets()
+
+    def _load_secrets(self):
+        """Load encrypted secrets from storage."""
+        secrets_file = self.storage_path / 'secrets.enc'
+        if secrets_file.exists():
+            try:
+                with open(secrets_file, 'r') as f:
+                    encrypted_data = f.read()
+
+                decrypted_data = self.encryption.decrypt(encrypted_data)
+                secrets_dict = json.loads(decrypted_data)
+
+                for name, secret_data in secrets_dict.items():
+                    secret = Secret(
+                        name=secret_data['name'],
+                        secret_type=SecretType(secret_data['secret_type']),
+                        value=secret_data['value'],
+                        description=secret_data.get('description', ''),
+                        created_at=datetime.fromisoformat(secret_data['created_at']),
+                        expires_at=datetime.fromisoformat(secret_data['expires_at']) if secret_data.get('expires_at') else None,
+                        last_accessed=datetime.fromisoformat(secret_data['last_accessed']) if secret_data.get('last_accessed') else None,
+                        access_count=secret_data.get('access_count', 0),
+                        tags=secret_data.get('tags', {}),
+                        rotation_required=secret_data.get('rotation_required', False)
+                    )
+                    self.secrets[name] = secret
+
+                self.logger.info(f"Loaded {len(self.secrets)} secrets from storage")
+
+            except Exception as e:
+                self.logger.error(f"Failed to load secrets: {e}")
+
+    def _save_secrets(self):
+        """Save encrypted secrets to storage."""
+        try:
+            secrets_dict = {}
+            for name, secret in self.secrets.items():
+                secrets_dict[name] = {
+                    'name': secret.name,
+                    'secret_type': secret.secret_type.value,
+                    'value': secret.value,
+                    'description': secret.description,
+                    'created_at': secret.created_at.isoformat(),
+                    'expires_at': secret.expires_at.isoformat() if secret.expires_at else None,
+                    'last_accessed': secret.last_accessed.isoformat() if secret.last_accessed else None,
+                    'access_count': secret.access_count,
+                    'tags': secret.tags,
+                    'rotation_required': secret.rotation_required
+                }
+
+            plaintext_data = json.dumps(secrets_dict, indent=2)
+            encrypted_data = self.encryption.encrypt(plaintext_data)
+
+            secrets_file = self.storage_path / 'secrets.enc'
+            with open(secrets_file, 'w') as f:
+                f.write(encrypted_data)
+
+            # Set restrictive permissions
+            os.chmod(secrets_file, 0o600)
+
+            self.logger.info(f"Saved {len(self.secrets)} secrets to storage")
+
+        except Exception as e:
+            self.logger.error(f"Failed to save secrets: {e}")
+            raise
+
+    def store_secret(self, name: str, value: str, secret_type: SecretType,
+                    description: str = "", expires_in_days: Optional[int] = None,
+                    tags: Optional[Dict[str, str]] = None) -> bool:
+        """Store a new secret."""
+        try:
+            expires_at = None
+            if expires_in_days:
+                expires_at = datetime.now() + timedelta(days=expires_in_days)
+
+            secret = Secret(
+                name=name,
+                secret_type=secret_type,
+                value=value,
+                description=description,
+                expires_at=expires_at,
+                tags=tags or {}
+            )
+
+            self.secrets[name] = secret
+            self._save_secrets()
+
+            self._audit_log('STORE_SECRET', {'secret_name': name, 'secret_type': secret_type.value})
+            self.logger.info(f"Stored secret: {name}")
+            return True
+
+        except Exception as e:
+            self.logger.error(f"Failed to store secret {name}: {e}")
+            return False
+
+    def get_secret(self, name: str, principal_id: Optional[str] = None) -> Optional[str]:
+        """Retrieve a secret value."""
+        try:
+            if name not in self.secrets:
+                self._audit_log('GET_SECRET_FAILED', {'secret_name': name, 'reason': 'not_found', 'principal_id': principal_id})
+                return None
+
+            secret = self.secrets[name]
+
+            # Check expiration
+            if secret.expires_at and datetime.now() > secret.expires_at:
+                self._audit_log('GET_SECRET_FAILED', {'secret_name': name, 'reason': 'expired', 'principal_id': principal_id})
+                self.logger.warning(f"Secret {name} has expired")
+                return None
+
+            # Update access tracking
+            secret.last_accessed = datetime.now()
+            secret.access_count += 1
+            self._save_secrets()
+
+            self._audit_log('GET_SECRET', {'secret_name': name, 'principal_id': principal_id})
+            return secret.value
+
+        except Exception as e:
+            self.logger.error(f"Failed to get secret {name}: {e}")
+            self._audit_log('GET_SECRET_ERROR', {'secret_name': name, 'error': str(e), 'principal_id': principal_id})
+            return None
+
+    def rotate_secret(self, name: str, new_value: str) -> bool:
+        """Rotate a secret value."""
+        try:
+            if name not in self.secrets:
+                return False
+
+            secret = self.secrets[name]
+            old_value_hash = hashlib.sha256(secret.value.encode()).hexdigest()[:8]
+
+            secret.value = new_value
+            secret.rotation_required = False
+            secret.created_at = datetime.now()  # Reset creation time
+
+            self._save_secrets()
+
+            self._audit_log('ROTATE_SECRET', {
+                'secret_name': name,
+                'old_value_hash': old_value_hash
+            })
+
+            self.logger.info(f"Rotated secret: {name}")
+            return True
+
+        except Exception as e:
+            self.logger.error(f"Failed to rotate secret {name}: {e}")
+            return False
+
+    def delete_secret(self, name: str) -> bool:
+        """Delete a secret."""
+        try:
+            if name in self.secrets:
+                del self.secrets[name]
+                self._save_secrets()
+
+                self._audit_log('DELETE_SECRET', {'secret_name': name})
+                self.logger.info(f"Deleted secret: {name}")
+                return True
+            return False
+
+        except Exception as e:
+            self.logger.error(f"Failed to delete secret {name}: {e}")
+            return False
+
+    def list_secrets(self) -> List[Dict[str, Any]]:
+        """List all secrets (metadata only)."""
+        result = []
+        for name, secret in self.secrets.items():
+            result.append({
+                'name': name,
+                'secret_type': secret.secret_type.value,
+                'description': secret.description,
+                'created_at': secret.created_at.isoformat(),
+                'expires_at': secret.expires_at.isoformat() if secret.expires_at else None,
+                'last_accessed': secret.last_accessed.isoformat() if secret.last_accessed else None,
+                'access_count': secret.access_count,
+                'rotation_required': secret.rotation_required,
+                'tags': secret.tags
+            })
+        return result
+
+    def check_expiring_secrets(self, days_ahead: int = 30) -> List[str]:
+        """Check for secrets expiring within specified days."""
+        cutoff_date = datetime.now() + timedelta(days=days_ahead)
+        expiring = []
+
+        for name, secret in self.secrets.items():
+            if secret.expires_at and secret.expires_at <= cutoff_date:
+                expiring.append(name)
+
+        return expiring
+
+    def mark_for_rotation(self, name: str) -> bool:
+        """Mark a secret for rotation."""
+        if name in self.secrets:
+            self.secrets[name].rotation_required = True
+            self._save_secrets()
+            self._audit_log('MARK_FOR_ROTATION', {'secret_name': name})
+            return True
+        return False
+
+    def _audit_log(self, action: str, details: Dict[str, Any]):
+        """Log audit events."""
+        log_entry = {
+            'timestamp': datetime.now().isoformat(),
+            'action': action,
+            'details': details
+        }
+        self.audit_log.append(log_entry)
+
+        # Keep only last 1000 entries
+        if len(self.audit_log) > 1000:
+            self.audit_log = self.audit_log[-1000:]
+
+
+class IAMManager:
+    """Identity and Access Management for trading systems."""
+
+    def __init__(self, secrets_manager: SecretsManager):
+        self.secrets_manager = secrets_manager
+        self.logger = logging.getLogger(__name__)
+        self.principals: Dict[str, Principal] = {}
+        self.policies: Dict[str, AccessPolicy] = {}
+        self.active_sessions: Dict[str, Dict[str, Any]] = {}
+        self._setup_default_policies()
+
+    def _setup_default_policies(self):
+        """Setup default access policies."""
+
+        # Trading system service policy
+        self.policies['trading_service'] = AccessPolicy(
+            name='trading_service',
+            description='Full access for trading service',
+            allowed_secrets={'broker_api_key', 'database_password', 'risk_engine_key'},
+            allowed_operations={'read', 'rotate'},
+            conditions={
+                'source_ip_ranges': ['10.0.0.0/8', '172.16.0.0/12'],
+                'time_restrictions': None
+            }
+        )
+
+        # Read-only policy for monitoring
+        self.policies['monitoring_readonly'] = AccessPolicy(
+            name='monitoring_readonly',
+            description='Read-only access for monitoring systems',
+            allowed_secrets={'monitoring_api_key'},
+            allowed_operations={'read'},
+            conditions={
+                'source_ip_ranges': ['10.0.0.0/8'],
+                'rate_limit_per_hour': 100
+            }
+        )
+
+        # Admin policy
+        self.policies['admin'] = AccessPolicy(
+            name='admin',
+            description='Full administrative access',
+            allowed_secrets=set(),  # Empty means all secrets
+            allowed_operations={'read', 'write', 'delete', 'rotate'},
+            conditions={
+                'mfa_required': True,
+                'session_timeout_minutes': 30
+            }
+        )
+
+    def create_principal(self, principal_id: str, name: str, principal_type: str,
+                        access_level: AccessLevel, policies: List[str] | None = None) -> bool:
+        """Create a new principal (user/service)."""
+        try:
+            # Validate policies exist
+            if policies:
+                for policy_name in policies:
+                    if policy_name not in self.policies:
+                        raise ValueError(f"Policy not found: {policy_name}")
+
+            principal = Principal(
+                id=principal_id,
+                name=name,
+                principal_type=principal_type,
+                access_level=access_level,
+                policies=policies or []
+            )
+
+            self.principals[principal_id] = principal
+            self.logger.info(f"Created principal: {principal_id} ({name})")
+            return True
+
+        except Exception as e:
+            self.logger.error(f"Failed to create principal {principal_id}: {e}")
+            return False
+
+    def generate_api_key(self, principal_id: str) -> Optional[str]:
+        """Generate API key for a principal."""
+        if principal_id not in self.principals:
+            return None
+
+        # Generate secure API key
+        api_key = base64.urlsafe_b64encode(os.urandom(32)).decode().rstrip('=')
+        api_key = f"wsb_{principal_id}_{api_key}"
+
+        # Store hash of API key
+        key_hash = hashlib.sha256(api_key.encode()).hexdigest()
+        self.principals[principal_id].api_key_hash = key_hash
+
+        self.logger.info(f"Generated API key for principal: {principal_id}")
+        return api_key
+
+    def validate_api_key(self, api_key: str) -> Optional[Principal]:
+        """Validate API key and return principal."""
+        try:
+            if not api_key.startswith('wsb_'):
+                return None
+
+            key_hash = hashlib.sha256(api_key.encode()).hexdigest()
+
+            for principal in self.principals.values():
+                if principal.api_key_hash == key_hash:
+                    principal.last_login = datetime.now()
+                    return principal
+
+            return None
+
+        except Exception as e:
+            self.logger.error(f"API key validation error: {e}")
+            return None
+
+    def check_access(self, principal_id: str, secret_name: str, operation: str,
+                    context: Dict[str, Any] | None = None) -> Tuple[bool, str]:
+        """Check if principal has access to perform operation on secret."""
+        try:
+            if principal_id not in self.principals:
+                return False, "Principal not found"
+
+            principal = self.principals[principal_id]
+            context = context or {}
+
+            # Check each policy attached to principal
+            for policy_name in principal.policies:
+                if policy_name not in self.policies:
+                    continue
+
+                policy = self.policies[policy_name]
+
+                # Check if policy allows this secret (empty set means all secrets)
+                if policy.allowed_secrets and secret_name not in policy.allowed_secrets:
+                    continue
+
+                # Check if policy allows this operation
+                if operation not in policy.allowed_operations:
+                    continue
+
+                # Check policy conditions
+                access_allowed, reason = self._check_policy_conditions(policy, context)
+                if access_allowed:
+                    return True, "Access granted"
+
+            return False, "No policy grants access"
+
+        except Exception as e:
+            self.logger.error(f"Access check error: {e}")
+            return False, f"Error: {e}"
+
+    def _check_policy_conditions(self, policy: AccessPolicy, context: Dict[str, Any]) -> Tuple[bool, str]:
+        """Check if policy conditions are met."""
+        conditions = policy.conditions
+
+        # Check IP restrictions
+        if 'source_ip_ranges' in conditions and 'source_ip' in context:
+            source_ip = context['source_ip']
+            allowed_ranges = conditions['source_ip_ranges']
+            # Simplified IP range check (would use ipaddress module in production)
+            ip_allowed = any(source_ip.startswith(range_prefix.split('/')[0][:3])
+                           for range_prefix in allowed_ranges)
+            if not ip_allowed:
+                return False, "IP address not in allowed ranges"
+
+        # Check MFA requirement
+        if conditions.get('mfa_required', False) and not context.get('mfa_verified', False):
+            return False, "MFA required but not verified"
+
+        # Check time restrictions
+        if conditions.get('time_restrictions'):
+            current_hour = datetime.now().hour
+            allowed_hours = conditions['time_restrictions']
+            if current_hour not in allowed_hours:
+                return False, "Access not allowed at this time"
+
+        # Check rate limiting
+        if 'rate_limit_per_hour' in conditions:
+            # Would implement rate limiting logic here
+            pass
+
+        return True, "All conditions met"
+
+    def create_session(self, principal_id: str, context: Dict[str, Any] | None = None) -> Optional[str]:
+        """Create authenticated session for principal."""
+        if principal_id not in self.principals:
+            return None
+
+        session_id = base64.urlsafe_b64encode(os.urandom(24)).decode().rstrip('=')
+
+        session_data = {
+            'principal_id': principal_id,
+            'created_at': datetime.now(),
+            'last_activity': datetime.now(),
+            'context': context or {}
+        }
+
+        self.active_sessions[session_id] = session_data
+        self.logger.info(f"Created session for principal: {principal_id}")
+        return session_id
+
+    def validate_session(self, session_id: str) -> Optional[Principal]:
+        """Validate session and return principal."""
+        if session_id not in self.active_sessions:
+            return None
+
+        session_data = self.active_sessions[session_id]
+        principal_id = session_data['principal_id']
+
+        if principal_id not in self.principals:
+            del self.active_sessions[session_id]
+            return None
+
+        # Check session timeout
+        session_timeout = timedelta(minutes=30)  # Default timeout
+        if datetime.now() - session_data['last_activity'] > session_timeout:
+            del self.active_sessions[session_id]
+            return None
+
+        # Update last activity
+        session_data['last_activity'] = datetime.now()
+
+        return self.principals[principal_id]
+
+    def revoke_session(self, session_id: str) -> bool:
+        """Revoke an active session."""
+        if session_id in self.active_sessions:
+            del self.active_sessions[session_id]
+            self.logger.info(f"Revoked session: {session_id}")
+            return True
+        return False
+
+    def get_principal_permissions(self, principal_id: str) -> Dict[str, Any]:
+        """Get detailed permissions for a principal."""
+        if principal_id not in self.principals:
+            return {}
+
+        principal = self.principals[principal_id]
+        permissions = {
+            'principal_id': principal_id,
+            'name': principal.name,
+            'access_level': principal.access_level.value,
+            'policies': [],
+            'effective_permissions': {
+                'secrets': set(),
+                'operations': set()
+            }
+        }
+
+        for policy_name in principal.policies:
+            if policy_name in self.policies:
+                policy = self.policies[policy_name]
+                permissions['policies'].append({
+                    'name': policy.name,
+                    'description': policy.description,
+                    'allowed_secrets': list(policy.allowed_secrets),
+                    'allowed_operations': list(policy.allowed_operations),
+                    'conditions': policy.conditions
+                })
+
+                # Aggregate effective permissions
+                if not policy.allowed_secrets:  # Empty means all secrets
+                    permissions['effective_permissions']['secrets'] = {'*'}
+                else:
+                    permissions['effective_permissions']['secrets'].update(policy.allowed_secrets)
+
+                permissions['effective_permissions']['operations'].update(policy.allowed_operations)
+
+        # Convert sets to lists for JSON serialization
+        permissions['effective_permissions']['secrets'] = list(permissions['effective_permissions']['secrets'])
+        permissions['effective_permissions']['operations'] = list(permissions['effective_permissions']['operations'])
+
+        return permissions
+
+
+class TradingSecretsHardening:
+    """Complete secrets and IAM hardening for trading systems."""
+
+    def __init__(self):
+        self.secrets_manager = SecretsManager()
+        self.iam_manager = IAMManager(self.secrets_manager)
+        self.logger = logging.getLogger(__name__)
+        self._setup_trading_secrets()
+        self._setup_trading_principals()
+
+    def _setup_trading_secrets(self):
+        """Setup standard trading system secrets."""
+
+        # Only setup if not already exist
+        existing_secrets = {s['name'] for s in self.secrets_manager.list_secrets()}
+
+        if 'broker_api_key' not in existing_secrets:
+            self.secrets_manager.store_secret(
+                'broker_api_key',
+                'demo_broker_key_' + base64.urlsafe_b64encode(os.urandom(16)).decode(),
+                SecretType.BROKER_CREDENTIALS,
+                'Broker API access key',
+                expires_in_days=90,
+                tags={'environment': 'production', 'rotation_frequency': 'quarterly'}
+            )
+
+        if 'database_password' not in existing_secrets:
+            self.secrets_manager.store_secret(
+                'database_password',
+                base64.urlsafe_b64encode(os.urandom(24)).decode(),
+                SecretType.DATABASE_PASSWORD,
+                'Trading database password',
+                expires_in_days=30,
+                tags={'environment': 'production', 'rotation_frequency': 'monthly'}
+            )
+
+        if 'risk_engine_key' not in existing_secrets:
+            self.secrets_manager.store_secret(
+                'risk_engine_key',
+                base64.urlsafe_b64encode(os.urandom(32)).decode(),
+                SecretType.ENCRYPTION_KEY,
+                'Risk engine encryption key',
+                expires_in_days=365,
+                tags={'environment': 'production', 'rotation_frequency': 'yearly'}
+            )
+
+        if 'webhook_secret' not in existing_secrets:
+            self.secrets_manager.store_secret(
+                'webhook_secret',
+                base64.urlsafe_b64encode(os.urandom(32)).decode(),
+                SecretType.WEBHOOK_SECRET,
+                'Webhook verification secret',
+                expires_in_days=180,
+                tags={'environment': 'production', 'rotation_frequency': 'biannually'}
+            )
+
+    def _setup_trading_principals(self):
+        """Setup standard trading system principals."""
+
+        # Trading service principal
+        self.iam_manager.create_principal(
+            'trading_service',
+            'Trading Service',
+            'service',
+            AccessLevel.SERVICE,
+            ['trading_service']
+        )
+
+        # Risk monitoring service
+        self.iam_manager.create_principal(
+            'risk_monitor',
+            'Risk Monitoring Service',
+            'service',
+            AccessLevel.READ_ONLY,
+            ['monitoring_readonly']
+        )
+
+        # Admin user
+        self.iam_manager.create_principal(
+            'admin_user',
+            'System Administrator',
+            'user',
+            AccessLevel.ADMIN,
+            ['admin']
+        )
+
+    def setup_production_secrets(self) -> Dict[str, str]:
+        """Setup production secrets and return API keys."""
+        api_keys = {}
+
+        # Generate API keys for services
+        for principal_id in ['trading_service', 'risk_monitor']:
+            api_key = self.iam_manager.generate_api_key(principal_id)
+            if api_key:
+                api_keys[principal_id] = api_key
+
+        return api_keys
+
+    def perform_security_audit(self) -> Dict[str, Any]:
+        """Perform comprehensive security audit."""
+        audit_results = {
+            'timestamp': datetime.now().isoformat(),
+            'secrets_audit': self._audit_secrets(),
+            'principals_audit': self._audit_principals(),
+            'sessions_audit': self._audit_sessions(),
+            'recommendations': []
+        }
+
+        # Generate recommendations
+        recommendations = self._generate_security_recommendations(audit_results)
+        audit_results['recommendations'] = recommendations
+
+        return audit_results
+
+    def _audit_secrets(self) -> Dict[str, Any]:
+        """Audit secrets management."""
+        secrets = self.secrets_manager.list_secrets()
+
+        return {
+            'total_secrets': len(secrets),
+            'expiring_soon': len(self.secrets_manager.check_expiring_secrets(30)),
+            'requiring_rotation': len([s for s in secrets if s['rotation_required']]),
+            'never_accessed': len([s for s in secrets if not s['last_accessed']]),
+            'high_access_count': len([s for s in secrets if s['access_count'] > 1000])
+        }
+
+    def _audit_principals(self) -> Dict[str, Any]:
+        """Audit principals and permissions."""
+        principals = list(self.iam_manager.principals.values())
+
+        return {
+            'total_principals': len(principals),
+            'service_accounts': len([p for p in principals if p.principal_type == 'service']),
+            'user_accounts': len([p for p in principals if p.principal_type == 'user']),
+            'mfa_enabled': len([p for p in principals if p.mfa_enabled]),
+            'admin_access': len([p for p in principals if p.access_level == AccessLevel.ADMIN])
+        }
+
+    def _audit_sessions(self) -> Dict[str, Any]:
+        """Audit active sessions."""
+        sessions = list(self.iam_manager.active_sessions.values())
+        now = datetime.now()
+
+        return {
+            'active_sessions': len(sessions),
+            'old_sessions': len([s for s in sessions if (now - s['last_activity']).total_seconds() > 3600])
+        }
+
+    def _generate_security_recommendations(self, audit_results: Dict[str, Any]) -> List[str]:
+        """Generate security recommendations based on audit."""
+        recommendations = []
+
+        # Secrets recommendations
+        secrets_audit = audit_results['secrets_audit']
+        if secrets_audit['expiring_soon'] > 0:
+            recommendations.append(f"Rotate {secrets_audit['expiring_soon']} secrets expiring within 30 days")
+
+        if secrets_audit['requiring_rotation'] > 0:
+            recommendations.append(f"Complete rotation for {secrets_audit['requiring_rotation']} marked secrets")
+
+        if secrets_audit['never_accessed'] > 0:
+            recommendations.append(f"Review {secrets_audit['never_accessed']} unused secrets for cleanup")
+
+        # Principals recommendations
+        principals_audit = audit_results['principals_audit']
+        if principals_audit['mfa_enabled'] < principals_audit['user_accounts']:
+            recommendations.append("Enable MFA for all user accounts")
+
+        if principals_audit['admin_access'] > 2:
+            recommendations.append("Review admin access - consider principle of least privilege")
+
+        # Sessions recommendations
+        sessions_audit = audit_results['sessions_audit']
+        if sessions_audit['old_sessions'] > 0:
+            recommendations.append(f"Cleanup {sessions_audit['old_sessions']} stale sessions")
+
+        return recommendations
+
+    def rotate_all_expiring_secrets(self) -> Dict[str, bool]:
+        """Rotate all secrets expiring within 7 days."""
+        expiring_secrets = self.secrets_manager.check_expiring_secrets(7)
+        results = {}
+
+        for secret_name in expiring_secrets:
+            # Generate new secret value based on type
+            secret_list = self.secrets_manager.list_secrets()
+            secret_info = next((s for s in secret_list if s['name'] == secret_name), None)
+
+            if secret_info:
+                secret_type = SecretType(secret_info['secret_type'])
+                new_value = self._generate_secret_value(secret_type)
+                results[secret_name] = self.secrets_manager.rotate_secret(secret_name, new_value)
+
+        return results
+
+    def _generate_secret_value(self, secret_type: SecretType) -> str:
+        """Generate new secret value based on type."""
+        if secret_type == SecretType.API_KEY:
+            return 'demo_key_' + base64.urlsafe_b64encode(os.urandom(24)).decode()
+        elif secret_type == SecretType.DATABASE_PASSWORD:
+            return base64.urlsafe_b64encode(os.urandom(24)).decode()
+        elif secret_type in [SecretType.ENCRYPTION_KEY, SecretType.WEBHOOK_SECRET]:
+            return base64.urlsafe_b64encode(os.urandom(32)).decode()
+        else:
+            return base64.urlsafe_b64encode(os.urandom(24)).decode()
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    def demo_secrets_and_iam():
+        print("=== Trading Secrets & IAM Hardening Demo ===")
+
+        # Initialize system
+        hardening = TradingSecretsHardening()
+
+        # Setup production secrets
+        api_keys = hardening.setup_production_secrets()
+        print(f"Generated API keys for {len(api_keys)} services")
+
+        # Test secret access
+        trading_key = hardening.secrets_manager.get_secret('broker_api_key', 'trading_service')
+        print(f"Retrieved broker API key: {trading_key[:10]}...")
+
+        # Test IAM access control
+        api_key = api_keys.get('trading_service')
+        if api_key:
+            principal = hardening.iam_manager.validate_api_key(api_key)
+            if principal:
+                can_access, reason = hardening.iam_manager.check_access(
+                    principal.id, 'broker_api_key', 'read'
+                )
+                print(f"Access check: {can_access} - {reason}")
+
+        # Perform security audit
+        audit = hardening.perform_security_audit()
+        print("\nSecurity Audit Results:")
+        print(f"- Total secrets: {audit['secrets_audit']['total_secrets']}")
+        print(f"- Active principals: {audit['principals_audit']['total_principals']}")
+        print(f"- Active sessions: {audit['sessions_audit']['active_sessions']}")
+        print(f"- Recommendations: {len(audit['recommendations'])}")
+
+        for rec in audit['recommendations'][:3]:  # Show first 3
+            print(f"  â€¢ {rec}")
+
+    demo_secrets_and_iam()
\ No newline at end of file
Index: backend/validation/execution_reality/slippage_calibration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/execution_reality/slippage_calibration.py b/backend/validation/execution_reality/slippage_calibration.py
new file mode 100644
--- /dev/null	(date 1758169876676)
+++ b/backend/validation/execution_reality/slippage_calibration.py	(date 1758169876676)
@@ -0,0 +1,501 @@
+"""Real-time slippage calibration and execution quality monitoring.
+
+Continuously calibrates slippage models against live market data and
+adjusts execution expectations based on actual fills.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Tuple, Any
+import logging
+from datetime import datetime, timedelta
+from dataclasses import dataclass, asdict
+from sklearn.linear_model import LinearRegression
+from sklearn.ensemble import RandomForestRegressor
+import json
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class ExecutionRecord:
+    """Record of a single execution with all relevant data."""
+    timestamp: datetime
+    symbol: str
+    side: str  # 'buy' or 'sell'
+    order_type: str  # 'market', 'limit', 'ioc', etc.
+    intended_quantity: int
+    filled_quantity: int
+    intended_price: Optional[float]  # For limit orders
+    fill_price: float
+    market_price_at_order: float  # Best bid/ask when order placed
+    spread_at_order: float
+    volume_at_order: int  # Volume in order book
+    volatility_at_order: float  # Recent realized volatility
+    time_of_day: str  # Market session info
+    days_to_expiry: Optional[int]  # For options
+    order_id: str
+    latency_ms: float  # Order to ack latency
+    partial_fill: bool
+
+
+@dataclass
+class SlippageModel:
+    """Slippage prediction model with coefficients."""
+    model_type: str  # 'linear', 'random_forest'
+    coefficients: Dict[str, float]
+    feature_names: List[str]
+    r_squared: float
+    mae: float  # Mean absolute error
+    training_samples: int
+    last_updated: datetime
+    asset_class: str  # 'equity', 'option', 'etf'
+
+
+class SlippagePredictor:
+    """Predicts expected slippage based on market conditions."""
+
+    def __init__(self):
+        self.models = {}  # Asset class -> SlippageModel
+        self.execution_history = []
+        self.calibration_window = 30  # Days of data for model training
+
+    def predict_slippage(self, symbol: str, side: str, quantity: int,
+                        market_conditions: Dict[str, float]) -> Dict[str, float]:
+        """Predict expected slippage for an order.
+
+        Args:
+            symbol: Trading symbol
+            side: 'buy' or 'sell'
+            quantity: Order size
+            market_conditions: Current market state
+
+        Returns:
+            Dictionary with slippage predictions and confidence intervals
+        """
+        asset_class = self._classify_asset(symbol)
+
+        if asset_class not in self.models:
+            # Use default model if no calibrated model exists
+            return self._default_slippage_estimate(symbol, side, quantity, market_conditions)
+
+        model = self.models[asset_class]
+        features = self._extract_features(symbol, side, quantity, market_conditions)
+
+        try:
+            if model.model_type == 'linear':
+                predicted_slippage = self._predict_linear(model, features)
+            elif model.model_type == 'random_forest':
+                predicted_slippage = self._predict_rf(model, features)
+            else:
+                return self._default_slippage_estimate(symbol, side, quantity, market_conditions)
+
+            # Add confidence intervals based on model error
+            confidence_interval = model.mae * 2  # Approximate 95% CI
+
+            return {
+                'expected_slippage_bps': predicted_slippage,
+                'confidence_interval_bps': confidence_interval,
+                'model_r_squared': model.r_squared,
+                'model_last_updated': model.last_updated.isoformat(),
+                'prediction_confidence': min(0.9, model.r_squared)
+            }
+
+        except Exception as e:
+            logger.error(f"Slippage prediction failed: {e}")
+            return self._default_slippage_estimate(symbol, side, quantity, market_conditions)
+
+    def record_execution(self, execution: ExecutionRecord):
+        """Record an execution for model calibration."""
+        self.execution_history.append(execution)
+
+        # Calculate actual slippage
+        if execution.side == 'buy':
+            slippage_bps = (execution.fill_price - execution.market_price_at_order) / execution.market_price_at_order * 10000
+        else:  # sell
+            slippage_bps = (execution.market_price_at_order - execution.fill_price) / execution.market_price_at_order * 10000
+
+        # Store for model training
+        execution.actual_slippage_bps = slippage_bps
+
+        logger.info(f"Recorded execution: {execution.symbol} {execution.side} "
+                   f"{execution.filled_quantity} @ {execution.fill_price:.4f}, "
+                   f"slippage: {slippage_bps:.2f} bps")
+
+    def calibrate_models(self) -> Dict[str, Any]:
+        """Recalibrate slippage models with recent execution data."""
+        if len(self.execution_history) < 50:
+            return {'status': 'insufficient_data', 'executions': len(self.execution_history)}
+
+        calibration_results = {}
+
+        # Group executions by asset class
+        executions_by_class = {}
+        for execution in self.execution_history:
+            asset_class = self._classify_asset(execution.symbol)
+            if asset_class not in executions_by_class:
+                executions_by_class[asset_class] = []
+            executions_by_class[asset_class].append(execution)
+
+        # Train models for each asset class
+        for asset_class, executions in executions_by_class.items():
+            if len(executions) < 30:
+                logger.warning(f"Insufficient data for {asset_class}: {len(executions)} executions")
+                continue
+
+            try:
+                # Filter to recent data
+                cutoff_date = datetime.now() - timedelta(days=self.calibration_window)
+                recent_executions = [e for e in executions if e.timestamp >= cutoff_date]
+
+                if len(recent_executions) < 20:
+                    continue
+
+                # Train models
+                linear_model = self._train_linear_model(recent_executions, asset_class)
+                rf_model = self._train_rf_model(recent_executions, asset_class)
+
+                # Choose best model
+                if rf_model.r_squared > linear_model.r_squared + 0.05:  # RF significantly better
+                    self.models[asset_class] = rf_model
+                    calibration_results[asset_class] = 'random_forest'
+                else:
+                    self.models[asset_class] = linear_model
+                    calibration_results[asset_class] = 'linear'
+
+                logger.info(f"Calibrated {calibration_results[asset_class]} model for {asset_class}: "
+                           f"RÂ² = {self.models[asset_class].r_squared:.3f}")
+
+            except Exception as e:
+                logger.error(f"Model calibration failed for {asset_class}: {e}")
+                calibration_results[asset_class] = f'failed: {e!s}'
+
+        return calibration_results
+
+    def _classify_asset(self, symbol: str) -> str:
+        """Classify asset type for model selection."""
+        if any(x in symbol.upper() for x in ['SPY', 'QQQ', 'IWM', 'VIX', 'ETF']):
+            return 'etf'
+        elif len(symbol) > 5 or any(x in symbol for x in ['C', 'P', '2']):  # Options-like
+            return 'option'
+        else:
+            return 'equity'
+
+    def _extract_features(self, symbol: str, side: str, quantity: int,
+                         market_conditions: Dict[str, float]) -> np.ndarray:
+        """Extract features for slippage prediction."""
+        features = [
+            np.log(quantity + 1),  # Log order size
+            market_conditions.get('spread_bps', 10),  # Bid-ask spread in bps
+            market_conditions.get('volume', 1000),  # Current volume
+            market_conditions.get('volatility', 0.02) * 10000,  # Volatility in bps
+            1 if side == 'buy' else 0,  # Buy indicator
+            market_conditions.get('time_score', 0.5),  # Time of day score (0-1)
+            market_conditions.get('market_impact_score', 0.1),  # Expected market impact
+        ]
+
+        return np.array(features).reshape(1, -1)
+
+    def _train_linear_model(self, executions: List[ExecutionRecord], asset_class: str) -> SlippageModel:
+        """Train linear regression model for slippage prediction."""
+        # Prepare training data
+        X, y = self._prepare_training_data(executions)
+
+        if len(X) < 10:
+            raise ValueError(f"Insufficient training data: {len(X)} samples")
+
+        # Train model
+        model = LinearRegression()
+        model.fit(X, y)
+
+        # Calculate metrics
+        y_pred = model.predict(X)
+        r_squared = model.score(X, y)
+        mae = np.mean(np.abs(y - y_pred))
+
+        # Extract coefficients
+        feature_names = ['log_quantity', 'spread_bps', 'volume', 'volatility', 'is_buy', 'time_score', 'market_impact']
+        coefficients = dict(zip(feature_names, model.coef_))
+        coefficients['intercept'] = model.intercept_
+
+        return SlippageModel(
+            model_type='linear',
+            coefficients=coefficients,
+            feature_names=feature_names,
+            r_squared=r_squared,
+            mae=mae,
+            training_samples=len(X),
+            last_updated=datetime.now(),
+            asset_class=asset_class
+        )
+
+    def _train_rf_model(self, executions: List[ExecutionRecord], asset_class: str) -> SlippageModel:
+        """Train random forest model for slippage prediction."""
+        X, y = self._prepare_training_data(executions)
+
+        if len(X) < 20:
+            raise ValueError(f"Insufficient training data for RF: {len(X)} samples")
+
+        # Train model
+        model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)
+        model.fit(X, y)
+
+        # Calculate metrics
+        y_pred = model.predict(X)
+        r_squared = model.score(X, y)
+        mae = np.mean(np.abs(y - y_pred))
+
+        # Feature importance as "coefficients"
+        feature_names = ['log_quantity', 'spread_bps', 'volume', 'volatility', 'is_buy', 'time_score', 'market_impact']
+        coefficients = dict(zip(feature_names, model.feature_importances_))
+
+        return SlippageModel(
+            model_type='random_forest',
+            coefficients=coefficients,
+            feature_names=feature_names,
+            r_squared=r_squared,
+            mae=mae,
+            training_samples=len(X),
+            last_updated=datetime.now(),
+            asset_class=asset_class
+        )
+
+    def _prepare_training_data(self, executions: List[ExecutionRecord]) -> Tuple[np.ndarray, np.ndarray]:
+        """Prepare training data from execution records."""
+        X, y = [], []
+
+        for execution in executions:
+            if not hasattr(execution, 'actual_slippage_bps'):
+                continue
+
+            # Features
+            features = [
+                np.log(execution.filled_quantity + 1),
+                execution.spread_at_order / execution.market_price_at_order * 10000,  # Spread in bps
+                execution.volume_at_order,
+                execution.volatility_at_order * 10000,  # Vol in bps
+                1 if execution.side == 'buy' else 0,
+                self._time_to_score(execution.time_of_day),
+                self._calculate_market_impact_score(execution)
+            ]
+
+            X.append(features)
+            y.append(execution.actual_slippage_bps)
+
+        return np.array(X), np.array(y)
+
+    def _time_to_score(self, time_of_day: str) -> float:
+        """Convert time of day to a score (0-1)."""
+        try:
+            if 'open' in time_of_day.lower():
+                return 0.1  # Market open - higher slippage
+            elif 'close' in time_of_day.lower():
+                return 0.2  # Market close - higher slippage
+            else:
+                return 0.5  # Mid-day - normal slippage
+        except Exception:
+            return 0.5
+
+    def _calculate_market_impact_score(self, execution: ExecutionRecord) -> float:
+        """Calculate a market impact score for the execution."""
+        # Simplified market impact based on order size relative to volume
+        if execution.volume_at_order > 0:
+            impact_ratio = execution.filled_quantity / execution.volume_at_order
+            return min(impact_ratio * 100, 1.0)  # Cap at 1.0
+        return 0.1
+
+    def _predict_linear(self, model: SlippageModel, features: np.ndarray) -> float:
+        """Make prediction using linear model."""
+        prediction = model.coefficients['intercept']
+        for i, feature_name in enumerate(model.feature_names):
+            prediction += model.coefficients[feature_name] * features[0, i]
+        return max(0, prediction)  # Non-negative slippage
+
+    def _predict_rf(self, model: SlippageModel, features: np.ndarray) -> float:
+        """Make prediction using random forest model (simplified)."""
+        # This is a simplified version - in production you'd want to save/load the actual sklearn model
+        # For now, use a weighted average based on feature importance
+        weighted_score = sum(
+            model.coefficients[feature_name] * features[0, i]
+            for i, feature_name in enumerate(model.feature_names)
+        )
+        return max(0, weighted_score * 10)  # Scale and ensure non-negative
+
+    def _default_slippage_estimate(self, symbol: str, side: str, quantity: int,
+                                 market_conditions: Dict[str, float]) -> Dict[str, float]:
+        """Provide default slippage estimate when no model is available."""
+        # Simple heuristic based on spread and quantity
+        spread_bps = market_conditions.get('spread_bps', 10)
+        base_slippage = spread_bps * 0.3  # 30% of spread
+
+        # Adjust for order size
+        if quantity > 10000:
+            base_slippage *= 1.5
+        elif quantity > 1000:
+            base_slippage *= 1.2
+
+        # Adjust for volatility
+        volatility = market_conditions.get('volatility', 0.02)
+        if volatility > 0.03:  # High volatility
+            base_slippage *= 1.3
+
+        return {
+            'expected_slippage_bps': base_slippage,
+            'confidence_interval_bps': base_slippage * 2,
+            'model_r_squared': 0.0,
+            'model_last_updated': 'never',
+            'prediction_confidence': 0.3
+        }
+
+
+class ExecutionQualityMonitor:
+    """Monitors execution quality and detects degradation."""
+
+    def __init__(self):
+        self.slippage_predictor = SlippagePredictor()
+        self.execution_metrics = {}
+        self.quality_thresholds = {
+            'max_slippage_bps': 20,
+            'max_latency_ms': 250,
+            'min_fill_rate': 0.95,
+            'max_adverse_selection_bps': 5
+        }
+
+    def monitor_execution(self, execution: ExecutionRecord) -> Dict[str, Any]:
+        """Monitor a single execution and update quality metrics."""
+        # Record execution for slippage calibration
+        self.slippage_predictor.record_execution(execution)
+
+        # Calculate quality metrics
+        quality_metrics = {
+            'timestamp': execution.timestamp,
+            'symbol': execution.symbol,
+            'fill_rate': execution.filled_quantity / execution.intended_quantity,
+            'latency_ms': execution.latency_ms,
+            'slippage_bps': execution.actual_slippage_bps,
+            'adverse_selection_bps': self._calculate_adverse_selection(execution)
+        }
+
+        # Check against thresholds
+        violations = []
+        if quality_metrics['slippage_bps'] > self.quality_thresholds['max_slippage_bps']:
+            violations.append(f"High slippage: {quality_metrics['slippage_bps']:.1f} bps")
+
+        if quality_metrics['latency_ms'] > self.quality_thresholds['max_latency_ms']:
+            violations.append(f"High latency: {quality_metrics['latency_ms']:.0f} ms")
+
+        if quality_metrics['fill_rate'] < self.quality_thresholds['min_fill_rate']:
+            violations.append(f"Low fill rate: {quality_metrics['fill_rate']:.1%}")
+
+        quality_metrics['violations'] = violations
+        quality_metrics['quality_score'] = self._calculate_quality_score(quality_metrics)
+
+        # Store in metrics history
+        symbol = execution.symbol
+        if symbol not in self.execution_metrics:
+            self.execution_metrics[symbol] = []
+        self.execution_metrics[symbol].append(quality_metrics)
+
+        # Keep only recent history (last 1000 executions per symbol)
+        if len(self.execution_metrics[symbol]) > 1000:
+            self.execution_metrics[symbol] = self.execution_metrics[symbol][-1000:]
+
+        return quality_metrics
+
+    def _calculate_adverse_selection(self, execution: ExecutionRecord) -> float:
+        """Calculate adverse selection cost."""
+        # Simplified calculation - in production you'd track price movement after execution
+        # For now, use latency as a proxy
+        if execution.latency_ms > 100:
+            return execution.latency_ms * 0.02  # Rough approximation
+        return 0
+
+    def _calculate_quality_score(self, metrics: Dict[str, Any]) -> float:
+        """Calculate overall quality score (0-1, higher is better)."""
+        score = 1.0
+
+        # Penalize high slippage
+        if metrics['slippage_bps'] > 0:
+            score -= min(metrics['slippage_bps'] / 50, 0.5)  # Max 50% penalty
+
+        # Penalize high latency
+        if metrics['latency_ms'] > 100:
+            score -= min((metrics['latency_ms'] - 100) / 500, 0.3)  # Max 30% penalty
+
+        # Penalize low fill rate
+        if metrics['fill_rate'] < 1.0:
+            score -= (1.0 - metrics['fill_rate']) * 0.5  # 50% penalty for unfilled
+
+        return max(0, score)
+
+    def get_execution_summary(self, symbol: Optional[str] = None,
+                            hours: int = 24) -> Dict[str, Any]:
+        """Get execution quality summary."""
+        cutoff_time = datetime.now() - timedelta(hours=hours)
+
+        if symbol:
+            symbols_to_check = [symbol] if symbol in self.execution_metrics else []
+        else:
+            symbols_to_check = list(self.execution_metrics.keys())
+
+        summary = {
+            'period_hours': hours,
+            'symbols_analyzed': len(symbols_to_check),
+            'total_executions': 0,
+            'avg_slippage_bps': 0,
+            'avg_latency_ms': 0,
+            'avg_fill_rate': 0,
+            'avg_quality_score': 0,
+            'violation_count': 0,
+            'symbol_breakdown': {}
+        }
+
+        all_metrics = []
+        for sym in symbols_to_check:
+            recent_metrics = [
+                m for m in self.execution_metrics[sym]
+                if m['timestamp'] >= cutoff_time
+            ]
+
+            if recent_metrics:
+                symbol_summary = {
+                    'executions': len(recent_metrics),
+                    'avg_slippage_bps': np.mean([m['slippage_bps'] for m in recent_metrics]),
+                    'avg_latency_ms': np.mean([m['latency_ms'] for m in recent_metrics]),
+                    'avg_fill_rate': np.mean([m['fill_rate'] for m in recent_metrics]),
+                    'avg_quality_score': np.mean([m['quality_score'] for m in recent_metrics]),
+                    'violations': sum(len(m['violations']) for m in recent_metrics)
+                }
+                summary['symbol_breakdown'][sym] = symbol_summary
+                all_metrics.extend(recent_metrics)
+
+        if all_metrics:
+            summary['total_executions'] = len(all_metrics)
+            summary['avg_slippage_bps'] = np.mean([m['slippage_bps'] for m in all_metrics])
+            summary['avg_latency_ms'] = np.mean([m['latency_ms'] for m in all_metrics])
+            summary['avg_fill_rate'] = np.mean([m['fill_rate'] for m in all_metrics])
+            summary['avg_quality_score'] = np.mean([m['quality_score'] for m in all_metrics])
+            summary['violation_count'] = sum(len(m['violations']) for m in all_metrics)
+
+        return summary
+
+    def calibrate_slippage_models(self) -> Dict[str, Any]:
+        """Trigger slippage model recalibration."""
+        return self.slippage_predictor.calibrate_models()
+
+    def save_models(self, filepath: str) -> bool:
+        """Save calibrated models to file."""
+        try:
+            models_data = {}
+            for asset_class, model in self.slippage_predictor.models.items():
+                models_data[asset_class] = asdict(model)
+
+            with open(filepath, 'w') as f:
+                json.dump(models_data, f, indent=2, default=str)
+
+            logger.info(f"Saved {len(models_data)} slippage models to {filepath}")
+            return True
+
+        except Exception as e:
+            logger.error(f"Failed to save models: {e}")
+            return False
\ No newline at end of file
Index: backend/validation/operations/slo_monitoring.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/operations/slo_monitoring.py b/backend/validation/operations/slo_monitoring.py
new file mode 100644
--- /dev/null	(date 1758169658986)
+++ b/backend/validation/operations/slo_monitoring.py	(date 1758169658986)
@@ -0,0 +1,507 @@
+"""
+Production SLO Monitoring & Error Budget Management
+Implements service level objectives and error budget tracking for trading systems.
+"""
+
+import asyncio
+import logging
+import time
+from datetime import datetime, timedelta
+from typing import Dict, List, Optional, Tuple, Any
+from dataclasses import dataclass, field
+from enum import Enum
+import numpy as np
+import pandas as pd
+from collections import defaultdict, deque
+import json
+
+
+class SLOStatus(Enum):
+    HEALTHY = "healthy"
+    WARNING = "warning"
+    CRITICAL = "critical"
+    EXHAUSTED = "exhausted"
+
+
+@dataclass
+class SLODefinition:
+    name: str
+    description: str
+    target_percentage: float  # 99.9 = 99.9%
+    measurement_window_hours: int  # Rolling window
+    error_budget_window_hours: int  # Error budget calculation window
+    alerting_thresholds: Dict[str, float] = field(default_factory=lambda: {
+        'warning': 0.8,  # 80% of error budget consumed
+        'critical': 0.95  # 95% of error budget consumed
+    })
+    dependencies: List[str] = field(default_factory=list)
+
+
+@dataclass
+class SLOMetrics:
+    timestamp: datetime
+    success_count: int
+    total_count: int
+    latency_p50: Optional[float] = None
+    latency_p95: Optional[float] = None
+    latency_p99: Optional[float] = None
+    error_details: Dict[str, int] = field(default_factory=dict)
+
+
+class SLOCalculator:
+    """Calculates SLO compliance and error budget consumption."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+
+    def calculate_availability(self, metrics: List[SLOMetrics],
+                             window_hours: int) -> Tuple[float, Dict[str, Any]]:
+        """Calculate availability percentage over time window."""
+        if not metrics:
+            return 0.0, {}
+
+        cutoff_time = datetime.now() - timedelta(hours=window_hours)
+        recent_metrics = [m for m in metrics if m.timestamp >= cutoff_time]
+
+        if not recent_metrics:
+            return 0.0, {}
+
+        total_requests = sum(m.total_count for m in recent_metrics)
+        successful_requests = sum(m.success_count for m in recent_metrics)
+
+        if total_requests == 0:
+            return 100.0, {'total_requests': 0}
+
+        availability = (successful_requests / total_requests) * 100
+
+        details = {
+            'total_requests': total_requests,
+            'successful_requests': successful_requests,
+            'failed_requests': total_requests - successful_requests,
+            'availability_percentage': availability,
+            'measurement_period_hours': window_hours,
+            'data_points': len(recent_metrics)
+        }
+
+        return availability, details
+
+    def calculate_latency_slo(self, metrics: List[SLOMetrics],
+                            percentile: str, threshold_ms: float,
+                            window_hours: int) -> Tuple[float, Dict[str, Any]]:
+        """Calculate latency SLO compliance."""
+        cutoff_time = datetime.now() - timedelta(hours=window_hours)
+        recent_metrics = [m for m in metrics if m.timestamp >= cutoff_time]
+
+        if not recent_metrics:
+            return 0.0, {}
+
+        latency_values = []
+        for m in recent_metrics:
+            if percentile == 'p50' and m.latency_p50 is not None:
+                latency_values.append(m.latency_p50)
+            elif percentile == 'p95' and m.latency_p95 is not None:
+                latency_values.append(m.latency_p95)
+            elif percentile == 'p99' and m.latency_p99 is not None:
+                latency_values.append(m.latency_p99)
+
+        if not latency_values:
+            return 0.0, {}
+
+        compliant_count = sum(1 for latency in latency_values if latency <= threshold_ms)
+        compliance_percentage = (compliant_count / len(latency_values)) * 100
+
+        details = {
+            'compliance_percentage': compliance_percentage,
+            'threshold_ms': threshold_ms,
+            'measurements': len(latency_values),
+            'compliant_measurements': compliant_count,
+            f'avg_{percentile}_ms': np.mean(latency_values),
+            f'max_{percentile}_ms': max(latency_values),
+            f'min_{percentile}_ms': min(latency_values)
+        }
+
+        return compliance_percentage, details
+
+    def calculate_error_budget(self, slo_def: SLODefinition,
+                             current_availability: float,
+                             window_hours: int) -> Dict[str, Any]:
+        """Calculate error budget consumption."""
+        # Error budget = (100 - SLO target) over the measurement window
+        error_budget_percentage = 100 - slo_def.target_percentage
+
+        # Current error rate
+        current_error_rate = 100 - current_availability
+
+        # Error budget consumption
+        if error_budget_percentage > 0:
+            budget_consumed_percentage = (current_error_rate / error_budget_percentage) * 100
+        else:
+            budget_consumed_percentage = 0 if current_error_rate == 0 else 100
+
+        # Time-based calculations
+        total_budget_minutes = (error_budget_percentage / 100) * window_hours * 60
+        consumed_budget_minutes = (current_error_rate / 100) * window_hours * 60
+        remaining_budget_minutes = max(0, total_budget_minutes - consumed_budget_minutes)
+
+        # Burn rate (how fast we're consuming error budget)
+        recent_window_hours = min(4, window_hours)  # Look at last 4 hours for burn rate
+        burn_rate = self._calculate_burn_rate(current_availability,
+                                            slo_def.target_percentage,
+                                            recent_window_hours)
+
+        # Time to exhaustion
+        if burn_rate > 0 and remaining_budget_minutes > 0:
+            hours_to_exhaustion = remaining_budget_minutes / 60 / burn_rate
+        else:
+            hours_to_exhaustion = float('inf')
+
+        return {
+            'error_budget_percentage': error_budget_percentage,
+            'budget_consumed_percentage': min(100, budget_consumed_percentage),
+            'remaining_budget_percentage': max(0, 100 - budget_consumed_percentage),
+            'total_budget_minutes': total_budget_minutes,
+            'consumed_budget_minutes': consumed_budget_minutes,
+            'remaining_budget_minutes': remaining_budget_minutes,
+            'current_burn_rate': burn_rate,
+            'hours_to_exhaustion': hours_to_exhaustion,
+            'status': self._get_budget_status(budget_consumed_percentage, slo_def)
+        }
+
+    def _calculate_burn_rate(self, current_availability: float,
+                           target_availability: float, window_hours: int) -> float:
+        """Calculate error budget burn rate (multiplier of normal consumption)."""
+        normal_error_rate = 100 - target_availability
+        current_error_rate = 100 - current_availability
+
+        if normal_error_rate > 0:
+            return current_error_rate / normal_error_rate
+        return 0
+
+    def _get_budget_status(self, consumed_percentage: float,
+                          slo_def: SLODefinition) -> SLOStatus:
+        """Determine SLO status based on error budget consumption."""
+        if consumed_percentage >= 100:
+            return SLOStatus.EXHAUSTED
+        elif consumed_percentage >= slo_def.alerting_thresholds['critical'] * 100:
+            return SLOStatus.CRITICAL
+        elif consumed_percentage >= slo_def.alerting_thresholds['warning'] * 100:
+            return SLOStatus.WARNING
+        else:
+            return SLOStatus.HEALTHY
+
+
+class TradingSLOMonitor:
+    """Production SLO monitoring for trading systems."""
+
+    def __init__(self):
+        self.logger = logging.getLogger(__name__)
+        self.calculator = SLOCalculator()
+        self.metrics_store: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
+        self.slo_definitions: Dict[str, SLODefinition] = {}
+        self.alert_callbacks: List[callable] = []
+        self._setup_trading_slos()
+
+    def _setup_trading_slos(self):
+        """Setup critical trading system SLOs."""
+
+        # Order execution availability
+        self.slo_definitions['order_execution'] = SLODefinition(
+            name='order_execution',
+            description='Order execution success rate',
+            target_percentage=99.5,  # 99.5% success rate
+            measurement_window_hours=24,
+            error_budget_window_hours=168,  # Weekly error budget
+            alerting_thresholds={'warning': 0.7, 'critical': 0.9}
+        )
+
+        # Market data feed availability
+        self.slo_definitions['market_data'] = SLODefinition(
+            name='market_data',
+            description='Market data feed availability',
+            target_percentage=99.9,  # 99.9% uptime
+            measurement_window_hours=24,
+            error_budget_window_hours=168,
+            alerting_thresholds={'warning': 0.8, 'critical': 0.95}
+        )
+
+        # Risk engine latency
+        self.slo_definitions['risk_engine_latency'] = SLODefinition(
+            name='risk_engine_latency',
+            description='Risk engine response time P95 < 100ms',
+            target_percentage=95.0,  # 95% of requests under 100ms
+            measurement_window_hours=4,
+            error_budget_window_hours=24,
+            alerting_thresholds={'warning': 0.8, 'critical': 0.9}
+        )
+
+        # Portfolio reconciliation
+        self.slo_definitions['portfolio_reconciliation'] = SLODefinition(
+            name='portfolio_reconciliation',
+            description='Portfolio reconciliation success rate',
+            target_percentage=99.0,  # 99% success rate
+            measurement_window_hours=24,
+            error_budget_window_hours=168,
+            alerting_thresholds={'warning': 0.6, 'critical': 0.8}
+        )
+
+        # Strategy execution latency
+        self.slo_definitions['strategy_execution'] = SLODefinition(
+            name='strategy_execution',
+            description='Strategy signal to order latency P99 < 500ms',
+            target_percentage=99.0,  # 99% under 500ms
+            measurement_window_hours=4,
+            error_budget_window_hours=24,
+            alerting_thresholds={'warning': 0.7, 'critical': 0.9}
+        )
+
+    def record_metrics(self, slo_name: str, metrics: SLOMetrics):
+        """Record metrics for SLO calculation."""
+        if slo_name not in self.slo_definitions:
+            self.logger.warning(f"Unknown SLO: {slo_name}")
+            return
+
+        self.metrics_store[slo_name].append(metrics)
+        self.logger.debug(f"Recorded metrics for {slo_name}: {metrics}")
+
+    def get_slo_status(self, slo_name: str) -> Dict[str, Any]:
+        """Get current SLO status and error budget."""
+        if slo_name not in self.slo_definitions:
+            raise ValueError(f"Unknown SLO: {slo_name}")
+
+        slo_def = self.slo_definitions[slo_name]
+        metrics = list(self.metrics_store[slo_name])
+
+        # Calculate availability SLO
+        availability, availability_details = self.calculator.calculate_availability(
+            metrics, slo_def.measurement_window_hours
+        )
+
+        # Calculate error budget
+        error_budget = self.calculator.calculate_error_budget(
+            slo_def, availability, slo_def.error_budget_window_hours
+        )
+
+        # Check for latency SLOs
+        latency_slos = {}
+        if 'latency' in slo_name.lower():
+            if 'risk_engine' in slo_name:
+                threshold_ms = 100
+                percentile = 'p95'
+            elif 'strategy' in slo_name:
+                threshold_ms = 500
+                percentile = 'p99'
+            else:
+                threshold_ms = 200
+                percentile = 'p95'
+
+            latency_compliance, latency_details = self.calculator.calculate_latency_slo(
+                metrics, percentile, threshold_ms, slo_def.measurement_window_hours
+            )
+            latency_slos[f'{percentile}_latency'] = {
+                'compliance': latency_compliance,
+                'details': latency_details
+            }
+
+        return {
+            'slo_name': slo_name,
+            'slo_definition': slo_def,
+            'availability': {
+                'percentage': availability,
+                'details': availability_details
+            },
+            'error_budget': error_budget,
+            'latency_slos': latency_slos,
+            'overall_status': error_budget['status'],
+            'timestamp': datetime.now()
+        }
+
+    def get_all_slo_status(self) -> Dict[str, Dict[str, Any]]:
+        """Get status for all SLOs."""
+        return {name: self.get_slo_status(name) for name in self.slo_definitions.keys()}
+
+    def add_alert_callback(self, callback: callable):
+        """Add callback for SLO alerts."""
+        self.alert_callbacks.append(callback)
+
+    async def monitor_slos(self, check_interval_seconds: int = 60):
+        """Continuous SLO monitoring with alerting."""
+        self.logger.info("Starting SLO monitoring")
+
+        while True:
+            try:
+                all_status = self.get_all_slo_status()
+
+                for slo_name, status in all_status.items():
+                    slo_status = status['overall_status']
+                    error_budget = status['error_budget']
+
+                    # Check for alerts
+                    if slo_status in [SLOStatus.WARNING, SLOStatus.CRITICAL, SLOStatus.EXHAUSTED]:
+                        alert_data = {
+                            'slo_name': slo_name,
+                            'status': slo_status,
+                            'error_budget_consumed': error_budget['budget_consumed_percentage'],
+                            'hours_to_exhaustion': error_budget['hours_to_exhaustion'],
+                            'burn_rate': error_budget['current_burn_rate'],
+                            'timestamp': datetime.now()
+                        }
+
+                        # Send alerts
+                        for callback in self.alert_callbacks:
+                            try:
+                                await callback(alert_data)
+                            except Exception as e:
+                                self.logger.error(f"Alert callback failed: {e}")
+
+                await asyncio.sleep(check_interval_seconds)
+
+            except Exception as e:
+                self.logger.error(f"SLO monitoring error: {e}")
+                await asyncio.sleep(check_interval_seconds)
+
+
+class ErrorBudgetManager:
+    """Manages error budget allocation and spending decisions."""
+
+    def __init__(self, slo_monitor: TradingSLOMonitor):
+        self.slo_monitor = slo_monitor
+        self.logger = logging.getLogger(__name__)
+        self.budget_policies: Dict[str, Dict[str, Any]] = {}
+        self._setup_budget_policies()
+
+    def _setup_budget_policies(self):
+        """Setup error budget spending policies."""
+        self.budget_policies = {
+            'order_execution': {
+                'max_burn_rate': 2.0,  # Max 2x normal error rate
+                'deployment_gate_threshold': 0.8,  # Block deploys at 80% budget consumed
+                'load_shedding_threshold': 0.9  # Start load shedding at 90%
+            },
+            'market_data': {
+                'max_burn_rate': 1.5,
+                'deployment_gate_threshold': 0.9,
+                'load_shedding_threshold': 0.95
+            },
+            'risk_engine_latency': {
+                'max_burn_rate': 3.0,  # Can tolerate higher burn for latency
+                'deployment_gate_threshold': 0.7,
+                'load_shedding_threshold': 0.85
+            }
+        }
+
+    def should_block_deployment(self, slo_name: str) -> Tuple[bool, str]:
+        """Check if deployment should be blocked due to error budget."""
+        status = self.slo_monitor.get_slo_status(slo_name)
+        error_budget = status['error_budget']
+
+        if slo_name not in self.budget_policies:
+            return False, "No policy defined"
+
+        policy = self.budget_policies[slo_name]
+        consumed_percentage = error_budget['budget_consumed_percentage'] / 100
+
+        if consumed_percentage >= policy['deployment_gate_threshold']:
+            return True, f"Error budget {consumed_percentage:.1%} >= threshold {policy['deployment_gate_threshold']:.1%}"
+
+        burn_rate = error_budget['current_burn_rate']
+        if burn_rate > policy['max_burn_rate']:
+            return True, f"Burn rate {burn_rate:.1f}x > max {policy['max_burn_rate']}x"
+
+        return False, "Deployment approved"
+
+    def should_enable_load_shedding(self, slo_name: str) -> Tuple[bool, str]:
+        """Check if load shedding should be enabled."""
+        status = self.slo_monitor.get_slo_status(slo_name)
+        error_budget = status['error_budget']
+
+        if slo_name not in self.budget_policies:
+            return False, "No policy defined"
+
+        policy = self.budget_policies[slo_name]
+        consumed_percentage = error_budget['budget_consumed_percentage'] / 100
+
+        if consumed_percentage >= policy['load_shedding_threshold']:
+            return True, f"Error budget {consumed_percentage:.1%} >= threshold {policy['load_shedding_threshold']:.1%}"
+
+        return False, "Load shedding not required"
+
+    def get_budget_recommendations(self) -> Dict[str, Dict[str, Any]]:
+        """Get error budget recommendations for all SLOs."""
+        recommendations = {}
+
+        for slo_name in self.slo_monitor.slo_definitions.keys():
+            status = self.slo_monitor.get_slo_status(slo_name)
+
+            block_deployment, deployment_reason = self.should_block_deployment(slo_name)
+            enable_load_shedding, load_shedding_reason = self.should_enable_load_shedding(slo_name)
+
+            recommendations[slo_name] = {
+                'current_status': status['overall_status'],
+                'error_budget_consumed': status['error_budget']['budget_consumed_percentage'],
+                'burn_rate': status['error_budget']['current_burn_rate'],
+                'hours_to_exhaustion': status['error_budget']['hours_to_exhaustion'],
+                'block_deployment': block_deployment,
+                'deployment_reason': deployment_reason,
+                'enable_load_shedding': enable_load_shedding,
+                'load_shedding_reason': load_shedding_reason,
+                'timestamp': datetime.now()
+            }
+
+        return recommendations
+
+
+# Example usage and testing
+if __name__ == "__main__":
+    import asyncio
+
+    async def example_alert_handler(alert_data):
+        print(f"ALERT: {alert_data['slo_name']} - {alert_data['status'].value}")
+        print(f"Error budget: {alert_data['error_budget_consumed']:.1f}% consumed")
+        print(f"Burn rate: {alert_data['burn_rate']:.1f}x")
+
+    async def simulate_trading_day():
+        monitor = TradingSLOMonitor()
+        budget_manager = ErrorBudgetManager(monitor)
+        monitor.add_alert_callback(example_alert_handler)
+
+        # Simulate some metrics
+        now = datetime.now()
+
+        # Good order execution
+        monitor.record_metrics('order_execution', SLOMetrics(
+            timestamp=now,
+            success_count=995,
+            total_count=1000,
+            latency_p95=50.0
+        ))
+
+        # Market data with some issues
+        monitor.record_metrics('market_data', SLOMetrics(
+            timestamp=now,
+            success_count=980,
+            total_count=1000
+        ))
+
+        # Risk engine latency issues
+        monitor.record_metrics('risk_engine_latency', SLOMetrics(
+            timestamp=now,
+            success_count=900,
+            total_count=1000,
+            latency_p95=150.0
+        ))
+
+        # Get status
+        all_status = monitor.get_all_slo_status()
+        for slo_name, status in all_status.items():
+            print(f"\n{slo_name.upper()}:")
+            print(f"  Availability: {status['availability']['percentage']:.2f}%")
+            print(f"  Error Budget: {status['error_budget']['budget_consumed_percentage']:.1f}% consumed")
+            print(f"  Status: {status['overall_status'].value}")
+
+        # Check budget recommendations
+        recommendations = budget_manager.get_budget_recommendations()
+        print("\nBUDGET RECOMMENDATIONS:")
+        for slo_name, rec in recommendations.items():
+            print(f"{slo_name}: Deploy={not rec['block_deployment']}, LoadShed={rec['enable_load_shedding']}")
+
+    asyncio.run(simulate_trading_day())
\ No newline at end of file
Index: backend/validation/validation_orchestrator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/validation_orchestrator.py b/backend/validation/validation_orchestrator.py
new file mode 100644
--- /dev/null	(date 1758169658988)
+++ b/backend/validation/validation_orchestrator.py	(date 1758169658988)
@@ -0,0 +1,637 @@
+"""Validation orchestrator for comprehensive strategy validation and alpha discovery.
+
+Coordinates all validation modules to provide complete strategy evaluation
+and deployment readiness assessment.
+"""
+
+import numpy as np
+import pandas as pd
+from typing import Dict, List, Optional, Any, Tuple
+import logging
+import asyncio
+from datetime import datetime
+import json
+import os
+
+from .factor_analysis import AlphaFactorAnalyzer
+from .regime_testing import RegimeValidator
+from .cross_market_validator import CrossMarketValidator
+from .ensemble_evaluator import EnsembleValidator
+from .capital_efficiency import CapitalEfficiencyAnalyzer
+from .alpha_validation_gate import AlphaValidationGate, ValidationCriteria
+
+logger = logging.getLogger(__name__)
+
+
+class ValidationOrchestrator:
+    """Orchestrates comprehensive strategy validation and alpha discovery."""
+
+    def __init__(self, criteria: ValidationCriteria = None):
+        self.criteria = criteria or ValidationCriteria()
+
+        # Initialize validators
+        self.factor_analyzer = AlphaFactorAnalyzer()
+        self.regime_validator = RegimeValidator()
+        self.cross_market_validator = CrossMarketValidator()
+        self.ensemble_evaluator = EnsembleValidator()
+        self.capital_analyzer = CapitalEfficiencyAnalyzer()
+        self.validation_gate = AlphaValidationGate(self.criteria)
+
+    def validate_strategy(self,
+                         strategy,
+                         start_date: str = '2020-01-01',
+                         end_date: str = '2024-12-31',
+                         market_data: Optional[Dict[str, pd.Series]] = None,
+                         run_parallel: bool = True) -> Dict[str, Any]:
+        """Run comprehensive validation for a single strategy.
+
+        Args:
+            strategy: Strategy object to validate
+            start_date: Start date for validation
+            end_date: End date for validation
+            market_data: Optional market data for regime analysis
+            run_parallel: Whether to run validations in parallel
+
+        Returns:
+            Comprehensive validation results
+        """
+        logger.info(f"Starting comprehensive validation for strategy: {getattr(strategy, 'name', 'Unknown')}")
+
+        validation_results = {
+            'strategy_info': self._extract_strategy_info(strategy),
+            'validation_period': {'start': start_date, 'end': end_date},
+            'validation_timestamp': datetime.now().isoformat()
+        }
+
+        try:
+            # Get strategy returns
+            strategy_returns = self._get_strategy_returns(strategy, start_date, end_date)
+            if strategy_returns is None or len(strategy_returns) < 60:
+                return self._create_insufficient_data_response(strategy_returns)
+
+            validation_results['data_quality'] = self._assess_data_quality(strategy_returns)
+
+            # Get market data if not provided
+            if market_data is None:
+                market_data = self._get_default_market_data(start_date, end_date)
+
+            if run_parallel:
+                # Run validations in parallel
+                validation_results.update(
+                    self._run_parallel_validations(strategy, strategy_returns, market_data, start_date, end_date)
+                )
+            else:
+                # Run validations sequentially
+                validation_results.update(
+                    self._run_sequential_validations(strategy, strategy_returns, market_data, start_date, end_date)
+                )
+
+            # Final evaluation
+            validation_results['deployment_decision'] = self.validation_gate.evaluate_go_no_go(validation_results)
+
+            logger.info(f"Validation completed. Recommendation: {validation_results['deployment_decision']['recommendation']}")
+
+        except Exception as e:
+            logger.error(f"Validation failed: {e}")
+            validation_results['error'] = str(e)
+            validation_results['deployment_decision'] = {
+                'recommendation': 'ERROR',
+                'error': str(e)
+            }
+
+        return validation_results
+
+    def validate_multiple_strategies(self,
+                                   strategies: Dict[str, Any],
+                                   start_date: str = '2020-01-01',
+                                   end_date: str = '2024-12-31') -> Dict[str, Any]:
+        """Validate multiple strategies and perform ensemble analysis.
+
+        Args:
+            strategies: Dictionary mapping strategy names to strategy objects
+            start_date: Start date for validation
+            end_date: End date for validation
+
+        Returns:
+            Comprehensive validation results for all strategies plus ensemble analysis
+        """
+        logger.info(f"Starting validation for {len(strategies)} strategies")
+
+        results = {
+            'individual_strategy_results': {},
+            'ensemble_analysis': {},
+            'comparative_analysis': {},
+            'validation_summary': {}
+        }
+
+        strategy_returns = {}
+
+        # Validate individual strategies
+        for strategy_name, strategy in strategies.items():
+            logger.info(f"Validating strategy: {strategy_name}")
+
+            try:
+                individual_results = self.validate_strategy(
+                    strategy, start_date, end_date, run_parallel=False
+                )
+                results['individual_strategy_results'][strategy_name] = individual_results
+
+                # Collect returns for ensemble analysis
+                if 'data_quality' in individual_results and 'returns' in individual_results['data_quality']:
+                    strategy_returns[strategy_name] = individual_results['data_quality']['returns']
+
+            except Exception as e:
+                logger.error(f"Failed to validate strategy {strategy_name}: {e}")
+                results['individual_strategy_results'][strategy_name] = {'error': str(e)}
+
+        # Ensemble analysis if we have multiple valid strategies
+        if len(strategy_returns) > 1:
+            try:
+                logger.info("Running ensemble analysis")
+                ensemble_results = self.ensemble_evaluator.analyze_strategy_correlations(strategy_returns)
+                results['ensemble_analysis'] = ensemble_results
+
+                # Build optimal ensemble
+                optimal_ensemble = self.ensemble_evaluator.build_optimal_ensemble(strategy_returns)
+                results['ensemble_analysis']['optimal_ensemble'] = optimal_ensemble
+
+            except Exception as e:
+                logger.error(f"Ensemble analysis failed: {e}")
+                results['ensemble_analysis'] = {'error': str(e)}
+
+        # Comparative analysis
+        results['comparative_analysis'] = self._create_comparative_analysis(
+            results['individual_strategy_results']
+        )
+
+        # Overall summary
+        results['validation_summary'] = self._create_validation_summary(results)
+
+        return results
+
+    def _run_parallel_validations(self,
+                                strategy: Any,
+                                strategy_returns: pd.Series,
+                                market_data: Dict[str, pd.Series],
+                                start_date: str,
+                                end_date: str) -> Dict[str, Any]:
+        """Run validations in parallel using asyncio."""
+        try:
+            # Note: This is a simplified parallel approach
+            # In production, you might want to use actual async implementations
+            results = {}
+
+            # Run factor analysis
+            try:
+                logger.info("Running factor analysis")
+                factor_results = self.factor_analyzer.run_factor_regression(
+                    strategy_returns, start_date, end_date
+                )
+                results['factor_analysis'] = factor_results
+            except Exception as e:
+                logger.warning(f"Factor analysis failed: {e}")
+                results['factor_analysis'] = {'error': str(e)}
+
+            # Run regime testing
+            try:
+                logger.info("Running regime testing")
+                regime_results = self.regime_validator.test_edge_persistence(
+                    strategy_returns, market_data
+                )
+                results['regime_testing'] = regime_results
+            except Exception as e:
+                logger.warning(f"Regime testing failed: {e}")
+                results['regime_testing'] = {'error': str(e)}
+
+            # Run cross-market validation
+            try:
+                logger.info("Running cross-market validation")
+                cross_market_results = self.cross_market_validator.validate_across_markets(
+                    type(strategy), self._extract_strategy_params(strategy), start_date, end_date
+                )
+                results['cross_market'] = cross_market_results
+            except Exception as e:
+                logger.warning(f"Cross-market validation failed: {e}")
+                results['cross_market'] = {'error': str(e)}
+
+            # Run capital efficiency analysis
+            try:
+                logger.info("Running capital efficiency analysis")
+                capital_results = self._run_capital_efficiency_analysis(strategy, strategy_returns)
+                results['capital_efficiency'] = capital_results
+            except Exception as e:
+                logger.warning(f"Capital efficiency analysis failed: {e}")
+                results['capital_efficiency'] = {'error': str(e)}
+
+            return results
+
+        except Exception as e:
+            logger.error(f"Parallel validation failed: {e}")
+            return {'error': f'Parallel validation failed: {e!s}'}
+
+    def _run_sequential_validations(self,
+                                  strategy: Any,
+                                  strategy_returns: pd.Series,
+                                  market_data: Dict[str, pd.Series],
+                                  start_date: str,
+                                  end_date: str) -> Dict[str, Any]:
+        """Run validations sequentially."""
+        return self._run_parallel_validations(strategy, strategy_returns, market_data, start_date, end_date)
+
+    def _run_capital_efficiency_analysis(self, strategy: Any, strategy_returns: pd.Series) -> Dict[str, Any]:
+        """Run capital efficiency analysis with proper error handling."""
+        results = {}
+
+        try:
+            # Kelly sizing analysis
+            kelly_results = self.capital_analyzer.kelly_sizing_analysis(strategy_returns)
+            results['kelly_analysis'] = kelly_results
+
+            # Leverage efficiency (simplified version)
+            if hasattr(strategy, 'backtest_with_capital') or hasattr(strategy, 'set_capital'):
+                capital_levels = [10000, 50000, 100000]
+                leverage_results = self.capital_analyzer.analyze_leverage_efficiency(
+                    strategy, capital_levels
+                )
+                results.update(leverage_results)
+            else:
+                results['leverage_analysis'] = {
+                    'error': 'Strategy does not support capital adjustment for leverage analysis'
+                }
+
+        except Exception as e:
+            logger.error(f"Capital efficiency analysis failed: {e}")
+            results['error'] = str(e)
+
+        return results
+
+    def _get_strategy_returns(self, strategy: Any, start_date: str, end_date: str) -> Optional[pd.Series]:
+        """Extract strategy returns with multiple fallback methods."""
+        try:
+            # Method 1: Direct returns attribute
+            if hasattr(strategy, 'returns'):
+                returns = strategy.returns
+                if isinstance(returns, pd.Series) and len(returns) > 0:
+                    return returns
+
+            # Method 2: Run backtest
+            if hasattr(strategy, 'backtest'):
+                backtest_results = strategy.backtest(start=start_date, end=end_date)
+                if hasattr(backtest_results, 'returns'):
+                    return backtest_results.returns
+                elif isinstance(backtest_results, dict) and 'returns' in backtest_results:
+                    return pd.Series(backtest_results['returns'])
+
+            # Method 3: Alternative backtest method
+            if hasattr(strategy, 'run_backtest'):
+                backtest_results = strategy.run_backtest(start_date, end_date)
+                if hasattr(backtest_results, 'returns'):
+                    return backtest_results.returns
+                elif isinstance(backtest_results, dict) and 'returns' in backtest_results:
+                    return pd.Series(backtest_results['returns'])
+
+            # Method 4: Generate returns if strategy supports it
+            if hasattr(strategy, 'generate_returns'):
+                returns = strategy.generate_returns(start_date, end_date)
+                if isinstance(returns, (pd.Series, list, np.ndarray)):
+                    return pd.Series(returns)
+
+            logger.warning("Could not extract strategy returns")
+            return None
+
+        except Exception as e:
+            logger.error(f"Failed to get strategy returns: {e}")
+            return None
+
+    def _get_default_market_data(self, start_date: str, end_date: str) -> Dict[str, pd.Series]:
+        """Get default market data for regime analysis."""
+        market_data = {}
+
+        try:
+            import yfinance as yf
+
+            # Download key market indicators
+            tickers = {
+                'SPY': 'SPY',    # S&P 500
+                'VIX': '^VIX',   # Volatility Index
+                'DGS10': '^TNX'  # 10-Year Treasury (approximate)
+            }
+
+            for name, ticker in tickers.items():
+                try:
+                    data = yf.download(ticker, start=start_date, end=end_date, progress=False)
+                    if not data.empty:
+                        market_data[name] = data['Close']
+                except Exception as e:
+                    logger.warning(f"Could not download {name} data: {e}")
+
+        except ImportError:
+            logger.warning("yfinance not available, using synthetic market data")
+            # Generate synthetic market data as fallback
+            date_range = pd.date_range(start=start_date, end=end_date)
+            market_data['SPY'] = pd.Series(
+                100 * np.cumprod(1 + np.random.normal(0.0005, 0.02, len(date_range))),
+                index=date_range
+            )
+            market_data['VIX'] = pd.Series(
+                15 + 10 * np.random.beta(2, 5, len(date_range)),
+                index=date_range
+            )
+
+        except Exception as e:
+            logger.warning(f"Failed to get market data: {e}")
+
+        return market_data
+
+    def _extract_strategy_info(self, strategy: Any) -> Dict[str, Any]:
+        """Extract strategy information for documentation."""
+        info = {
+            'class_name': strategy.__class__.__name__,
+            'name': getattr(strategy, 'name', 'Unknown'),
+            'description': getattr(strategy, 'description', 'No description available')
+        }
+
+        # Try to get additional strategy attributes
+        for attr in ['version', 'author', 'parameters', 'asset_class']:
+            if hasattr(strategy, attr):
+                info[attr] = getattr(strategy, attr)
+
+        return info
+
+    def _extract_strategy_params(self, strategy: Any) -> Dict[str, Any]:
+        """Extract strategy parameters for cross-market testing."""
+        params = {}
+
+        # Common parameter attributes to look for
+        param_attrs = ['lookback', 'threshold', 'max_positions', 'stop_loss', 'take_profit']
+
+        for attr in param_attrs:
+            if hasattr(strategy, attr):
+                params[attr] = getattr(strategy, attr)
+
+        # If strategy has a parameters dict
+        if hasattr(strategy, 'parameters'):
+            params.update(strategy.parameters)
+
+        return params
+
+    def _assess_data_quality(self, returns: pd.Series) -> Dict[str, Any]:
+        """Assess quality of strategy returns data."""
+        if returns is None:
+            return {'error': 'No returns data available'}
+
+        quality_metrics = {
+            'total_observations': len(returns),
+            'non_null_observations': returns.count(),
+            'null_percentage': (len(returns) - returns.count()) / len(returns),
+            'date_range': {
+                'start': returns.index[0].strftime('%Y-%m-%d') if len(returns) > 0 else None,
+                'end': returns.index[-1].strftime('%Y-%m-%d') if len(returns) > 0 else None
+            },
+            'returns_statistics': {
+                'mean': returns.mean(),
+                'std': returns.std(),
+                'min': returns.min(),
+                'max': returns.max(),
+                'skewness': returns.skew(),
+                'kurtosis': returns.kurtosis()
+            },
+            'outliers': {
+                'extreme_positive': (returns > returns.quantile(0.99)).sum(),
+                'extreme_negative': (returns < returns.quantile(0.01)).sum()
+            },
+            'returns': returns  # Include for downstream analysis
+        }
+
+        # Data quality assessment
+        quality_score = 1.0
+        quality_issues = []
+
+        if quality_metrics['null_percentage'] > 0.05:
+            quality_score -= 0.2
+            quality_issues.append('High percentage of missing data')
+
+        if len(returns) < 252:
+            quality_score -= 0.3
+            quality_issues.append('Insufficient data for annual analysis')
+
+        if abs(returns.skew()) > 3:
+            quality_score -= 0.1
+            quality_issues.append('Highly skewed returns distribution')
+
+        quality_metrics['quality_score'] = max(0, quality_score)
+        quality_metrics['quality_issues'] = quality_issues
+
+        return quality_metrics
+
+    def _create_comparative_analysis(self, individual_results: Dict[str, Any]) -> Dict[str, Any]:
+        """Create comparative analysis across strategies."""
+        comparison = {
+            'strategy_rankings': {},
+            'best_performers': {},
+            'risk_comparison': {},
+            'deployment_readiness': {}
+        }
+
+        valid_strategies = {name: results for name, results in individual_results.items()
+                          if 'error' not in results}
+
+        if not valid_strategies:
+            return {'error': 'No valid strategies for comparison'}
+
+        # Extract key metrics for comparison
+        metrics = {}
+        for name, results in valid_strategies.items():
+            strategy_metrics = {}
+
+            # Risk management metrics
+            if 'data_quality' in results:
+                data_quality = results['data_quality']
+                if 'returns_statistics' in data_quality:
+                    stats = data_quality['returns_statistics']
+                    returns = data_quality.get('returns')
+                    if returns is not None and len(returns) > 0:
+                        strategy_metrics['sharpe_ratio'] = self._calculate_sharpe(returns)
+                        strategy_metrics['max_drawdown'] = self._calculate_max_drawdown(returns)
+                        strategy_metrics['volatility'] = stats['std'] * np.sqrt(252)
+
+            # Factor analysis metrics
+            if 'factor_analysis' in results and 'annualized_alpha' in results['factor_analysis']:
+                strategy_metrics['alpha'] = results['factor_analysis']['annualized_alpha']
+                strategy_metrics['alpha_significant'] = results['factor_analysis'].get('alpha_significant', False)
+
+            # Deployment decision
+            if 'deployment_decision' in results:
+                strategy_metrics['recommendation'] = results['deployment_decision']['recommendation']
+                strategy_metrics['confidence'] = results['deployment_decision'].get('confidence_score', 0)
+
+            metrics[name] = strategy_metrics
+
+        # Create rankings
+        for metric in ['sharpe_ratio', 'alpha', 'confidence']:
+            if any(metric in m for m in metrics.values()):
+                sorted_strategies = sorted(
+                    [(name, m.get(metric, 0)) for name, m in metrics.items()],
+                    key=lambda x: x[1], reverse=True
+                )
+                comparison['strategy_rankings'][metric] = sorted_strategies
+
+        # Best performers
+        comparison['best_performers'] = {
+            'highest_sharpe': max(metrics.items(), key=lambda x: x[1].get('sharpe_ratio', 0))[0] if metrics else None,
+            'highest_alpha': max(metrics.items(), key=lambda x: x[1].get('alpha', 0))[0] if metrics else None,
+            'most_deployable': max(metrics.items(), key=lambda x: x[1].get('confidence', 0))[0] if metrics else None
+        }
+
+        # Deployment readiness summary
+        deployment_counts = {
+            'GO': 0,
+            'CAUTIOUS_GO': 0,
+            'NO_GO': 0,
+            'INSUFFICIENT_VALIDATION': 0,
+            'ERROR': 0
+        }
+
+        for strategy_metrics in metrics.values():
+            recommendation = strategy_metrics.get('recommendation', 'ERROR')
+            deployment_counts[recommendation] = deployment_counts.get(recommendation, 0) + 1
+
+        comparison['deployment_readiness'] = deployment_counts
+
+        return comparison
+
+    def _create_validation_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
+        """Create overall validation summary."""
+        summary = {
+            'total_strategies': len(results.get('individual_strategy_results', {})),
+            'validation_completion': {},
+            'overall_assessment': {},
+            'next_steps': []
+        }
+
+        individual_results = results.get('individual_strategy_results', {})
+
+        # Count completion rates
+        validation_components = [
+            'factor_analysis', 'regime_testing', 'cross_market',
+            'capital_efficiency', 'deployment_decision'
+        ]
+
+        completion_rates = {}
+        for component in validation_components:
+            completed = sum(1 for r in individual_results.values()
+                          if component in r and 'error' not in r[component])
+            completion_rates[component] = completed / len(individual_results) if individual_results else 0
+
+        summary['validation_completion'] = completion_rates
+
+        # Overall assessment
+        if 'comparative_analysis' in results:
+            comparative = results['comparative_analysis']
+            if 'deployment_readiness' in comparative:
+                deployment_readiness = comparative['deployment_readiness']
+                ready_strategies = deployment_readiness.get('GO', 0) + deployment_readiness.get('CAUTIOUS_GO', 0)
+                total_strategies = sum(deployment_readiness.values())
+
+                summary['overall_assessment'] = {
+                    'deployment_ready_strategies': ready_strategies,
+                    'deployment_ready_percentage': ready_strategies / total_strategies if total_strategies > 0 else 0,
+                    'validation_quality': np.mean(list(completion_rates.values())),
+                    'ensemble_available': len(results.get('ensemble_analysis', {})) > 0
+                }
+
+        # Generate next steps
+        if summary['overall_assessment'].get('deployment_ready_percentage', 0) == 0:
+            summary['next_steps'].append('Address critical issues preventing deployment')
+        elif summary['overall_assessment'].get('deployment_ready_percentage', 0) < 0.5:
+            summary['next_steps'].append('Improve underperforming strategies before deployment')
+        else:
+            summary['next_steps'].append('Proceed with deployment of validated strategies')
+
+        if summary['overall_assessment'].get('ensemble_available', False):
+            summary['next_steps'].append('Consider ensemble deployment for risk reduction')
+
+        return summary
+
+    def _create_insufficient_data_response(self, returns: Optional[pd.Series]) -> Dict[str, Any]:
+        """Create response for insufficient data scenarios."""
+        return {
+            'error': 'Insufficient data for validation',
+            'data_available': len(returns) if returns is not None else 0,
+            'minimum_required': 60,
+            'deployment_decision': {
+                'recommendation': 'INSUFFICIENT_DATA',
+                'confidence_score': 0.0,
+                'error': 'Need at least 60 data points for validation'
+            }
+        }
+
+    def _calculate_sharpe(self, returns: pd.Series, risk_free_rate: float = 0.02) -> float:
+        """Calculate Sharpe ratio with error handling."""
+        try:
+            if len(returns) == 0:
+                return np.nan
+
+            excess_returns = returns - risk_free_rate / 252
+
+            if excess_returns.std() == 0:
+                return 0.0 if excess_returns.mean() == 0 else np.inf * np.sign(excess_returns.mean())
+
+            return excess_returns.mean() / excess_returns.std() * np.sqrt(252)
+
+        except Exception:
+            return np.nan
+
+    def _calculate_max_drawdown(self, returns: pd.Series) -> float:
+        """Calculate maximum drawdown."""
+        try:
+            cumulative = (1 + returns).cumprod()
+            running_max = cumulative.expanding().max()
+            drawdowns = (cumulative - running_max) / running_max
+            return drawdowns.min()
+        except Exception:
+            return np.nan
+
+    def save_validation_results(self, results: Dict[str, Any], filepath: str) -> bool:
+        """Save validation results to file."""
+        try:
+            # Convert pandas objects to serializable format
+            serializable_results = self._make_serializable(results)
+
+            os.makedirs(os.path.dirname(filepath), exist_ok=True)
+
+            with open(filepath, 'w') as f:
+                json.dump(serializable_results, f, indent=2, default=str)
+
+            logger.info(f"Validation results saved to {filepath}")
+            return True
+
+        except Exception as e:
+            logger.error(f"Failed to save validation results: {e}")
+            return False
+
+    def _make_serializable(self, obj: Any) -> Any:
+        """Convert objects to JSON-serializable format."""
+        if isinstance(obj, dict):
+            return {k: self._make_serializable(v) for k, v in obj.items()}
+        elif isinstance(obj, list):
+            return [self._make_serializable(item) for item in obj]
+        elif isinstance(obj, pd.Series):
+            return {
+                'type': 'pandas_series',
+                'data': obj.tolist(),
+                'index': obj.index.tolist()
+            }
+        elif isinstance(obj, pd.DataFrame):
+            return {
+                'type': 'pandas_dataframe',
+                'data': obj.to_dict('records')
+            }
+        elif isinstance(obj, (np.integer, np.floating)):
+            return float(obj)
+        elif isinstance(obj, np.ndarray):
+            return obj.tolist()
+        elif pd.isna(obj):
+            return None
+        else:
+            return obj
\ No newline at end of file
Index: backend/validation/validation_runner.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend/validation/validation_runner.py b/backend/validation/validation_runner.py
new file mode 100644
--- /dev/null	(date 1758168088193)
+++ b/backend/validation/validation_runner.py	(date 1758168088193)
@@ -0,0 +1,173 @@
+"""Validation Runner - Entry point for comprehensive strategy validation."""
+
+import argparse
+import logging
+from datetime import datetime
+from typing import Dict, Any
+import pandas as pd
+
+# Import validation modules
+from .reality_check import RealityCheckValidator
+from .drift_monitor import PerformanceDriftMonitor
+from .factor_analysis import AlphaFactorAnalyzer
+from .regime_testing import RegimeValidator
+
+logger = logging.getLogger(__name__)
+
+
+class ValidationRunner:
+    """Main validation runner for comprehensive strategy evaluation."""
+    
+    def __init__(self):
+        self.validators = {
+            'reality_check': RealityCheckValidator(),
+            'factor_analyzer': AlphaFactorAnalyzer(),
+            'regime_validator': RegimeValidator()
+        }
+    
+    def run_comprehensive_validation(self, strategy_returns: Dict[str, pd.Series], 
+                                   benchmark_returns: pd.Series,
+                                   market_data: pd.DataFrame,
+                                   start_date: str = '2020-01-01',
+                                   end_date: str = '2024-12-31') -> Dict[str, Any]:
+        """Run comprehensive validation suite."""
+        
+        logger.info(f"Starting comprehensive validation from {start_date} to {end_date}")
+        
+        results = {}
+        
+        # 1. Reality Check / SPA Test
+        logger.info("Running Reality Check / SPA Test...")
+        try:
+            reality_check_results = self.validators['reality_check'].validate_strategy_universe(
+                strategy_returns, benchmark_returns
+            )
+            results['reality_check'] = reality_check_results
+        except Exception as e:
+            logger.error(f"Reality check failed: {e}")
+            results['reality_check'] = {'error': str(e)}
+        
+        # 2. Factor Analysis
+        logger.info("Running Factor Analysis...")
+        try:
+            # Create factor proxies if needed
+            factor_df = self.validators['factor_analyzer'].create_factor_proxies(market_data)
+            
+            factor_results = {}
+            for strategy_name, returns in strategy_returns.items():
+                try:
+                    factor_result = self.validators['factor_analyzer'].run_factor_regression(
+                        returns, factor_df
+                    )
+                    factor_results[strategy_name] = factor_result
+                except Exception as e:
+                    logger.warning(f"Factor analysis failed for {strategy_name}: {e}")
+                    continue
+            
+            results['factor_analysis'] = factor_results
+        except Exception as e:
+            logger.error(f"Factor analysis failed: {e}")
+            results['factor_analysis'] = {'error': str(e)}
+        
+        # 3. Regime Testing
+        logger.info("Running Regime Testing...")
+        try:
+            regime_results = {}
+            for strategy_name, returns in strategy_returns.items():
+                try:
+                    regime_result = self.validators['regime_validator'].test_edge_persistence(
+                        returns, market_data
+                    )
+                    regime_results[strategy_name] = regime_result
+                except Exception as e:
+                    logger.warning(f"Regime testing failed for {strategy_name}: {e}")
+                    continue
+            
+            results['regime_testing'] = regime_results
+        except Exception as e:
+            logger.error(f"Regime testing failed: {e}")
+            results['regime_testing'] = {'error': str(e)}
+        
+        # 4. Generate Summary
+        results['summary'] = self._generate_validation_summary(results)
+        
+        logger.info("Comprehensive validation completed")
+        return results
+    
+    def _generate_validation_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
+        """Generate validation summary."""
+        summary = {
+            'validation_timestamp': datetime.now().isoformat(),
+            'validation_modules_run': list(results.keys()),
+            'overall_status': 'COMPLETED',
+            'recommendations': []
+        }
+        
+        # Analyze reality check results
+        if 'reality_check' in results and 'error' not in results['reality_check']:
+            rc = results['reality_check']
+            if rc.get('validation_passed', False):
+                summary['recommendations'].append("Strategy passes multiple hypothesis testing")
+            else:
+                summary['recommendations'].append("Strategy fails multiple hypothesis testing - consider reducing universe")
+        
+        # Analyze factor analysis results
+        if 'factor_analysis' in results and 'error' not in results['factor_analysis']:
+            fa = results['factor_analysis']
+            significant_strategies = [k for k, v in fa.items() if v.alpha_significant]
+            if significant_strategies:
+                summary['recommendations'].append(f"Significant alpha found in: {significant_strategies}")
+            else:
+                summary['recommendations'].append("No significant alpha detected")
+        
+        # Analyze regime testing results
+        if 'regime_testing' in results and 'error' not in results['regime_testing']:
+            rt = results['regime_testing']
+            robust_strategies = [k for k, v in rt.items() if v.get('edge_is_robust', False)]
+            if robust_strategies:
+                summary['recommendations'].append(f"Regime-robust strategies: {robust_strategies}")
+            else:
+                summary['recommendations'].append("No regime-robust strategies found")
+        
+        return summary
+
+
+def main():
+    """Main entry point for validation runner."""
+    parser = argparse.ArgumentParser(description='Run comprehensive strategy validation')
+    parser.add_argument('--start', default='2020-01-01', help='Start date for validation')
+    parser.add_argument('--end', default='2024-12-31', help='End date for validation')
+    parser.add_argument('--strategy', required=True, help='Strategy name to validate')
+    parser.add_argument('--output', help='Output file for results')
+    
+    args = parser.parse_args()
+    
+    # Setup logging
+    logging.basicConfig(level=logging.INFO)
+    
+    # Initialize runner
+    runner = ValidationRunner()
+    
+    # TODO: Load actual strategy data
+    # This would typically load from your strategy backtest results
+    logger.info(f"Running validation for strategy: {args.strategy}")
+    
+    # For now, return placeholder
+    results = {
+        'strategy': args.strategy,
+        'start_date': args.start,
+        'end_date': args.end,
+        'status': 'Validation framework ready - implement data loading'
+    }
+    
+    if args.output:
+        import json
+        with open(args.output, 'w') as f:
+            json.dump(results, f, indent=2, default=str)
+    
+    print(f"Validation completed for {args.strategy}")
+    print(f"Results: {results}")
+
+
+if __name__ == '__main__':
+    main()
